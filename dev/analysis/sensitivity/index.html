<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Local Sensitivity Analysis (Automatic Differentiation) · DifferentialEquations.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-90474609-3', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://diffeq.sciml.ai/stable/analysis/sensitivity/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DifferentialEquations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DifferentialEquations.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/ode_example/">Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/advanced_ode_example/">Solving Stiff Equations</a></li><li><a class="tocitem" href="../../tutorials/sde_example/">Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/rode_example/">Random Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/dde_example/">Delay Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/dae_example/">Differential Algebraic Equations</a></li><li><a class="tocitem" href="../../tutorials/discrete_stochastic_example/">Discrete Stochastic (Gillespie) Equations</a></li><li><a class="tocitem" href="../../tutorials/jump_diffusion/">Jump Diffusion Equations</a></li><li><a class="tocitem" href="../../tutorials/bvp_example/">Boundary Value Problems</a></li><li><a class="tocitem" href="../../tutorials/additional/">Additional Tutorials</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../../basics/overview/">Overview of DifferentialEquations.jl</a></li><li><a class="tocitem" href="../../basics/common_solver_opts/">Common Solver Options</a></li><li><a class="tocitem" href="../../basics/solution/">Solution Handling</a></li><li><a class="tocitem" href="../../basics/plot/">Plot Functions</a></li><li><a class="tocitem" href="../../basics/integrator/">Integrator Interface</a></li><li><a class="tocitem" href="../../basics/problem/">Problem Interface</a></li><li><a class="tocitem" href="../../basics/faq/">Frequently Asked Questions</a></li><li><a class="tocitem" href="../../basics/compatibility_chart/">Solver Compatibility Chart</a></li></ul></li><li><span class="tocitem">Problem Types</span><ul><li><a class="tocitem" href="../../types/discrete_types/">Discrete Problems</a></li><li><a class="tocitem" href="../../types/ode_types/">ODE Problems</a></li><li><a class="tocitem" href="../../types/nonautonomous_linear_ode/">Non-autonomous Linear ODE / Lie Group Problems</a></li><li><a class="tocitem" href="../../types/dynamical_types/">Dynamical, Hamiltonian and 2nd Order ODE Problems</a></li><li><a class="tocitem" href="../../types/split_ode_types/">Split ODE Problems</a></li><li><a class="tocitem" href="../../types/steady_state_types/">Steady State Problems</a></li><li><a class="tocitem" href="../../types/bvp_types/">BVP Problems</a></li><li><a class="tocitem" href="../../types/sde_types/">SDE Problems</a></li><li><a class="tocitem" href="../../types/sdae_types/">SDAE Problems</a></li><li><a class="tocitem" href="../../types/rode_types/">RODE Problems</a></li><li><a class="tocitem" href="../../types/dde_types/">DDE Problems</a></li><li><a class="tocitem" href="../../types/sdde_types/">SDDE Problems</a></li><li><a class="tocitem" href="../../types/dae_types/">DAE Problems</a></li><li><a class="tocitem" href="../../types/jump_types/">Jump Problems</a></li></ul></li><li><span class="tocitem">Solver Algorithms</span><ul><li><a class="tocitem" href="../../solvers/discrete_solve/">Discrete Solvers</a></li><li><a class="tocitem" href="../../solvers/ode_solve/">ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/nonautonomous_linear_ode/">Non-autonomous Linear ODE / Lie Group ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/dynamical_solve/">Dynamical, Hamiltonian, and 2nd Order ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/split_ode_solve/">Split ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/steady_state_solve/">Steady State Solvers</a></li><li><a class="tocitem" href="../../solvers/bvp_solve/">BVP Solvers</a></li><li><a class="tocitem" href="../../solvers/jump_solve/">Jump Problem and Jump Diffusion Solvers</a></li><li><a class="tocitem" href="../../solvers/sde_solve/">SDE Solvers</a></li><li><a class="tocitem" href="../../solvers/sdae_solve/">SDAE Solvers</a></li><li><a class="tocitem" href="../../solvers/rode_solve/">RODE Solvers</a></li><li><a class="tocitem" href="../../solvers/dde_solve/">DDE Solvers</a></li><li><a class="tocitem" href="../../solvers/sdde_solve/">SDDE Solvers</a></li><li><a class="tocitem" href="../../solvers/dae_solve/">DAE Solvers</a></li><li><a class="tocitem" href="../../solvers/benchmarks/">Solver Benchmarks</a></li></ul></li><li><span class="tocitem">Additional Features</span><ul><li><a class="tocitem" href="../../features/performance_overloads/">DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types</a></li><li><a class="tocitem" href="../../features/diffeq_arrays/">DiffEq-Specific Array Types</a></li><li><a class="tocitem" href="../../features/diffeq_operator/">DiffEqOperators</a></li><li><a class="tocitem" href="../../features/noise_process/">Noise Processes</a></li><li><a class="tocitem" href="../../features/linear_nonlinear/">Specifying (Non)Linear Solvers</a></li><li><a class="tocitem" href="../../features/callback_functions/">Event Handling and Callback Functions</a></li><li><a class="tocitem" href="../../features/callback_library/">Callback Library</a></li><li><a class="tocitem" href="../../features/ensemble/">Parallel Ensemble Simulations</a></li><li><a class="tocitem" href="../../features/io/">I/O: Saving and Loading Solution Data</a></li><li><a class="tocitem" href="../../features/low_dep/">Low Dependency Usage</a></li><li><a class="tocitem" href="../../features/progress_bar/">Progress Bar Integration</a></li></ul></li><li><span class="tocitem">Analysis Tools</span><ul><li><a class="tocitem" href="../parameterized_functions/">ParameterizedFunctions</a></li><li><a class="tocitem" href="../parameter_estimation/">Parameter Estimation and Bayesian Analysis</a></li><li><a class="tocitem" href="../bifurcation/">Bifurcation Analysis</a></li><li class="is-active"><a class="tocitem" href>Local Sensitivity Analysis (Automatic Differentiation)</a><ul class="internal"><li><a class="tocitem" href="#Installation"><span>Installation</span></a></li><li><a class="tocitem" href="#High-Level-Interface:-sensealg"><span>High Level Interface: <code>sensealg</code></span></a></li><li><a class="tocitem" href="#Sensitivity-Algorithms"><span>Sensitivity Algorithms</span></a></li><li class="toplevel"><a class="tocitem" href="#Lower-Level-Sensitivity-Analysis-Interfaces"><span>Lower Level Sensitivity Analysis Interfaces</span></a></li><li><a class="tocitem" href="#Local-Forward-Sensitivity-Analysis-via-ODEForwardSensitivityProblem"><span>Local Forward Sensitivity Analysis via ODEForwardSensitivityProblem</span></a></li><li><a class="tocitem" href="#Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-(Backpropogation)"><span>Adjoint Sensitivity Analysis via <code>adjoint_sensitivities</code> (Backpropogation)</span></a></li><li><a class="tocitem" href="#Second-Order-Sensitivity-Analysis-via-second_order_sensitivities-(Experimental)"><span>Second Order Sensitivity Analysis via <code>second_order_sensitivities</code> (Experimental)</span></a></li></ul></li><li><a class="tocitem" href="../global_sensitivity/">Global Sensitivity Analysis</a></li><li><a class="tocitem" href="../uncertainty_quantification/">Uncertainty Quantification</a></li><li><a class="tocitem" href="../neural_networks/">Neural Networks</a></li><li><a class="tocitem" href="../dev_and_test/">Algorithm Development and Testing</a></li></ul></li><li><span class="tocitem">Domain Modeling Tools</span><ul><li><a class="tocitem" href="../../models/multiscale/">Multi-Scale Models</a></li><li><a class="tocitem" href="../../models/physical/">Physical Models</a></li><li><a class="tocitem" href="../../models/financial/">Financial Models</a></li><li><a class="tocitem" href="../../models/chemical_reactions/">Chemical Reactions</a></li><li><a class="tocitem" href="../../models/external_modeling/">External Modeling Packages</a></li></ul></li><li><span class="tocitem">Extra Details</span><ul><li><a class="tocitem" href="../../extras/timestepping/">Timestepping Method Descriptions</a></li><li><a class="tocitem" href="../../extras/sensitivity_math/">Mathematics of Sensitivity Analysis</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Analysis Tools</a></li><li class="is-active"><a href>Local Sensitivity Analysis (Automatic Differentiation)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Local Sensitivity Analysis (Automatic Differentiation)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqDocs.jl/blob/master/docs/src/analysis/sensitivity.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="sensitivity"><a class="docs-heading-anchor" href="#sensitivity">Local Sensitivity Analysis (Automatic Differentiation)</a><a id="sensitivity-1"></a><a class="docs-heading-anchor-permalink" href="#sensitivity" title="Permalink"></a></h1><p>Sensitivity analysis, or automatic differentiation of the solver, is provided by the DiffEq suite. The model sensitivities are the derivatives of the solution <span>$u(t)$</span> with respect to the parameters. Specifically, the local sensitivity of the solution to a parameter is defined by how much the solution would change by changes in the parameter, i.e. the sensitivity of the ith independent variable to the jth parameter is <span>$\frac{\partial u_i}{\partial p_{j}}$</span>.</p><p>Sensitivity analysis serves two major purposes. On one hand, the sensitivities are diagnostics of the model which are useful for understand how it will change in accordance to changes in the parameters. But another use is simply because in many cases these derivatives are useful. Sensitivity analysis provides a cheap way to calculate the gradient of the solution which can be used in parameter estimation and other optimization tasks.</p><p>There are three types of sensitivity analysis. Local forward sensitivity analysis directly gives the gradient of the solution with respect to each parameter along the time series. The computational cost scales like <code>N*M</code>, where <code>N</code> is the number of states and <code>M</code> is the number of parameters. While this gives all of the information, it can be expensive for models with large numbers of parameters. Local adjoint sensitivity analysis solves directly for the gradient of some functional of the solution, such as a cost function or energy functional, in a manner that is cheaper when the number of parameters is large. Global Sensitivity Analysis methods are meant to be used for exploring the sensitivity over a larger domain without calculating derivatives and are covered on a different page.</p><h2 id="Installation"><a class="docs-heading-anchor" href="#Installation">Installation</a><a id="Installation-1"></a><a class="docs-heading-anchor-permalink" href="#Installation" title="Permalink"></a></h2><p>This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqSensitivity.jl:</p><pre><code class="language-julia">]add DiffEqSensitivity
using DiffEqSensitivity</code></pre><h2 id="High-Level-Interface:-sensealg"><a class="docs-heading-anchor" href="#High-Level-Interface:-sensealg">High Level Interface: <code>sensealg</code></a><a id="High-Level-Interface:-sensealg-1"></a><a class="docs-heading-anchor-permalink" href="#High-Level-Interface:-sensealg" title="Permalink"></a></h2><p>The highest level interface is provided by the function <code>solve</code>:</p><pre><code class="language-julia">solve(prob,args...;sensealg=InterpolatingAdjoint(),
      checkpoints=sol.t,kwargs...)</code></pre><p><code>solve</code> is fully compatible with automatic differentiation libraries like:</p><ul><li><a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a></li><li><a href="https://github.com/JuliaDiff/ReverseDiff.jl">ReverseDiff.jl</a></li><li><a href="https://github.com/FluxML/Tracker.jl">Tracker.jl</a></li><li><a href="https://github.com/JuliaDiff/ForwardDiff.jl">ForwardDiff.jl</a></li></ul><p>and will automatically replace any calculations of the solution&#39;s derivative with a fast method. The keyword argument <code>sensealg</code> controls the dispatch to the <code>AbstractSensitivityAlgorithm</code> used for the sensitivity calculation. Note that <code>solve</code> in an AD context does not allow higher order interpolations unless <code>sensealg=DiffEqBase.SensitivityADPassThrough()</code> is used, i.e. going back to the AD mechanism.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>ForwardDiff.jl only does forward differentiation pass through.</p></div></div><h3 id="solve-Differentiation-Examples"><a class="docs-heading-anchor" href="#solve-Differentiation-Examples">solve Differentiation Examples</a><a id="solve-Differentiation-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#solve-Differentiation-Examples" title="Permalink"></a></h3><pre><code class="language-julia">using DiffEqSensitivity, OrdinaryDiffEq, ForwardDiff, Zygote

function fiip(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]
end
p = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]
prob = ODEProblem(fiip,u0,(0.0,10.0),p)
sol = solve(prob,Tsit5())</code></pre><p>But if we want to perturb <code>u0</code> and <code>p</code> in a gradient calculation then we can do forward-mode:</p><pre><code class="language-julia">function sum_of_solution(x)
    _prob = remake(prob,u0=x[1:2],p=x[3:end])
    sum(solve(_prob,Tsit5(),saveat=0.1))
end
dx = ForwardDiff.gradient(sum_of_solution,[u0;p])</code></pre><p>or reverse-mode:</p><pre><code class="language-julia">function sum_of_solution(u0,p)
  _prob = remake(prob,u0=u0,p=p)
  sum(solve(_prob,Tsit5(),saveat=0.1,sensealg=QuadratureAdjoint()))
end
du01,dp1 = Zygote.gradient(sum_of_solution,u0,p)</code></pre><p>Or we can use the <code>u0</code> and <code>p</code> keyword argument short hands to tell it to replace <code>u0</code> and <code>p</code> by the inputs:</p><pre><code class="language-julia">du01,dp1 = Zygote.gradient((u0,p)-&gt;sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1,sensealg=QuadratureAdjoint())),u0,p)</code></pre><p>Here this computes the derivative of the output with respect to the initial condition and the the derivative with respect to the parameters respectively using the <code>QuadratureAdjoint()</code>.</p><p>When Zygote.jl is used in a larger context, these gradients are implicitly calculated and utilized. For example, the <a href="https://github.com/FluxML/Flux.jl">Flux.jl deep learning package</a> uses Zygote.jl in its training loop, so if we use <code>solve</code> in a likelihood of a Flux training loop then the derivative choice we make will be used in the optimization:</p><pre><code class="language-julia">using Flux, Plots

p = [2.2, 1.0, 2.0, 0.4] # Initial Parameter Vector
function predict_adjoint() # Our 1-layer neural network
  Array(solve(prob,Tsit5(),p=p,saveat=0.0:0.1:10.0,sensealg=BacksolveAdjoint())) # Concretize to a matrix
end
loss_adjoint() = sum(abs2,x-1 for x in predict_adjoint())

data = Iterators.repeated((), 100)
opt = ADAM(0.1)
cb = function () #callback function to observe training
  display(loss_adjoint())
  # using `remake` to re-create our `prob` with current parameters `p`
  display(plot(solve(remake(prob,p=p),Tsit5(),saveat=0.0:0.1:10.0),ylim=(0,6)))
end

# Display the ODE with the initial parameter values.
cb()

Flux.train!(loss_adjoint, Flux.params(p), data, opt, cb = cb)</code></pre><p>This optimizes the parameters from a starting point <code>p</code> where the gradients are calculated using the <code>BacksolveAdjoint</code> method.</p><p>Using this technique, we can define and mix neural networks into the differential equation:</p><pre><code class="language-julia">using DiffEqFlux, Flux, OrdinaryDiffEq, DiffEqSensitivity

u0 = Float32[0.0; 1.1]
tspan = (0.0f0,1.0f0)

ann = Chain(Dense(2,10,tanh), Dense(10,1))

p1,re = Flux.destructure(ann)
p2 = Float32[-0.5,-0.5]
p3 = [p1;p2]
ps = Flux.params(p3,u0)

function dudt_(du,u,p,t)
    x, y = u
    du[1] = re(p[1:41])(u)[1]
    du[2] = p[end-1]*y + p[end]*x
end
prob = ODEProblem(dudt_,u0,tspan,p3)

function predict_adjoint()
  Array(solve(prob,Tsit5(),u0=u0,p=p3,saveat=0.0:0.1:1.0,abstol=1e-8,
              reltol=1e-6,sensealg=InterpolatingAdjoint(checkpointing=true)))
  # ^ wrapped this in Array as done in the previous example
end
loss_adjoint() = sum(abs2,x-1 for x in predict_adjoint())

data = Iterators.repeated((), 100)
opt = ADAM(0.1)
cb = function ()
  display(loss_adjoint())
  #display(plot(solve(remake(prob,p=p3,u0=u0),Tsit5(),saveat=0.1),ylim=(0,6)))
end

# Display the ODE with the current parameter values.
cb()

Flux.train!(loss_adjoint, ps, data, opt, cb = cb)</code></pre><p>For more details and helper function for using DifferentialEquations.jl with neural networks, see the <a href="https://github.com/JuliaDiffEq/DiffEqFlux.jl">DiffEqFlux.jl repository</a>.</p><h2 id="Sensitivity-Algorithms"><a class="docs-heading-anchor" href="#Sensitivity-Algorithms">Sensitivity Algorithms</a><a id="Sensitivity-Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Sensitivity-Algorithms" title="Permalink"></a></h2><p>The following algorithm choices exist for <code>sensealg</code>. See <a href="../../extras/sensitivity_math/#sensitivity_math">the sensitivity mathematics page</a> for more details on the definition of the methods.</p><ul><li><code>ForwardSensitivity(;ADKwargs...)</code>: An implementation of continuous forward sensitivity analysis for propagating derivatives by solving the extended ODE. Only supports ODEs.</li><li><code>ForwardDiffSensitivity(;chunk_size=0,convert_tspan=true)</code>: An implementation of discrete forward sensitivity analysis through ForwardDiff.jl. This algorithm can differentiate code with callbacks when <code>convert_tspan=true</code>, but will be faster when <code>convert_tspan=false</code>.</li><li><code>BacksolveAdjoint(;checkpointing=true,ADKwargs...)</code>: An implementation of adjoint sensitivity analysis using a backwards solution of the ODE. By default this algorithm will use the values from the forward pass to perturb the backwards solution to the correct spot, allowing reduced memory with stabilization. Only supports ODEs and SDEs.</li><li><code>InterpolatingAdjoint(;checkpointing=false;ADKwargs...)</code>: The default. An implementation of adjoint sensitivity analysis which uses the interpolation of the forward solution for the reverse solve vector-Jacobian products. By default it requires a dense solution of the forward pass and will internally ignore saving arguments during the gradient calculation. When checkpointing is enabled it will only require the memory to interpolate between checkpoints. Only supports ODEs and SDEs.</li><li><code>QuadratureAdjoint(;abstol=1e-6,reltol=1e-3,compile=false,ADKwargs...)</code>: An implementation of adjoint sensitivity analysis which develops a full continuous solution of the reverse solve in order to perform a post-ODE quadrature. This method requires the the dense solution and will ignore saving arguments during the gradient calculation. The tolerances in the constructor control the inner quadrature. The inner quadrature uses a ReverseDiff vjp if autojacvec, and <code>compile=false</code> by default but can compile the tape under the same circumstances as <code>ReverseDiffVJP</code>. Only supports ODEs.</li><li><code>ReverseDiffAdjoint()</code>: An implementation of discrete adjoint sensitivity analysis using the ReverseDiff.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.</li><li><code>TrackerAdjoint()</code>: An implementation of discrete adjoint sensitivity analysis using the Tracker.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.</li><li><code>ZygoteAdjoint()</code>: An implementation of discrete adjoint sensitivity analysis using the Zygote.jl source-to-source AD directly on the differential equation solver. Currently fails.</li><li><code>SensitivityADPassThrough()</code>: Ignores all adjoint definitions and proceeds to do standard AD through the <code>solve</code> functions.</li></ul><p>The <code>ReverseDiffAdjoint()</code>, <code>TrackerAdjoint()</code>, <code>ZygoteAdjoint()</code>, and <code>SensitivityADPassThrough()</code> algorithms all offer differentiate-through-the-solver adjoints, each based on their respective automatic differentiation packages. If you&#39;re not sure which to use, <code>ReverseDiffAdjoint()</code> is generally a stable and performant best if using the CPU, while <code>TrackerAdjoint()</code> is required if you need GPUs. Note that <code>SensitivityADPassThrough()</code> is more or less an internal implementation detail. For example, <code>ReverseDiffAdjoint()</code> is implemented by invoking <code>ReverseDiff</code>&#39;s AD functionality on <code>solve(...; sensealg=SensitivityADPassThrough())</code>.</p><h3 id="Internal-Automatic-Differentiation-Options-(ADKwargs)"><a class="docs-heading-anchor" href="#Internal-Automatic-Differentiation-Options-(ADKwargs)">Internal Automatic Differentiation Options (ADKwargs)</a><a id="Internal-Automatic-Differentiation-Options-(ADKwargs)-1"></a><a class="docs-heading-anchor-permalink" href="#Internal-Automatic-Differentiation-Options-(ADKwargs)" title="Permalink"></a></h3><p>Many sensitivity algorithms share the same options for controlling internal use of automatic differentiation. The following arguments constitute the <code>ADKwargs</code>:</p><ul><li><code>autodiff</code>: Use automatic differentiation in the internal sensitivity algorithm computations. Default is <code>true</code>.</li><li><code>chunk_size</code>: Chunk size for forward mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</li><li><code>autojacvec</code>: Calculate the Jacobian-vector (forward sensitivity) or vector-Jacobian (adjoint sensitivity analysis) product via automatic differentiation with special seeding. For adjoint methods this option requires <code>autodiff=true</code>. If <code>autojacvec=false</code>, then a full Jacobian has to be computed, and this will default to using a <code>f.jac</code> function provided by the user from the problem of the forward pass. Otherwise, if <code>autodiff=true</code> and <code>autojacvec=false</code> then it will use forward-mode AD for the Jacobian, otherwise it will fall back to using a numerical approximation to the Jacobian. Additionally, if the method is an adjoint method, there are three choices which can be made explicitly:<ul><li><code>TrackerVJP</code>: Uses Tracker.jl for the vjp. Default of in-place definitions.</li><li><code>ZygoteVJP</code>: Uses Zygote.jl for the vjp. Default for out-of-place definitions.</li><li><code>ReverseDiffVJP(compile=false)</code>: Uses ReverseDiff.jl for the vjp. <code>compile</code> is a boolean for whether to precompile the tape, which should only be done if there are no branches (<code>if</code> or <code>while</code> statements) in the <code>f</code> function. When applicable, <code>ReverseDiffVJP(true)</code> is the fastest method, and then <code>ReverseDiffVJP(false)</code> is the second fastest, but this method is not compatible with third party libraries like Flux.jl, FFTW.jl, etc. (only linear algebra and basic mathematics is supported) so it should be swapped in only as an optimization.</li></ul></li></ul><p>Note that the Jacobian-vector products and vector-Jacobian products can be directly specified by the user using the <a href="../../features/performance_overloads/#performance_overloads">performance overloads</a>.</p><h3 id="Choosing-a-Sensitivity-Algorithm"><a class="docs-heading-anchor" href="#Choosing-a-Sensitivity-Algorithm">Choosing a Sensitivity Algorithm</a><a id="Choosing-a-Sensitivity-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Choosing-a-Sensitivity-Algorithm" title="Permalink"></a></h3><p>For an analysis of which methods will be most efficient for computing the solution derivatives for a given problem, consult our analysis <a href="https://arxiv.org/abs/1812.01892">in this arxiv paper</a>. A general rule of thumb is:</p><ul><li><code>ForwardDiffSensitivity</code> is the fastest for differential equations with small numbers of parameters (&lt;100) and can be used on any differential equation solver that is native Julia.</li><li>Adjoint senstivity analysis is the fastest when the number of parameters is sufficiently large. There are three configurations of note. Using <code>QuadratureAdjoint</code> is the fastest for small systems, <code>BacksolveAdjoint</code> uses the least memory but on very stiff problems it may be unstable and require a lot of checkpoints, while <code>InterpolatingAdjoint</code> is in the middle, allowing checkpointing to control total memory use.</li><li>The methods which use automatic differentiation (<code>ReverseDiffAdjoint</code>, <code>TrackerAdjoint</code>, <code>ForwardDiffSensitivity</code>, and <code>ZygoteAdjoint</code>) support the full range of DifferentialEquations.jl features (SDEs, DDEs, events, etc.), but only work on native Julia solvers. The methods which utilize altered differential equation systems only work on ODEs (without events), but work on any ODE solver.</li><li>For non-ODEs with large numbers of parameters, <code>TrackerAdjoint</code> in out-of-place form may be the best performer.</li><li><code>TrackerAdjoint</code> is able to use a <code>TrackedArray</code> form with out-of-place functions <code>du = f(u,p,t)</code> but requires an <code>Array{TrackedReal}</code> form for <code>f(du,u,p,t)</code> mutating <code>du</code>. The latter has much more overhead, and should be avoided if possible. Thus if solving non-ODEs with lots of parameters, using <code>TrackerAdjoint</code> with an out-of-place definition may be the current best option.</li></ul><h1 id="Lower-Level-Sensitivity-Analysis-Interfaces"><a class="docs-heading-anchor" href="#Lower-Level-Sensitivity-Analysis-Interfaces">Lower Level Sensitivity Analysis Interfaces</a><a id="Lower-Level-Sensitivity-Analysis-Interfaces-1"></a><a class="docs-heading-anchor-permalink" href="#Lower-Level-Sensitivity-Analysis-Interfaces" title="Permalink"></a></h1><p>While the high level interface is sufficient for interfacing with automatic differentiation, for example allowing compatibility with neural network libraries, in some cases one may want more control over the way the sensitivities are calculated in order to squeeze out every ounce of optimization. If you&#39;re that user, then this section of the docs is for you.</p><h2 id="Local-Forward-Sensitivity-Analysis-via-ODEForwardSensitivityProblem"><a class="docs-heading-anchor" href="#Local-Forward-Sensitivity-Analysis-via-ODEForwardSensitivityProblem">Local Forward Sensitivity Analysis via ODEForwardSensitivityProblem</a><a id="Local-Forward-Sensitivity-Analysis-via-ODEForwardSensitivityProblem-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Forward-Sensitivity-Analysis-via-ODEForwardSensitivityProblem" title="Permalink"></a></h2><p>Local forward sensitivity analysis gives a solution along with a timeseries of the sensitivities. Thus if one wishes to have a derivative at every possible time point, directly utilizing the <code>ODELocalSensitivityProblem</code> can be more efficient.</p><h3 id="ODEForwardSensitivityProblem-Syntax"><a class="docs-heading-anchor" href="#ODEForwardSensitivityProblem-Syntax">ODEForwardSensitivityProblem Syntax</a><a id="ODEForwardSensitivityProblem-Syntax-1"></a><a class="docs-heading-anchor-permalink" href="#ODEForwardSensitivityProblem-Syntax" title="Permalink"></a></h3><p><code>ODELocalSensitivityProblem</code> is similar to an <code>ODEProblem</code>, but takes an <code>AbstractSensitivityAlgorithm</code> that describes how to append the forward sensitivity equation calculation to the time evolution to simultaneously compute the derivative of the solution with respect to parameters.</p><pre><code class="language-julia">ODEForwardSensitivityProblem(f::SciMLBase.AbstractODEFunction,u0,
                             tspan,p=nothing,
                             sensealg::AbstractForwardSensitivityAlgorithm = ForwardSensitivity();
                             kwargs...)</code></pre><p>Once constructed, this problem can be used in <code>solve</code>. The solution can be deconstructed into the ODE solution and sensitivities parts using the <code>extract_local_sensitivities</code> function, with the following dispatches:</p><pre><code class="language-julia">extract_local_sensitivities(sol, asmatrix::Val=Val(false)) # Decompose the entire time series
extract_local_sensitivities(sol, i::Integer, asmatrix::Val=Val(false)) # Decompose sol[i]
extract_local_sensitivities(sol, t::Union{Number,AbstractVector}, asmatrix::Val=Val(false)) # Decompose sol(t)</code></pre><p>For information on the mathematics behind these calculations, consult <a href="../../extras/sensitivity_math/#sensitivity_math">the sensitivity math page</a></p><h3 id="Example-using-an-ODELocalSensitivityProblem"><a class="docs-heading-anchor" href="#Example-using-an-ODELocalSensitivityProblem">Example using an ODELocalSensitivityProblem</a><a id="Example-using-an-ODELocalSensitivityProblem-1"></a><a class="docs-heading-anchor-permalink" href="#Example-using-an-ODELocalSensitivityProblem" title="Permalink"></a></h3><p>To define a sensitivity problem, simply use the <code>ODELocalSensitivityProblem</code> type instead of an ODE type. For example, we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by:</p><pre><code class="language-julia">function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + u[1]*u[2]
end

p = [1.5,1.0,3.0]
prob = ODELocalSensitivityProblem(f,[1.0;1.0],(0.0,10.0),p)</code></pre><p>This generates a problem which the ODE solvers can solve:</p><pre><code class="language-julia">sol = solve(prob,DP8())</code></pre><p>Note that the solution is the standard ODE system and the sensitivity system combined. We can use the following helper functions to extract the sensitivity information:</p><pre><code class="language-julia">x,dp = extract_local_sensitivities(sol)
x,dp = extract_local_sensitivities(sol,i)
x,dp = extract_local_sensitivities(sol,t)</code></pre><p>In each case, <code>x</code> is the ODE values and <code>dp</code> is the matrix of sensitivities The first gives the full timeseries of values and <code>dp[i]</code> contains the time series of the sensitivities of all components of the ODE with respect to <code>i</code>th parameter. The second returns the <code>i</code>th time step, while the third interpolates to calculate the sensitivities at time <code>t</code>. For example, if we do:</p><pre><code class="language-julia">x,dp = extract_local_sensitivities(sol)
da = dp[1]</code></pre><p>then <code>da</code> is the timeseries for <span>$\frac{\partial u(t)}{\partial p}$</span>. We can plot this</p><pre><code class="language-julia">plot(sol.t,da&#39;,lw=3)</code></pre><p>transposing so that the rows (the timeseries) is plotted.</p><p><img src="../../assets/sensitivityplot.png" alt="Local Sensitivity Solution"/></p><p>Here we see that there is a periodicity to the sensitivity which matches the periodicity of the Lotka-Volterra solutions. However, as time goes on the sensitivity increases. This matches the analysis of Wilkins in Sensitivity Analysis for Oscillating Dynamical Systems.</p><p>We can also quickly see that these values are equivalent to those given by automatic differentiation and numerical differentiation through the ODE solver:</p><pre><code class="language-julia">using ForwardDiff, Calculus
function test_f(p)
  prob = ODEProblem(f,eltype(p).([1.0,1.0]),eltype(p).((0.0,10.0)),p)
  solve(prob,Vern9(),abstol=1e-14,reltol=1e-14,save_everystep=false)[end]
end

p = [1.5,1.0,3.0]
fd_res = ForwardDiff.jacobian(test_f,p)
calc_res = Calculus.finite_difference_jacobian(test_f,p)</code></pre><p>Here we just checked the derivative at the end point.</p><h3 id="Internal-representation"><a class="docs-heading-anchor" href="#Internal-representation">Internal representation</a><a id="Internal-representation-1"></a><a class="docs-heading-anchor-permalink" href="#Internal-representation" title="Permalink"></a></h3><p>For completeness, we detail the internal representation. When using ForwardDiffSensitivity, the representation is with <code>Dual</code> numbers under the standard interpretation. The values for the ODE&#39;s solution at time <code>i</code> are the <code>ForwardDiff.value.(sol[i])</code> portions, and the derivative with respect to parameter <code>j</code> is given by <code>ForwardDiff.partials.(sol[i])[j]</code>.</p><p>When using ForwardSensitivity, the solution to the ODE are the first <code>n</code> components of the solution. This means we can grab the matrix of solution values like:</p><pre><code class="language-julia">x = sol[1:sol.prob.indvars,:]</code></pre><p>Since each sensitivity is a vector of derivatives for each function, the sensitivities are each of size <code>sol.prob.indvars</code>. We can pull out the parameter sensitivities from the solution as follows:</p><pre><code class="language-julia">da = sol[sol.prob.indvars+1:sol.prob.indvars*2,:]
db = sol[sol.prob.indvars*2+1:sol.prob.indvars*3,:]
dc = sol[sol.prob.indvars*3+1:sol.prob.indvars*4,:]</code></pre><p>This means that <code>da[1,i]</code> is the derivative of the <code>x(t)</code> by the parameter <code>a</code> at time <code>sol.t[i]</code>. Note that all of the functionality available to ODE solutions is available in this case, including interpolations and plot recipes (the recipes will plot the expanded system).</p><h2 id="Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-(Backpropogation)"><a class="docs-heading-anchor" href="#Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-(Backpropogation)">Adjoint Sensitivity Analysis via <code>adjoint_sensitivities</code> (Backpropogation)</a><a id="Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-(Backpropogation)-1"></a><a class="docs-heading-anchor-permalink" href="#Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-(Backpropogation)" title="Permalink"></a></h2><p>Adjoint sensitivity analysis is used to find the gradient of the solution with respect to some functional of the solution. In many cases this is used in an optimization problem to return the gradient with respect to some cost function. It is equivalent to &quot;backpropagation&quot; or reverse-mode automatic differentiation of a differential equation.</p><p>Using <code>adjoint_sensitivities</code> directly let&#39;s you do three things. One it can allow you to be more efficient, since the sensitivity calculation can be done directly on a cost function, avoiding the overhead of building the derivative of the full concretized solution. It can also allow you to be more efficient by directly controlling the forward solve that is then reversed over. Lastly, it allows one to define a continuous cost function on the continuous solution, instead of just at discrete data points.</p><h3 id="Syntax"><a class="docs-heading-anchor" href="#Syntax">Syntax</a><a id="Syntax-1"></a><a class="docs-heading-anchor-permalink" href="#Syntax" title="Permalink"></a></h3><p>There are two forms. For discrete adjoints, the form is:</p><pre><code class="language-julia">du0,dp = adjoint_sensitivities(sol,alg,dg,ts;sensealg=InterpolatingAdjoint(),
                               checkpoints=sol.t,kwargs...)</code></pre><p>where <code>alg</code> is the ODE algorithm to solve the adjoint problem, <code>dg</code> is the jump function, <code>sensealg</code> is the sensitivity algorithm, and <code>ts</code> is the time points for data. <code>dg</code> is given by:</p><pre><code class="language-julia">dg(out,u,p,t,i)</code></pre><p>which is the in-place gradient of the cost functional <code>g</code> at time point <code>ts[i]</code> with <code>u=u(t)</code>.</p><p>For continuous functionals, the form is:</p><pre><code class="language-julia">du0,dp = adjoint_sensitivities(sol,alg,g,nothing,(dgdu,dgdp);sensealg=InterpolatingAdjoint(),
                               checkpoints=sol.t,,kwargs...)</code></pre><p>for the cost functional</p><pre><code class="language-julia">g(u,p,t)</code></pre><p>with in-place gradient</p><pre><code class="language-julia">dgdu(out,u,p,t)
dgdp(out,u,p,t)</code></pre><p>If the gradient is omitted, i.e.</p><pre><code class="language-julia">du0,dp = adjoint_sensitivities(sol,alg,g,nothing;kwargs...)</code></pre><p>then we assume <code>dgdp</code> is zero and <code>dgdu</code> will be computed automatically using ForwardDiff or finite differencing, depending on the <code>autodiff</code> setting in the <code>AbstractSensitivityAlgorithm</code>. Note that the keyword arguments are passed to the internal ODE solver for solving the adjoint problem.</p><h3 id="Example-discrete-adjoints-on-a-cost-function"><a class="docs-heading-anchor" href="#Example-discrete-adjoints-on-a-cost-function">Example discrete adjoints on a cost function</a><a id="Example-discrete-adjoints-on-a-cost-function-1"></a><a class="docs-heading-anchor-permalink" href="#Example-discrete-adjoints-on-a-cost-function" title="Permalink"></a></h3><p>In this example we will show solving for the adjoint sensitivities of a discrete cost functional. First let&#39;s solve the ODE and get a high quality continuous solution:</p><pre><code class="language-julia">function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + u[1]*u[2]
end

p = [1.5,1.0,3.0]
prob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)
sol = solve(prob,Vern9(),abstol=1e-10,reltol=1e-10)</code></pre><p>Now let&#39;s calculate the sensitivity of the <span>$\ell_2$</span> error against 1 at evenly spaced points in time, that is:</p><p class="math-container">\[L(u,p,t)=\sum_{i=1}^{n}\frac{\Vert1-u(t_{i},p)\Vert^{2}}{2}\]</p><p>for <span>$t_i = 0.5i$</span>. This is the assumption that the data is <code>data[i]=1.0</code>. For this function, notice we have that:</p><p class="math-container">\[\begin{aligned}
dg_{1}&amp;=1-u_{1} \\
dg_{2}&amp;=1-u_{2} \\
&amp; \quad \vdots
\end{aligned}\]</p><p>and thus:</p><pre><code class="language-julia">dg(out,u,p,t,i) = (out.=1.0.-u)</code></pre><p>Also, we can omit <code>dgdp</code>, because the cost function doesn&#39;t dependent on <code>p</code>. If we had data, we&#39;d just replace <code>1.0</code> with <code>data[i]</code>. To get the adjoint sensitivities, call:</p><pre><code class="language-julia">ts = 0:0.5:10
res = adjoint_sensitivities(sol,Vern9(),dg,ts,abstol=1e-14,
                            reltol=1e-14)</code></pre><p>This is super high accuracy. As always, there&#39;s a tradeoff between accuracy and computation time. We can check this almost exactly matches the autodifferentiation and numerical differentiation results:</p><pre><code class="language-julia">using ForwardDiff,Calculus
function G(p)
  tmp_prob = remake(prob,u0=convert.(eltype(p),prob.u0),p=p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=ts)
  A = convert(Array,sol)
  sum(((1 .- A).^2)./2)
end
G([1.5,1.0,3.0])
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])
res4 = Flux.Tracker.gradient(G,[1.5,1.0,3.0])
res5 = ReverseDiff.gradient(G,[1.5,1.0,3.0])</code></pre><p>and see this gives the same values.</p><h3 id="Example-controlling-adjoint-method-choices-and-checkpointing"><a class="docs-heading-anchor" href="#Example-controlling-adjoint-method-choices-and-checkpointing">Example controlling adjoint method choices and checkpointing</a><a id="Example-controlling-adjoint-method-choices-and-checkpointing-1"></a><a class="docs-heading-anchor-permalink" href="#Example-controlling-adjoint-method-choices-and-checkpointing" title="Permalink"></a></h3><p>In the previous examples, all calculations were done using the interpolating method. This maximizes speed but at a cost of requiring a dense <code>sol</code>. If it is not possible to hold a dense forward solution in memory, then one can use checkpointing. For example:</p><pre><code class="language-julia">ts = [0.0,0.2,0.5,0.7]
sol = solve(prob,Vern9(),saveat=ts)</code></pre><p>Creates a non-dense solution with checkpoints at <code>[0.0,0.2,0.5,0.7]</code>. Now we can do:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),dg,t,
                            sensealg=InterpolatingAdjoint(checkpointing=true))</code></pre><p>When grabbing a Jacobian value during the backwards solution, it will no longer interpolate to get the value. Instead, it will start a forward solution at the nearest checkpoint to build local interpolants in a way that conserves memory. By default the checkpoints are at <code>sol.t</code>, but we can override this:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),dg,t,
                            sensealg=InterpolatingAdjoint(checkpointing=true),
                            checkpoints = [0.0,0.5])</code></pre><h3 id="Example-continuous-adjoints-on-an-energy-functional"><a class="docs-heading-anchor" href="#Example-continuous-adjoints-on-an-energy-functional">Example continuous adjoints on an energy functional</a><a id="Example-continuous-adjoints-on-an-energy-functional-1"></a><a class="docs-heading-anchor-permalink" href="#Example-continuous-adjoints-on-an-energy-functional" title="Permalink"></a></h3><p>In this case we&#39;d like to calculate the adjoint sensitivity of the scalar energy functional:</p><p class="math-container">\[G(u,p)=\int_{0}^{T}\frac{\sum_{i=1}^{n}u_{i}^{2}(t)}{2}dt\]</p><p>which is:</p><pre><code class="language-julia">g(u,p,t) = (sum(u).^2) ./ 2</code></pre><p>Notice that the gradient of this function with respect to the state <code>u</code> is:</p><pre><code class="language-julia">function dg(out,u,p,t)
  out[1]= u[1] + u[2]
  out[2]= u[1] + u[2]
end</code></pre><p>To get the adjoint sensitivities, we call:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),g,nothing,dg,abstol=1e-8,
                                 reltol=1e-8,iabstol=1e-8,ireltol=1e-8)</code></pre><p>Notice that we can check this against autodifferentiation and numerical differentiation as follows:</p><pre><code class="language-julia">function G(p)
  tmp_prob = remake(prob,p=p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)
  res,err = quadgk((t)-&gt; (sum(sol(t)).^2)./2,0.0,10.0,abstol=1e-14,reltol=1e-10)
  res
end
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])</code></pre><h3 id="Applicability-of-Backsolve-and-Caution"><a class="docs-heading-anchor" href="#Applicability-of-Backsolve-and-Caution">Applicability of Backsolve and Caution</a><a id="Applicability-of-Backsolve-and-Caution-1"></a><a class="docs-heading-anchor-permalink" href="#Applicability-of-Backsolve-and-Caution" title="Permalink"></a></h3><p>When <code>BacksolveAdjoint</code> is applicable it is a fast method and requires the least memory. However, one must be cautious because not all ODEs are stable under backwards integration by the majority of ODE solvers. An example of such an equation is the Lorenz equation. Notice that if one solves the Lorenz equation forward and then in reverse with any adaptive time step and non-reversible integrator, then the backwards solution diverges from the forward solution. As a quick demonstration:</p><pre><code class="language-julia">using Sundials
function lorenz(du,u,p,t)
 du[1] = 10.0*(u[2]-u[1])
 du[2] = u[1]*(28.0-u[3]) - u[2]
 du[3] = u[1]*u[2] - (8/3)*u[3]
end
u0 = [1.0;0.0;0.0]
tspan = (0.0,100.0)
prob = ODEProblem(lorenz,u0,tspan)
sol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)
prob2 = ODEProblem(lorenz,sol[end],(100.0,0.0))
sol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)
@show sol[end]-u0 #[-3.22091, -1.49394, 21.3435]</code></pre><p>Thus one should check the stability of the backsolve on their type of problem before enabling this method. Additionally, using checkpointing with backsolve can be a low memory way to stabilize it.</p><h2 id="Second-Order-Sensitivity-Analysis-via-second_order_sensitivities-(Experimental)"><a class="docs-heading-anchor" href="#Second-Order-Sensitivity-Analysis-via-second_order_sensitivities-(Experimental)">Second Order Sensitivity Analysis via <code>second_order_sensitivities</code> (Experimental)</a><a id="Second-Order-Sensitivity-Analysis-via-second_order_sensitivities-(Experimental)-1"></a><a class="docs-heading-anchor-permalink" href="#Second-Order-Sensitivity-Analysis-via-second_order_sensitivities-(Experimental)" title="Permalink"></a></h2><p>Second order sensitivity analysis is used for the fast calculation of Hessian matrices. Currently there are two functions available. The first, <code>second_order_sensitivities</code>, calculates the Hessian of the solution to a differential equation with respect to a loss function on the solution <code>loss(sol)</code>. The second calculates Hessian-vector products, i.e. <code>H*v</code>, with respect to such a loss. The syntax is:</p><pre><code class="language-julia">H  = second_order_sensitivities(loss,prob,alg,args...;kwargs...)
Hv  = second_order_sensitivity_product(loss,v,prob,alg,args...;kwargs...)</code></pre><p>These methods utilize what is known as forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a fast computation.</p><h3 id="Example-second-order-sensitivity-analysis-calculation"><a class="docs-heading-anchor" href="#Example-second-order-sensitivity-analysis-calculation">Example second order sensitivity analysis calculation</a><a id="Example-second-order-sensitivity-analysis-calculation-1"></a><a class="docs-heading-anchor-permalink" href="#Example-second-order-sensitivity-analysis-calculation" title="Permalink"></a></h3><pre><code class="language-julia">using DiffEqSensitivity, OrdinaryDiffEq, ForwardDiff
using Test

function lotka!(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]
end

p = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]
prob = ODEProblem(lotka!,u0,(0.0,10.0),p)
loss(sol) = sum(sol)
v = ones(4)

H  = second_order_sensitivities(loss,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)
Hv = second_order_sensitivity_product(loss,v,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../bifurcation/">« Bifurcation Analysis</a><a class="docs-footer-nextpage" href="../global_sensitivity/">Global Sensitivity Analysis »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Thursday 6 May 2021 22:48">Thursday 6 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
