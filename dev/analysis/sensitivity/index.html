<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Local Sensitivity Analysis (Automatic Differentiation) · DifferentialEquations.jl</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-90474609-3', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link rel="canonical" href="https://docs.juliadiffeq.org/stable/analysis/sensitivity/index.html"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DifferentialEquations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">DifferentialEquations.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/ode_example/">Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/advanced_ode_example/">Solving Stiff Equations</a></li><li><a class="tocitem" href="../../tutorials/sde_example/">Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/rode_example/">Random Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/dde_example/">Delay Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/dae_example/">Differential Algebraic Equations</a></li><li><a class="tocitem" href="../../tutorials/discrete_stochastic_example/">Discrete Stochastic (Gillespie) Equations</a></li><li><a class="tocitem" href="../../tutorials/jump_diffusion/">Jump Diffusion Equations</a></li><li><a class="tocitem" href="../../tutorials/bvp_example/">Boundary Value Problems</a></li><li><a class="tocitem" href="../../tutorials/additional/">Additional Tutorials</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../../basics/overview/">Overview of DifferentialEquations.jl</a></li><li><a class="tocitem" href="../../basics/common_solver_opts/">Common Solver Options</a></li><li><a class="tocitem" href="../../basics/solution/">Solution Handling</a></li><li><a class="tocitem" href="../../basics/plot/">Plot Functions</a></li><li><a class="tocitem" href="../../basics/integrator/">Integrator Interface</a></li><li><a class="tocitem" href="../../basics/problem/">Problem Interface</a></li><li><a class="tocitem" href="../../basics/faq/">Frequently Asked Questions</a></li><li><a class="tocitem" href="../../basics/compatibility_chart/">Solver Compatibility Chart</a></li></ul></li><li><span class="tocitem">Problem Types</span><ul><li><a class="tocitem" href="../../types/discrete_types/">Discrete Problems</a></li><li><a class="tocitem" href="../../types/ode_types/">ODE Problems</a></li><li><a class="tocitem" href="../../types/dynamical_types/">Dynamical, Hamiltonian and 2nd Order ODE Problems</a></li><li><a class="tocitem" href="../../types/split_ode_types/">Split ODE Problems</a></li><li><a class="tocitem" href="../../types/steady_state_types/">Steady State Problems</a></li><li><a class="tocitem" href="../../types/bvp_types/">BVP Problems</a></li><li><a class="tocitem" href="../../types/sde_types/">SDE Problems</a></li><li><a class="tocitem" href="../../types/rode_types/">RODE Problems</a></li><li><a class="tocitem" href="../../types/dde_types/">DDE Problems</a></li><li><a class="tocitem" href="../../types/dae_types/">DAE Problems</a></li><li><a class="tocitem" href="../../types/jump_types/">Jump Problems</a></li></ul></li><li><span class="tocitem">Solver Algorithms</span><ul><li><a class="tocitem" href="../../solvers/discrete_solve/">Discrete Solvers</a></li><li><a class="tocitem" href="../../solvers/ode_solve/">ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/dynamical_solve/">Dynamical, Hamiltonian, and 2nd Order ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/split_ode_solve/">Split ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/steady_state_solve/">Steady State Solvers</a></li><li><a class="tocitem" href="../../solvers/bvp_solve/">BVP Solvers</a></li><li><a class="tocitem" href="../../solvers/jump_solve/">Jump Problem Solvers</a></li><li><a class="tocitem" href="../../solvers/sde_solve/">SDE Solvers</a></li><li><a class="tocitem" href="../../solvers/rode_solve/">RODE Solvers</a></li><li><a class="tocitem" href="../../solvers/dde_solve/">DDE Solvers</a></li><li><a class="tocitem" href="../../solvers/dae_solve/">DAE Solvers</a></li><li><a class="tocitem" href="../../solvers/benchmarks/">Solver Benchmarks</a></li></ul></li><li><span class="tocitem">Additional Features</span><ul><li><a class="tocitem" href="../../features/performance_overloads/">DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types</a></li><li><a class="tocitem" href="../../features/diffeq_arrays/">DiffEq-Specific Array Types</a></li><li><a class="tocitem" href="../../features/diffeq_operator/">DiffEqOperators</a></li><li><a class="tocitem" href="../../features/noise_process/">Noise Processes</a></li><li><a class="tocitem" href="../../features/linear_nonlinear/">Specifying (Non)Linear Solvers</a></li><li><a class="tocitem" href="../../features/callback_functions/">Event Handling and Callback Functions</a></li><li><a class="tocitem" href="../../features/callback_library/">Callback Library</a></li><li><a class="tocitem" href="../../features/ensemble/">Parallel Ensemble Simulations</a></li><li><a class="tocitem" href="../../features/io/">I/O: Saving and Loading Solution Data</a></li><li><a class="tocitem" href="../../features/low_dep/">Low Dependency Usage</a></li><li><a class="tocitem" href="../../features/progress_bar/">Juno Progress Bar Integration</a></li></ul></li><li><span class="tocitem">Analysis Tools</span><ul><li><a class="tocitem" href="../parameterized_functions/">ParameterizedFunctions</a></li><li><a class="tocitem" href="../parameter_estimation/">Parameter Estimation and Bayesian Analysis</a></li><li><a class="tocitem" href="../bifurcation/">Bifurcation Analysis</a></li><li class="is-active"><a class="tocitem" href>Local Sensitivity Analysis (Automatic Differentiation)</a><ul class="internal"><li><a class="tocitem" href="#Installation-1"><span>Installation</span></a></li><li><a class="tocitem" href="#Efficiency-of-the-Different-Methods-1"><span>Efficiency of the Different Methods</span></a></li><li><a class="tocitem" href="#Local-Forward-Sensitivity-Analysis-1"><span>Local Forward Sensitivity Analysis</span></a></li><li><a class="tocitem" href="#Adjoint-Sensitivity-Analysis-1"><span>Adjoint Sensitivity Analysis</span></a></li></ul></li><li><a class="tocitem" href="../global_sensitivity/">Global Sensitivity Analysis</a></li><li><a class="tocitem" href="../uncertainty_quantification/">Uncertainty Quantification</a></li><li><a class="tocitem" href="../neural_networks/">Neural Networks</a></li><li><a class="tocitem" href="../dev_and_test/">Algorithm Development and Testing</a></li></ul></li><li><span class="tocitem">Domain Modeling Tools</span><ul><li><a class="tocitem" href="../../models/multiscale/">Multi-Scale Models</a></li><li><a class="tocitem" href="../../models/physical/">Physical Models</a></li><li><a class="tocitem" href="../../models/financial/">Financial Models</a></li><li><a class="tocitem" href="../../models/biological/">Chemical Reaction Models</a></li><li><a class="tocitem" href="../../models/external_modeling/">External Modeling Packages</a></li></ul></li><li><span class="tocitem">APIs</span><ul><li><a class="tocitem" href="../../apis/diffeqbio/">DiffEqBiological.jl API</a></li></ul></li><li><span class="tocitem">Extra Details</span><ul><li><a class="tocitem" href="../../extras/timestepping/">Timestepping Method Descriptions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Analysis Tools</a></li><li class="is-active"><a href>Local Sensitivity Analysis (Automatic Differentiation)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Local Sensitivity Analysis (Automatic Differentiation)</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaDiffEq/DiffEqDocs.jl/blob/master/docs/src/analysis/sensitivity.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="sensitivity-1"><a class="docs-heading-anchor" href="#sensitivity-1">Local Sensitivity Analysis (Automatic Differentiation)</a><a class="docs-heading-anchor-permalink" href="#sensitivity-1" title="Permalink"></a></h1><p>Sensitivity analysis for ODE models is provided by the DiffEq suite. The model sensitivities are the derivatives of the solution <span>$u(t)$</span> with respect to the parameters. Specifically, the local sensitivity of the solution to a parameter is defined by how much the solution would change by changes in the parameter, i.e. the sensitivity of the ith independent variable to the jth parameter is <span>$\frac{\partial u_i}{\partial p_{j}}$</span>.</p><p>Sensitivity analysis serves two major purposes. On one hand, the sensitivities are diagnostics of the model which are useful for understand how it will change in accordance to changes in the parameters. But another use is simply because in many cases these derivatives are useful. Sensitivity analysis provides a cheap way to calculate the gradient of the solution which can be used in parameter estimation and other optimization tasks.</p><p>There are three types of sensitivity analysis. Local forward sensitivity analysis directly gives the gradient of the solution with respect to each parameter along the time series. The computational cost scales like <code>N*M</code>, where <code>N</code> is the number of states and <code>M</code> is the number of parameters. While this gives all of the information, it can be expensive for models with large numbers of parameters. Local adjoint sensitivity analysis solves directly for the gradient of some functional of the solution, such as a cost function or energy functional, in a manner that is cheaper when the number of parameters is large. Global Sensitivity Analysis methods are meant to be used for exploring the sensitivity over a larger domain without calculating derivatives and are covered on a different page.</p><h2 id="Installation-1"><a class="docs-heading-anchor" href="#Installation-1">Installation</a><a class="docs-heading-anchor-permalink" href="#Installation-1" title="Permalink"></a></h2><p>This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqSensitivty.jl:</p><pre><code class="language-julia">]add DiffEqSensitivity
using DiffEqSensitivity</code></pre><h2 id="Efficiency-of-the-Different-Methods-1"><a class="docs-heading-anchor" href="#Efficiency-of-the-Different-Methods-1">Efficiency of the Different Methods</a><a class="docs-heading-anchor-permalink" href="#Efficiency-of-the-Different-Methods-1" title="Permalink"></a></h2><p>For an analysis of which methods will be most efficient for computing the solution derivatives for a given problem, consult our analysis <a href="https://arxiv.org/abs/1812.01892">in this arxiv paper</a>. A general rule of thumb is:</p><ul><li>Discrete Forward Sensitivity Analysis via ForwardDiff.jl is the fastest for ODEs with small numbers of parameters (&lt;100).</li><li>Adjoint senstivity analysis is the fastest when the number of parameters is sufficiently large. There are three configurations of note. Using <code>backsolve</code> is the fastest and uses the least memory, but is not guaranteed to be stable. Checkpointing is the slowest but uses O(1) memory and is stable. Interpolating is the second fastest, is stable, but requires the ability to hold the full forward solution and its interpolation in memory.</li><li>The methods which use automatic differentiation support the full range of DifferentialEquations.jl features (SDEs, DDEs, events, etc.), but only work on native Julia solvers. The methods which utilize altered ODE systems only work on ODEs (without events), but work on any ODE solver.</li></ul><h2 id="Local-Forward-Sensitivity-Analysis-1"><a class="docs-heading-anchor" href="#Local-Forward-Sensitivity-Analysis-1">Local Forward Sensitivity Analysis</a><a class="docs-heading-anchor-permalink" href="#Local-Forward-Sensitivity-Analysis-1" title="Permalink"></a></h2><p>Local forward sensitivity analysis gives a solution along with a timeseries of the sensitivities along the solution.</p><h3 id="Discrete-Local-Forward-Sensitivity-Analysis-via-ForwardDiff.jl-1"><a class="docs-heading-anchor" href="#Discrete-Local-Forward-Sensitivity-Analysis-via-ForwardDiff.jl-1">Discrete Local Forward Sensitivity Analysis via ForwardDiff.jl</a><a class="docs-heading-anchor-permalink" href="#Discrete-Local-Forward-Sensitivity-Analysis-via-ForwardDiff.jl-1" title="Permalink"></a></h3><p>This method is the application of ForwardDiff.jl numbers to the ODE solver. This is done simply by making the <code>u0</code> state vector a vector of Dual numbers, and multiple dispatch then allows the internals of the solver to propagate the derivatives along the solution.</p><h4 id="Examples-using-ForwardDiff.jl-1"><a class="docs-heading-anchor" href="#Examples-using-ForwardDiff.jl-1">Examples using ForwardDiff.jl</a><a class="docs-heading-anchor-permalink" href="#Examples-using-ForwardDiff.jl-1" title="Permalink"></a></h4><p>The easiest way to use ForwardDiff.jl for local forward sensitivity analysis is to simply put the ODE solve inside of a function which you would like to differentiate. For example, let&#39;s define the ODE system for the Lotka-Volterra equations:</p><pre><code class="language-julia">function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + u[1]*u[2]
end

p = [1.5,1.0,3.0]
u0 = [1.0;1.0]
tspan = (0.0, 10.0)
prob = ODEProblem(f,u0,tspan,p)</code></pre><p>Let&#39;s say we wanted to get the derivative of the final value w.r.t. each of the parameters. We can define the following function:</p><pre><code class="language-julia">function test_f(p)
  _prob = remake(prob;u0=convert.(eltype(p),prob.u0),p=p)
  solve(_prob,Vern9(),save_everystep=false)[end]
end</code></pre><p>What this function does is use the <code>remake</code> function <a href="http://docs.juliadiffeq.org/dev/basics/problem/#Modification-of-problem-types-1">from the Problem Interface page</a> to generate a new ODE problem with the new parameters, solves it, and returns the solution at the final time point. Notice that it takes care to make sure that the type of <code>u0</code> matches the type of <code>p</code>. This is because ForwardDiff.jl will want to use Dual numbers, and thus to propagate the Duals throughout the solver we need to make sure the initial condition is also of the type of Dual number. On this function we can call ForwardDiff.jl and it will return the derivatives we wish to calculate:</p><pre><code class="language-julia">using ForwardDiff
fd_res = ForwardDiff.jacobian(test_f,p)</code></pre><p>If we would like to get the solution and the value at the time point at the same time, we can use <a href="https://github.com/JuliaDiff/DiffResults.jl">DiffResults.jl</a>. For example, the following uses a single ODE solution to calculate the value at the end point and its parameter Jacobian:</p><pre><code class="language-julia">using DiffResults
res = DiffResults.JacobianResult(u0,p) # Build the results object
DiffResults.jacobian!(res,p) # Populate it with the results
val = DiffResults.value(res) # This is the sol[end]
jac = DiffResults.jacobian(res) # This is dsol/dp</code></pre><p>If we would like to get the time series, we can do so by seeding the dual numbers directly. To do this, we use the <code>Dual</code> constructor. The first value is the value of the parameter. The second is a tuple of the derivatives. For each value we want to take the derivative by, we seed a derivative with a 1 in a unique index. For example, we can build our parameter vector like:</p><pre><code class="language-julia">using ForwardDiff: Dual
struct MyTag end
p1dual = Dual{MyTag}(1.5, (1.0, 0.0, 0.0))
p2dual = Dual{MyTag}(1.0, (0.0, 1.0, 0.0))
p3dual = Dual{MyTag}(3.0, (0.0, 0.0, 1.0))
pdual = [p1dual, p2dual, p3dual]</code></pre><p>or equivalently using the <code>seed_duals</code> convenience function:</p><pre><code class="language-julia">function seed_duals(x::AbstractArray{V},::Type{T},
                    ::ForwardDiff.Chunk{N} = ForwardDiff.Chunk(x)) where {V,T,N}
  seeds = ForwardDiff.construct_seeds(ForwardDiff.Partials{N,V})
  duals = [Dual{T}(x[i],seeds[i]) for i in eachindex(x)]
end
pdual = seed_duals(p,MyTag)</code></pre><p>Next we need to make our initial condition Dual numbers so that these propogate through the solution. We can do this manually like:</p><pre><code class="language-julia">u0dual = [Dual{MyTag}(1.0, (0.0, 0.0, 0.0)),Dual{MyTag}(1.0, (0.0, 0.0, 0.0))]</code></pre><p>or use the same shorthand from before:</p><pre><code class="language-julia">u0dual = convert.(eltype(pdual),u0)</code></pre><p>Now we just use these Dual numbers to solve:</p><pre><code class="language-julia">prob_dual = ODEProblem(f,u0dual,tspan,pdual)
sol_dual = solve(prob_dual,Tsit5(), saveat=0.2)</code></pre><p>The solution is now in terms of Dual numbers. We can extract the derivatives by looking at the partials of the duals in the solution. For example, <code>sol_dual[1,end]</code> contains the <code>x</code> component of the solution and the Dual number for this component at the end of the integration, and so <code>sol_dual[1,end][i+1]</code> is <code>dx(t_end)/dp_i</code>.</p><h3 id="Local-Forward-Sensitivity-Analysis-via-ODELocalSensitivityProblem-1"><a class="docs-heading-anchor" href="#Local-Forward-Sensitivity-Analysis-via-ODELocalSensitivityProblem-1">Local Forward Sensitivity Analysis via ODELocalSensitivityProblem</a><a class="docs-heading-anchor-permalink" href="#Local-Forward-Sensitivity-Analysis-via-ODELocalSensitivityProblem-1" title="Permalink"></a></h3><p>For this method local sensitivity is computed using the sensitivity ODE:</p><div>\[\frac{d}{dt}\frac{\partial u}{\partial p_{j}}=\frac{\partial f}{\partial u}\frac{\partial u}{\partial p_{j}}+\frac{\partial f}{\partial p_{j}}=J\cdot S_{j}+F_{j}\]</div><p>where</p><div>\[J=\left(\begin{array}{cccc}
\frac{\partial f_{1}}{\partial u_{1}} &amp; \frac{\partial f_{1}}{\partial u_{2}} &amp; \cdots &amp; \frac{\partial f_{1}}{\partial u_{k}}\\
\frac{\partial f_{2}}{\partial u_{1}} &amp; \frac{\partial f_{2}}{\partial u_{2}} &amp; \cdots &amp; \frac{\partial f_{2}}{\partial u_{k}}\\
\cdots &amp; \cdots &amp; \cdots &amp; \cdots\\
\frac{\partial f_{k}}{\partial u_{1}} &amp; \frac{\partial f_{k}}{\partial u_{2}} &amp; \cdots &amp; \frac{\partial f_{k}}{\partial u_{k}}
\end{array}\right)\]</div><p>is the Jacobian of the system,</p><div>\[F_{j}=\left(\begin{array}{c}
\frac{\partial f_{1}}{\partial p_{j}}\\
\frac{\partial f_{2}}{\partial p_{j}}\\
\vdots\\
\frac{\partial f_{k}}{\partial p_{j}}
\end{array}\right)\]</div><p>are the parameter derivatives, and</p><div>\[S_{j}=\left(\begin{array}{c}
\frac{\partial u_{1}}{\partial p_{j}}\\
\frac{\partial u_{2}}{\partial p_{j}}\\
\vdots\\
\frac{\partial u_{k}}{\partial p_{j}}
\end{array}\right)\]</div><p>is the vector of sensitivities. Since this ODE is dependent on the values of the independent variables themselves, this ODE is computed simultaneously with the actual ODE system.</p><p>Note that the Jacobian-vector product</p><div>\[\frac{\partial f}{\partial u}\frac{\partial u}{\partial p_{j}}\]</div><p>can be computed without forming the Jacobian. With finite differences, this through using the following formula for the directional derivative</p><div>\[Jv \approx \frac{f(x+v \epsilon) - f(x)}{\epsilon},\]</div><p>or, alternatively and without truncation error,  by using a dual number with a single partial dimension, <span>$d = x + v \epsilon$</span> we get that</p><div>\[f(d) = f(x) + Jv \epsilon\]</div><p>as a fast way to calcuate <span>$Jv$</span>. Thus, except when a sufficiently good function for <code>J</code> is given by the user, the Jacobian is never formed. For more details, consult the  <a href="https://mitmath.github.io/18337/lecture9/autodiff_dimensions">MIT 18.337 lecture notes on forward mode AD</a>.</p><h4 id="Syntax-1"><a class="docs-heading-anchor" href="#Syntax-1">Syntax</a><a class="docs-heading-anchor-permalink" href="#Syntax-1" title="Permalink"></a></h4><p><code>ODELocalSensitivityProblem</code> is similar to an <code>ODEProblem</code>:</p><pre><code class="language-julia">function ODELocalSensitivityProblem(f::DiffEqBase.AbstractODEFunction,u0,
                                    tspan,p=nothing,
                                    SensitivityAlg();
                                    kwargs...)</code></pre><p>The <code>SensitivityAlg</code> is used to mirror the definition of the derivative options seen generally throughout OrdinaryDiffEq.jl. The keyword options on the <code>SensitivityAlg</code> are as follows:</p><ul><li><code>autojacvec</code>: Calculate Jacobian-vector (local sensitivity analysis) product  via automatic differentiation with special seeding to avoid building the Jacobian.  Default is <code>true</code>. If <code>autodiff=false</code>, it will use the Jacobian-free forward differencing approximation. If <code>false</code>, the Jacobian will prefer to use any  <code>f.jac</code> function provided by the user. If none is provided by the user, then it  will fall back to automatic or finite differentiation, though this choice is  not recommended.</li><li><code>autodiff</code>: Use automatic differentiation in the internal sensitivity algorithm computations. Default is <code>true</code>.</li><li><code>chunk_size</code>: Chunk size for forward mode differentiation. Default is <code>0</code> for automatic chunk size choice. Only used when <code>autojacvec=false</code>.</li><li><code>diff_type</code>: Choice of differencing used to build the Jacobian when <code>autojacvec=false</code> and <code>autodiff=false</code>. Defaults to <code>Val{:central}</code> for central differencing with DiffEqDiffTools.jl.</li></ul><h4 id="Example-solving-an-ODELocalSensitivityProblem-1"><a class="docs-heading-anchor" href="#Example-solving-an-ODELocalSensitivityProblem-1">Example solving an ODELocalSensitivityProblem</a><a class="docs-heading-anchor-permalink" href="#Example-solving-an-ODELocalSensitivityProblem-1" title="Permalink"></a></h4><p>To define a sensitivity problem, simply use the <code>ODELocalSensitivityProblem</code> type instead of an ODE type. For example, we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by:</p><pre><code class="language-julia">function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + u[1]*u[2]
end

p = [1.5,1.0,3.0]
prob = ODELocalSensitivityProblem(f,[1.0;1.0],(0.0,10.0),p)</code></pre><p>This generates a problem which the ODE solvers can solve:</p><pre><code class="language-julia">sol = solve(prob,DP8())</code></pre><p>Note that the solution is the standard ODE system and the sensitivity system combined. We can use the following helper functions to extract the sensitivity information:</p><pre><code class="language-julia">x,dp = extract_local_sensitivities(sol)
x,dp = extract_local_sensitivities(sol,i)
x,dp = extract_local_sensitivities(sol,t)</code></pre><p>In each case, <code>x</code> is the ODE values and <code>dp</code> is the matrix of sensitivities The first gives the full timeseries of values and <code>dp[i]</code> contains the time series of the  sensitivities of all components of the ODE with respect to <code>i</code>th parameter.  The second returns the <code>i</code>th time step, while the third interpolates to calculate the sensitivities at time <code>t</code>. For example, if we do:</p><pre><code class="language-julia">x,dp = extract_local_sensitivities(sol)
da = dp[1]</code></pre><p>then <code>da</code> is the timeseries for <span>$\frac{\partial u(t)}{\partial p}$</span>. We can plot this</p><pre><code class="language-julia">plot(sol.t,da&#39;,lw=3)</code></pre><p>transposing so that the rows (the timeseries) is plotted.</p><p><img src="../../assets/sensitivityplot.png" alt="Local Sensitivity Solution"/></p><p>Here we see that there is a periodicity to the sensitivity which matches the periodicity of the Lotka-Volterra solutions. However, as time goes on the sensitivity increases. This matches the analysis of Wilkins in Sensitivity Analysis for Oscillating Dynamical Systems.</p><p>We can also quickly see that these values are equivalent to those given by autodifferentiation and numerical differentiation through the ODE solver:</p><pre><code class="language-julia">using ForwardDiff, Calculus
function test_f(p)
  prob = ODEProblem(f,eltype(p).([1.0,1.0]),eltype(p).((0.0,10.0)),p)
  solve(prob,Vern9(),abstol=1e-14,reltol=1e-14,save_everystep=false)[end]
end

p = [1.5,1.0,3.0]
fd_res = ForwardDiff.jacobian(test_f,p)
calc_res = Calculus.finite_difference_jacobian(test_f,p)</code></pre><p>Here we just checked the derivative at the end point.</p><h4 id="Internal-representation-1"><a class="docs-heading-anchor" href="#Internal-representation-1">Internal representation</a><a class="docs-heading-anchor-permalink" href="#Internal-representation-1" title="Permalink"></a></h4><p>For completeness, we detail the internal representation. Therefore, the solution to the ODE are the first <code>n</code> components of the solution. This means we can grab the matrix of solution values like:</p><pre><code class="language-julia">x = sol[1:sol.prob.indvars,:]</code></pre><p>Since each sensitivity is a vector of derivatives for each function, the sensitivities are each of size <code>sol.prob.indvars</code>. We can pull out the parameter sensitivities from the solution as follows:</p><pre><code class="language-julia">da = sol[sol.prob.indvars+1:sol.prob.indvars*2,:]
db = sol[sol.prob.indvars*2+1:sol.prob.indvars*3,:]
dc = sol[sol.prob.indvars*3+1:sol.prob.indvars*4,:]</code></pre><p>This means that <code>da[1,i]</code> is the derivative of the <code>x(t)</code> by the parameter <code>a</code> at time <code>sol.t[i]</code>. Note that all of the functionality available to ODE solutions is available in this case, including interpolations and plot recipes (the recipes will plot the expanded system).</p><h2 id="Adjoint-Sensitivity-Analysis-1"><a class="docs-heading-anchor" href="#Adjoint-Sensitivity-Analysis-1">Adjoint Sensitivity Analysis</a><a class="docs-heading-anchor-permalink" href="#Adjoint-Sensitivity-Analysis-1" title="Permalink"></a></h2><p>Adjoint sensitivity analysis is used to find the gradient of the solution with respect to some functional of the solution. In many cases this is used in an optimization problem to return the gradient with respect to some cost function. It is equivalent to &quot;backpropagation&quot; or reverse-mode automatic differentiation of a differential equation.</p><h3 id="Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-1"><a class="docs-heading-anchor" href="#Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-1">Adjoint Sensitivity Analysis via adjoint_sensitivities</a><a class="docs-heading-anchor-permalink" href="#Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-1" title="Permalink"></a></h3><p>This adjoint requires the definition of some scalar functional <span>$g(u,p)$</span> where <span>$u(t,p)$</span> is the (numerical) solution to the differential equation <span>$d/dt u(t,p)=f(t,u,p)$</span> with <span>$t\in [0,T]$</span> and <span>$u(t_0,p)=u_0$</span>. Adjoint sensitivity analysis finds the gradient of</p><div>\[G(u,p)=G(u(\cdot,p))=\int_{t_{0}}^{T}g(u(t,p),p)dt\]</div><p>some integral of the solution. It does so by solving the adjoint problem</p><div>\[\frac{d\lambda^{\star}}{dt}=g_{u}(u(t,p),p)-\lambda^{\star}(t)f_{u}(t,u(t,p),p),\thinspace\thinspace\thinspace\lambda^{\star}(T)=0\]</div><p>where <span>$f_u$</span> is the Jacobian of the system with respect to the state <span>$u$</span> while <span>$f_p$</span> is the Jacobian with respect to the parameters. The adjoint problem&#39;s solution gives the sensitivities through the integral:</p><div>\[\frac{dG}{dp}=\int_{t_{0}}^{T}\lambda^{\star}(t)f_{p}(t)+g_{p}(t)dt+\lambda^{\star}(t_{0})u_{p}(t_{0})\]</div><p>Notice that since the adjoints require the Jacobian of the system at the state, it requires the ability to evaluate the state at any point in time. Thus it requires the continuous forward solution in order to solve the adjoint solution, and the adjoint solution is required to be continuous in order to calculate the resulting integral.</p><p>There is one extra detail to consider. In many cases we would like to calculate the adjoint sensitivity of some discontinuous functional of the solution. One canonical function is the L2 loss against some data points, that is:</p><div>\[L(u,p)=\sum_{i=1}^{n}\Vert\tilde{u}(t_{i})-u(t_{i},p)\Vert^{2}\]</div><p>In this case, we can reinterpret our summation as the distribution integral:</p><div>\[G(u,p)=\int_{0}^{T}\sum_{i=1}^{n}\Vert\tilde{u}(t_{i})-u(t_{i},p)\Vert^{2}\delta(t_{i}-t)dt\]</div><p>where <span>$δ$</span> is the Dirac distribution. In this case, the integral is continuous except at finitely many points. Thus it can be calculated between each <span>$t_i$</span>. At a given <span>$t_i$</span>, given that the <span>$t_i$</span> are unique, we have that</p><div>\[g_{u}(t_{i})=2\left(\tilde{u}(t_{i})-u(t_{i},p)\right)\]</div><p>Thus the adjoint solution <span>$\lambda^{\star}(t)$</span> is given by integrating between the integrals and applying the jump function <span>$g_u$</span> at every data point <span>$t_i$</span>.</p><p>We note that</p><div>\[\lambda^{\star}(t)f_{u}(t)\]</div><p>is a vector-transpose Jacobian product, also known as a <code>vjp</code>, which can be efficiently computed  using the pullback of backpropogation on the user function <code>f</code> with a forward pass at <code>u</code> with a pullback vector <span>$\lambda^{\star}$</span>. For more information, consult the <a href="https://mitmath.github.io/18337/lecture10/estimation_identification">MIT 18.337 lecture notes on reverse mode AD</a></p><h4 id="Syntax-2"><a class="docs-heading-anchor" href="#Syntax-2">Syntax</a><a class="docs-heading-anchor-permalink" href="#Syntax-2" title="Permalink"></a></h4><p>There are two forms. For discrete adjoints, the form is:</p><pre><code class="language-julia">s = adjoint_sensitivities(sol,alg,dg,ts;kwargs...)</code></pre><p>where <code>alg</code> is the ODE algorithm to solve the adjoint problem, <code>dg</code> is the jump function, and <code>ts</code> is the time points for data. <code>dg</code> is given by:</p><pre><code class="language-julia">dg(out,u,p,t,i)</code></pre><p>which is the in-place gradient of the cost functional <code>g</code> at time point <code>ts[i]</code> with <code>u=u(t)</code>.</p><p>For continuous functionals, the form is:</p><pre><code class="language-julia">s = adjoint_sensitivities(sol,alg,g,nothing,dg;kwargs...)</code></pre><p>for the cost functional</p><pre><code class="language-julia">g(u,p,t)</code></pre><p>with in-place gradient</p><pre><code class="language-julia">dg(out,u,p,t)</code></pre><p>If the gradient is omitted, i.e.</p><pre><code class="language-julia">s = adjoint_sensitivities(sol,alg,g,nothing;kwargs...)</code></pre><p>then it will be computed automatically using ForwardDiff or finite differencing, depending on the <code>autodiff</code> setting in the <code>SensitivityAlg</code>. Note that the keyword arguments are passed to the internal ODE solver for  solving the adjoint problem. Two special keyword arguments are <code>iabstol</code> and  <code>ireltol</code> which are the tolerances for the internal quadrature via QuadGK for  the resulting functional.</p><h4 id="Options-1"><a class="docs-heading-anchor" href="#Options-1">Options</a><a class="docs-heading-anchor-permalink" href="#Options-1" title="Permalink"></a></h4><p>Options for handling the adjoint computation are set by passing a <code>SensitivityAlg</code> type, e.g. <code>SensitivityAlg(backsolve=true)</code>. Additionally, if Gauss-Kronrod quadrature is used, the options <code>ireltol</code> and <code>iabstol</code> into <code>adjoint_sensitivities</code> controls the behavior of the quadrature. Example calls:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Rodas4(),dg,t,ireltol=1e-8)

res = adjoint_sensitivities(sol,Vern9(),dg,t,reltol=1e-8,
                            sensealg=SensitivityAlg(backsolve=true))</code></pre><ul><li><code>checkpointing</code>: When enabled, the adjoint solutions compute the Jacobians by starting from the nearest saved value in <code>sol</code> and computing forward. By default, this is false if <code>sol.dense==true</code>, i.e. if <code>sol</code> has its higher order interpolation then this is by default disabled.</li><li><code>quad</code>: Use Gauss-Kronrod quadrature to integrate the adjoint sensitivity integral. Disabling it can decrease memory usage but increase computation time, since post-solution quadrature can be more accurate with less points using the continuous function. Default is <code>true</code>.</li><li><code>backsolve</code>: Solve the differential equation backward to get the past values. Note that for chaotic or non-reversible systems, such as though that solve to a steady state, enabling this option can lead to wildly incorrect results.  Enabling it can decrease memory usage but increase computation time. When it  is set to <code>true</code>, <code>quad</code> will be automatically set to <code>false</code>. Default is <code>false</code>.</li><li><code>autodiff</code>: Use automatic differentiation in the internal sensitivity algorithm computations. This is the only option that is passed, this will flip <code>autojacvec</code> to false, since that requires reverse-mode AD, and thus finite differencing for the full Jacobian will be used. Default is <code>true</code>.</li><li><code>chunk_size</code>: Chunk size for forward mode differentiation if full Jacobians are built (<code>autojacvec=false</code> and <code>autodiff=true</code>). Default is <code>0</code> for automatic choice of chunk size.</li><li><code>autojacvec</code>: Calculate the vector-Jacobian (adjoint sensitivity analysis) product  via automatic differentiation with special seeding. Due to being a <code>vjp</code> function, this option requires using automatic differentiation, currently implemented with Tracker.jl. Default is <code>true</code> if <code>autodiff</code> is true, and otherwise is false. If <code>autojacvec=false</code>, then a full Jacobian has to be computed, and this will default to using a <code>f.jac</code> function provided by the user from the problem of the forward pass. Otherwise, if <code>autodiff=true</code> then it will use forward-mode AD for the Jacobian, otherwise it will fall back to using a numerical approximation to the Jacobian.</li></ul><p>A way to understand these options at a higher level is as follows:</p><ul><li>For the choice of the overall sensitivity calculation algorithm, using interpolation is preferred if the <code>sol</code> is continuous. Otherwise it will use checkpointed adjoints by default if the user passes in a <code>sol</code> without a good interpolation. Using <code>backsolve</code> is unstable  except in specific conditions, and thus is only used when chosen by the user.</li><li>In any of these cases <code>quad=false</code> is the default, which enlarges the ODE system to calculate the integral simultaniously to the ODE solution. This reduces the memory cost, though in some cases solving the reverse problem with a continuous solution and then using QuadGK.jl to perform the quadrature can use less reverse-pass points and thus decrease the computation time, though this is rare when the number of parameters is large.</li><li>Using automatic differentiation everywhere is the preferred default. This means the <code>vjp</code> will be performed using reverse-mode AD with Tracker.jl and no Jacobian will be formed. If <code>autodiff=false</code>, then <code>autojacvec=false</code> is set since it assumes that user function  is not compatible with any automatic differentiation. In this case, if a user-defined Jacobian function exists, this will be used. Otherwise this means that the  <code>vjp</code> will be computed by forming the Jacobian with finite differences and then doing  the matrix-vector product. As an intermediate option, one can set <code>autodiff=true</code> with <code>autojacvec=false</code> to compute the Jacobian with forward-mode AD and then perform the vector-Jacobian product using that matrix. </li></ul><h4 id="Example-discrete-adjoints-on-a-cost-function-1"><a class="docs-heading-anchor" href="#Example-discrete-adjoints-on-a-cost-function-1">Example discrete adjoints on a cost function</a><a class="docs-heading-anchor-permalink" href="#Example-discrete-adjoints-on-a-cost-function-1" title="Permalink"></a></h4><p>In this example we will show solving for the adjoint sensitivities of a discrete cost functional. First let&#39;s solve the ODE and get a high quality continuous solution:</p><pre><code class="language-julia">function f(du,u,p,t)
  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]
  du[2] = dy = -p[3]*u[2] + u[1]*u[2]
end

p = [1.5,1.0,3.0]
prob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)
sol = solve(prob,Vern9(),abstol=1e-10,reltol=1e-10)</code></pre><p>Now let&#39;s calculate the sensitivity of the L2 error against 1 at evenly spaced points in time, that is:</p><div>\[L(u,p,t)=\sum_{i=1}^{n}\frac{\Vert1-u(t_{i},p)\Vert^{2}}{2}\]</div><p>for <span>$t_i = 0.5i$</span>. This is the assumption that the data is <code>data[i]=1.0</code>. For this function, notice we have that:</p><div>\[\begin{aligned}
dg_{1}&amp;=1-u_{1} \\
dg_{2}&amp;=1-u_{2}
\end{aligned}\]</div><p>and thus:</p><pre><code class="language-julia">dg(out,u,i) = (out.=1.0.-u)</code></pre><p>If we had data, we&#39;d just replace <code>1.0</code> with <code>data[i]</code>. To get the adjoint sensitivities, call:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),dg,t,abstol=1e-14,
                            reltol=1e-14,iabstol=1e-14,ireltol=1e-12)</code></pre><p>This is super high accuracy. As always, there&#39;s a tradeoff between accuracy and computation time. We can check this almost exactly matches the autodifferentiation and numerical differentiation results:</p><pre><code class="language-julia">using ForwardDiff,Calculus
function G(p)
  tmp_prob = remake(prob,u0=convert.(eltype(p),prob.u0),p=p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=t)
  A = convert(Array,sol)
  sum(((1-A).^2)./2)
end
G([1.5,1.0,3.0])
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])
res4 = Flux.Tracker.gradient(G,[1.5,1.0,3.0])
res5 = ReverseDiff.gradient(G,[1.5,1.0,3.0])</code></pre><p>and see this gives the same values.</p><h4 id="Example-controlling-adjoint-method-choices-and-checkpointing-1"><a class="docs-heading-anchor" href="#Example-controlling-adjoint-method-choices-and-checkpointing-1">Example controlling adjoint method choices and checkpointing</a><a class="docs-heading-anchor-permalink" href="#Example-controlling-adjoint-method-choices-and-checkpointing-1" title="Permalink"></a></h4><p>In the previous examples, all calculations were done using the interpolating method. This maximizes speed but at a cost of requiring a dense <code>sol</code>. If it is not possible to hold a dense forward solution in memory, then one can use checkpointing. This is enabled by default if <code>sol</code> is not dense, so for example</p><pre><code class="language-julia">sol = solve(prob,Vern9(),saveat=[0.0,0.2,0.5,0.7])</code></pre><p>Creates a non-dense solution with checkpoints at <code>[0.0,0.2,0.5,0.7]</code>. Now we can do</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),dg,t)</code></pre><p>When grabbing a Jacobian value during the backwards solution, it will no longer interpolate to get the value. Instead, it will start a forward solution at the nearest checkpoint and solve until the necessary time.</p><p>To eliminate the extra forward solutions, one can instead pass the <code>SensitivityAlg</code> with the <code>backsolve=true</code> option:</p><pre><code class="language-julia">sol = solve(prob,Vern9(),save_everystep=false,save_start=false)
res = adjoint_sensitivities(sol,Vern9(),dg,t,sensealg=SensitivityAlg(backsolve=true))</code></pre><p>When this is done, the values for the Jacobian will be computing the original ODE in reverse. Note that this only requires the final value of the solution.</p><h4 id="Applicability-of-Backsolve-and-Caution-1"><a class="docs-heading-anchor" href="#Applicability-of-Backsolve-and-Caution-1">Applicability of Backsolve and Caution</a><a class="docs-heading-anchor-permalink" href="#Applicability-of-Backsolve-and-Caution-1" title="Permalink"></a></h4><p>When <code>backsolve</code> is applicable it is the fastest method and requires the least memory. However, one must be cautious because not all ODEs are stable under backwards integration by the majority of ODE solvers. An example of such an equation is the Lorenz equation. Notice that if one solves the Lorenz equation forward and then in reverse with any adaptive time step and non-reversible integrator, then the backwards solution diverges from the forward solution. As a quick demonstration:</p><pre><code class="language-julia">using Sundials, DiffEqBase
function lorenz(du,u,p,t)
 du[1] = 10.0*(u[2]-u[1])
 du[2] = u[1]*(28.0-u[3]) - u[2]
 du[3] = u[1]*u[2] - (8/3)*u[3]
end
u0 = [1.0;0.0;0.0]
tspan = (0.0,100.0)
prob = ODEProblem(lorenz,u0,tspan)
sol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)
prob2 = ODEProblem(lorenz,sol[end],(100.0,0.0))
sol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)
@show sol[end]-u0 #[-3.22091, -1.49394, 21.3435]</code></pre><p>Thus one should check the stability of the backsolve on their type of problem before enabling this method.</p><h4 id="Example-continuous-adjoints-on-an-energy-functional-1"><a class="docs-heading-anchor" href="#Example-continuous-adjoints-on-an-energy-functional-1">Example continuous adjoints on an energy functional</a><a class="docs-heading-anchor-permalink" href="#Example-continuous-adjoints-on-an-energy-functional-1" title="Permalink"></a></h4><p>In this case we&#39;d like to calculate the adjoint sensitivity of the scalar energy functional</p><div>\[G(u,p)=\int_{0}^{T}\frac{\sum_{i=1}^{n}u_{i}^{2}(t)}{2}dt\]</div><p>which is</p><pre><code class="language-julia">g(u,p,t) = (sum(u).^2) ./ 2</code></pre><p>Notice that the gradient of this function with respect to the state <code>u</code> is:</p><pre><code class="language-julia">function dg(out,u,p,t)
  out[1]= u[1] + u[2]
  out[2]= u[1] + u[2]
end</code></pre><p>To get the adjoint sensitivities, we call:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),g,nothing,dg,abstol=1e-8,
                                 reltol=1e-8,iabstol=1e-8,ireltol=1e-8)</code></pre><p>Notice that we can check this against autodifferentiation and numerical differentiation as follows:</p><pre><code class="language-julia">function G(p)
  tmp_prob = remake(prob,p=p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)
  res,err = quadgk((t)-&gt; (sum(sol(t)).^2)./2,0.0,10.0,abstol=1e-14,reltol=1e-10)
  res
end
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../bifurcation/">« Bifurcation Analysis</a><a class="docs-footer-nextpage" href="../global_sensitivity/">Global Sensitivity Analysis »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 29 December 2019 22:17">Sunday 29 December 2019</span>. Using Julia version 1.1.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
