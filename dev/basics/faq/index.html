<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Frequently Asked Questions · DifferentialEquations.jl</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-90474609-3"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-90474609-3', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://docs.sciml.ai/DiffEqDocs/stable/basics/faq/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="DifferentialEquations.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DifferentialEquations.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/ode_example/">Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/faster_ode_example/">Code Optimization for Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/advanced_ode_example/">Solving Large Stiff Equations</a></li><li><a class="tocitem" href="../../tutorials/sde_example/">Stochastic Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/rode_example/">Random Ordinary Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/dde_example/">Delay Differential Equations</a></li><li><a class="tocitem" href="../../tutorials/dae_example/">Differential Algebraic Equations</a></li><li><a class="tocitem" href="../../tutorials/discrete_stochastic_example/">Continuous-Time Jump Processes and Gillespie Methods</a></li><li><a class="tocitem" href="../../tutorials/jump_diffusion/">Jump Diffusion Equations</a></li><li><a class="tocitem" href="../../tutorials/bvp_example/">Boundary Value Problems</a></li><li><a class="tocitem" href="../../tutorials/additional/">Additional Tutorials</a></li></ul></li><li><span class="tocitem">Basics</span><ul><li><a class="tocitem" href="../overview/">Overview of DifferentialEquations.jl</a></li><li><a class="tocitem" href="../common_solver_opts/">Common Solver Options (Solve Keyword Arguments)</a></li><li><a class="tocitem" href="../solution/">Solution Handling</a></li><li><a class="tocitem" href="../plot/">Plot Functions</a></li><li><a class="tocitem" href="../integrator/">Integrator Interface</a></li><li><a class="tocitem" href="../problem/">Problem Interface</a></li><li class="is-active"><a class="tocitem" href>Frequently Asked Questions</a><ul class="internal"><li><a class="tocitem" href="#faq_stability"><span>Stability and Divergence of ODE Solves</span></a></li><li><a class="tocitem" href="#faq_performance"><span>Performance</span></a></li><li><a class="tocitem" href="#Complicated-Models"><span>Complicated Models</span></a></li><li><a class="tocitem" href="#Numerical-Error"><span>Numerical Error</span></a></li><li><a class="tocitem" href="#Autodifferentiation-and-Dual-Numbers"><span>Autodifferentiation and Dual Numbers</span></a></li><li><a class="tocitem" href="#Sparse-Jacobians"><span>Sparse Jacobians</span></a></li><li><a class="tocitem" href="#Odd-Error-Messages"><span>Odd Error Messages</span></a></li></ul></li><li><a class="tocitem" href="../compatibility_chart/">Solver Compatibility Chart</a></li></ul></li><li><span class="tocitem">Problem Types</span><ul><li><a class="tocitem" href="../../types/discrete_types/">Discrete Problems</a></li><li><a class="tocitem" href="../../types/ode_types/">ODE Problems</a></li><li><a class="tocitem" href="../../types/nonautonomous_linear_ode/">Non-autonomous Linear ODE / Lie Group Problems</a></li><li><a class="tocitem" href="../../types/dynamical_types/">Dynamical, Hamiltonian and 2nd Order ODE Problems</a></li><li><a class="tocitem" href="../../types/split_ode_types/">Split ODE Problems</a></li><li><a class="tocitem" href="../../types/steady_state_types/">Steady State Problems</a></li><li><a class="tocitem" href="../../types/bvp_types/">BVP Problems</a></li><li><a class="tocitem" href="../../types/sde_types/">SDE Problems</a></li><li><a class="tocitem" href="../../types/sdae_types/">SDAE Problems</a></li><li><a class="tocitem" href="../../types/rode_types/">RODE Problems</a></li><li><a class="tocitem" href="../../types/dde_types/">DDE Problems</a></li><li><a class="tocitem" href="../../types/sdde_types/">SDDE Problems</a></li><li><a class="tocitem" href="../../types/dae_types/">DAE Problems</a></li><li><a class="tocitem" href="../../types/jump_types/">Jump Problems</a></li></ul></li><li><span class="tocitem">Solver Algorithms</span><ul><li><a class="tocitem" href="../../solvers/discrete_solve/">Discrete Solvers</a></li><li><a class="tocitem" href="../../solvers/ode_solve/">ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/nonautonomous_linear_ode/">Non-autonomous Linear ODE / Lie Group ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/dynamical_solve/">Dynamical, Hamiltonian, and 2nd Order ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/split_ode_solve/">Split ODE Solvers</a></li><li><a class="tocitem" href="../../solvers/steady_state_solve/">Steady State Solvers</a></li><li><a class="tocitem" href="../../solvers/bvp_solve/">BVP Solvers</a></li><li><a class="tocitem" href="../../solvers/jump_solve/">Jump Problem and Jump Diffusion Solvers</a></li><li><a class="tocitem" href="../../solvers/sde_solve/">SDE Solvers</a></li><li><a class="tocitem" href="../../solvers/sdae_solve/">SDAE Solvers</a></li><li><a class="tocitem" href="../../solvers/rode_solve/">RODE Solvers</a></li><li><a class="tocitem" href="../../solvers/dde_solve/">DDE Solvers</a></li><li><a class="tocitem" href="../../solvers/sdde_solve/">SDDE Solvers</a></li><li><a class="tocitem" href="../../solvers/dae_solve/">DAE Solvers</a></li><li><a class="tocitem" href="../../solvers/benchmarks/">Solver Benchmarks</a></li></ul></li><li><span class="tocitem">Additional Features</span><ul><li><a class="tocitem" href="../../features/performance_overloads/">Jacobians, Gradients, etc.</a></li><li><a class="tocitem" href="../../features/diffeq_arrays/">DiffEq-Specific Array Types</a></li><li><a class="tocitem" href="../../features/diffeq_operator/">DiffEqOperators</a></li><li><a class="tocitem" href="../../features/noise_process/">Noise Processes</a></li><li><a class="tocitem" href="../../features/linear_nonlinear/">Specifying (Non)Linear Solvers and Preconditioners</a></li><li><a class="tocitem" href="../../features/callback_functions/">Event Handling and Callback Functions</a></li><li><a class="tocitem" href="../../features/callback_library/">Callback Library</a></li><li><a class="tocitem" href="../../features/ensemble/">Parallel Ensemble Simulations</a></li><li><a class="tocitem" href="../../features/io/">I/O: Saving and Loading Solution Data</a></li><li><a class="tocitem" href="../../features/low_dep/">Reduced Compile Time, Optimizing Runtime, and Low Dependency Usage</a></li><li><a class="tocitem" href="../../features/progress_bar/">Progress Bar Integration</a></li></ul></li><li><span class="tocitem">Detailed Solver APIs</span><ul><li><a class="tocitem" href="../../api/sundials/">Sundials.jl</a></li><li><a class="tocitem" href="../../api/daskr/">DASKR.jl</a></li></ul></li><li><span class="tocitem">Extra Details</span><ul><li><a class="tocitem" href="../../extras/timestepping/">Timestepping Method Descriptions</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Basics</a></li><li class="is-active"><a href>Frequently Asked Questions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Frequently Asked Questions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/SciML/DiffEqDocs.jl/blob/master/docs/src/basics/faq.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="faq"><a class="docs-heading-anchor" href="#faq">Frequently Asked Questions</a><a id="faq-1"></a><a class="docs-heading-anchor-permalink" href="#faq" title="Permalink"></a></h1><p>This page is a compilation of frequently asked questions and answers.</p><h2 id="faq_stability"><a class="docs-heading-anchor" href="#faq_stability">Stability and Divergence of ODE Solves</a><a id="faq_stability-1"></a><a class="docs-heading-anchor-permalink" href="#faq_stability" title="Permalink"></a></h2><p>For guidelines on debugging ODE solve issues, see <a href="https://discourse.julialang.org/t/psa-how-to-help-yourself-debug-differential-equation-solving-issues/62489">PSA: How to help yourself debug differential equation solving issues</a>.</p><h4 id="My-model-is-reporting-unstable-results.-What-can-I-do?"><a class="docs-heading-anchor" href="#My-model-is-reporting-unstable-results.-What-can-I-do?">My model is reporting unstable results. What can I do?</a><a id="My-model-is-reporting-unstable-results.-What-can-I-do?-1"></a><a class="docs-heading-anchor-permalink" href="#My-model-is-reporting-unstable-results.-What-can-I-do?" title="Permalink"></a></h4><p>First of all, don&#39;t panic. You may have experienced one of the following warnings:</p><blockquote><p>dt &lt;= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.</p><p>NaN dt detected. Likely a NaN value in the state, parameters, or derivative value caused this outcome.</p><p>Instability detected. Aborting</p></blockquote><p>These are all pointing to a similar behavior: for some reason or another, the ODE solve is diverging to infinity. As it diverges to infinity, the <code>dt</code> of the integrator will drop (trying to control the speed and error), so it will either hit the minimum <code>dt</code>, hit <code>dt=NaN</code>, or have a value in the ODE hit <code>Inf</code>. Whichever one occurs first will throw the respective warning.</p><p>How to handle this? 99.99% of the time this has been debugged, it has turned out to be an error in the user&#39;s model! A missing minus sign, an incorrect term, etc. There are many other behaviors to watch out for. In some ODEs, increasing a parameter can cause a bifurcation so that the solution diverges. With <code>u&#39;=a*u</code>, if <code>a</code> is negative then it nicely falls to zero, but if <code>a</code> is positive the solution quickly diverges to infinity! This means, double check your parameters are indexed correctly!</p><p><strong>Note: if you see these warnings during a parameter estimation process, this is likely the underlying problem. Simply check <code>sol.retcode != :Success</code> and throw an <code>Inf</code> cost and most optimizers will reject steps in those parameter regimes!</strong></p><p>There are a few other things to check as well. In many cases, the stability of an ODE solve improves as you decrease the tolerance, so you may want to try a smaller <code>abstol</code> and <code>reltol</code>. One behavior to watch out for is that if your model is a differential-algebraic equation and your DAE is of high index (say index&gt;1), this can impact the numerical solution. In this case you may want to use the <a href="https://mtk.sciml.ai/dev/mtkitize_tutorials/modelingtoolkitize_index_reduction/">ModelingToolkit.jl index reduction tools</a> to improve the numerical stability of a solve. In addition, if it&#39;s a highly stiff ODE/DAE that is large and you&#39;re using a matrix-free solver (such as GMRES), make sure the tolerance of the GMRES is well-tuned and an appropriate preconditioner is applied. Finally, try other solvers. They all have different stability, so try <code>Tsit5()</code>, <code>Vern7()</code>, <code>QNDF()</code>, <code>Rodas5()</code>, <code>TRBDF2()</code>, <code>KenCarp4()</code>, <code>Sundials.CVODE_BDF()</code>, etc. and see what works.</p><p>If none of this works out, double check that your ODE truly has the behavior that you believe it should. This is one of the most common issues: your intuition may be deceiving. For example, <code>u&#39; = -sqrt(u)</code> with <code>u(0)=1</code> cannot hit zero because its derivative shrinks to zero, right? Wrong! <a href="https://www.wolframalpha.com/input/?i=u%27%3D-sqrt%28u%29">It will hit zero in a finite time, after which the solution is undefined and does not have a purely real solution</a>. <code>u&#39; = u^2 - 100u</code> will &quot;usually&quot; go to zero, but if <code>u(0)&gt;10</code> then it will go to infinity. Plot out your diverging solution and see whether the asymtopics are correct: if <code>u[i]</code> gets big, do you equations make <code>u&#39;[i]</code> positive and growing? That&#39;s would be a problem!</p><p>Let&#39;s say you don&#39;t believe you made an error at all and you want to file a bug report. To do so, you&#39;ll first want to prove that it&#39;s isolated to a solver. If it&#39;s a solver issue, then you shouldn&#39;t see it happen with every single solver. Do you think it&#39;s an issue with the Julia solvers? Well fortunately, DifferentialEquations.jl offers direct unmodified wrappers to almost all previously built solvers, so if you think it&#39;s a Julia issue, try running your ODE through:</p><ul><li>Sundials.jl, a wrapper for the C++ SUNDIALS library though <code>CVODE_Adams</code>, <code>CVODE_BDF</code>, <code>IDA</code>, and <code>ARKODE</code>.</li><li>ODEInterfaceDiffEq.jl, a wrapper for the classic Hairer Fortran codes like <code>dorpi5</code>, <code>dop853</code>, <code>radau</code>, <code>rodas</code>, etc.</li><li>LSODA.jl, a wrapper for the classic <code>lsoda</code> algorithm.</li><li>MATLABDiffEq.jl, a wrapper for the MATLAB ODE solvers <code>ode45</code>, <code>ode15s</code>, etc.</li><li>SciPyDiffEq.jl, a wrapper for SciPy&#39;s <code>odeint</code> (LSODA) and other methods (LSODE, etc.).</li><li>deSolveDiffEq.jl, a wrapper for the commonly used R library.</li></ul><p>And many more. Testing this is as simple as changing <code>solve(prob,Tsit5())</code> to <code>solve(prob,lsoda())</code>, so please give this a try. If you translated your code from another language, like Python or MATLAB, use the direct wrapper to double check the steps are the same. If they are not, then your ODE is not the same, because it&#39;s using a direct call to the solvers of those packages!</p><p>If your ODE diverges to infinity with every ODE solver ever made, the problem is most likely not the ODE solvers. Or rather, to put it in meme form:</p><p><img src="https://user-images.githubusercontent.com/1814174/120933617-eb65ac80-c6c8-11eb-85f7-ef98688d054c.jpg" alt/></p><p>Don&#39;t be like Patrick. If after trying these ideas your ODE solve still seems to have issues and you haven&#39;t narrowed it down, feel free to ask on the <a href="https://discourse.julialang.org/">Julia Discourse</a> to get some help diagnosing it. If you did find a solver issue, please open an issue on the Github repository.</p><h4 id="A-larger-maxiters-seems-to-be-needed,-but-it&#39;s-already-high?"><a class="docs-heading-anchor" href="#A-larger-maxiters-seems-to-be-needed,-but-it&#39;s-already-high?">A larger maxiters seems to be needed, but it&#39;s already high?</a><a id="A-larger-maxiters-seems-to-be-needed,-but-it&#39;s-already-high?-1"></a><a class="docs-heading-anchor-permalink" href="#A-larger-maxiters-seems-to-be-needed,-but-it&#39;s-already-high?" title="Permalink"></a></h4><p>If you see:</p><blockquote><p>Interrupted. Larger maxiters is needed.</p></blockquote><p>Note that it could quite possibly arise just from having a very long timespan. If you check <code>sol.t</code> from the returned object and it looks like it&#39;s stepping at reasonable lengths, feel free to just pass <code>maxiters=...</code> into solve to bump it up from the default of <code>Int(1e5)</code>.</p><p>But if your <code>maxiters</code> is already high, then the problem is likely that your model is stiff. A stiff ODE requires very small time steps from many explicit solvers, such as <code>Tsit5()</code>, <code>Vern7()</code>, etc., and thus those methods are not appropriate for this kind of problem. You will want to change to a different method, like <code>Rodas5()</code>, <code>Rosenbrock23()</code>, <code>TRBDF2()</code>, <code>KenCarp4()</code>, or <code>QNDF()</code>.</p><h4 id="My-ODE-goes-negative-but-should-stay-positive,-what-tools-can-help?"><a class="docs-heading-anchor" href="#My-ODE-goes-negative-but-should-stay-positive,-what-tools-can-help?">My ODE goes negative but should stay positive, what tools can help?</a><a id="My-ODE-goes-negative-but-should-stay-positive,-what-tools-can-help?-1"></a><a class="docs-heading-anchor-permalink" href="#My-ODE-goes-negative-but-should-stay-positive,-what-tools-can-help?" title="Permalink"></a></h4><p>There are many tools to help! However, let&#39;s first focus on one piece first: when you say &quot;should&quot; be positive, what do you mean by &quot;should&quot;? If you mean &quot;mathematically you can prove that the ODE with these values and these initial conditions will have a solution that is positive for all time&quot; then yes, you&#39;re looking in the right place. If by &quot;should&quot; you mean &quot;it&#39;s a model of biochemical reactions so the concentration should always be positive&quot;, well ask yourself first, did you write down a model where it will always be positive?</p><p>The following set of tools are designed to accuracy enforce positivity in ODE models which mathematically should be positive in the true solution. If they encounter a model that is actually going negative, they will work really hard to get a positive but correct solution, which is impossible, so they will simply error out. This can be more subtle than you think. Solving <code>u&#39;=-sqrt(u)</code> is not guaranteed to stay positive, even though the derivative goes to zero as <code>u</code> goes to zero (check the analytical solution if you&#39;re curious). Similarly, analyzing nonlinear models can showcase all sorts of behavior. A common cause for accidental negativity is Hill functions in systems biology models: just because derivatives go to zero doesn&#39;t mean they are going to zero fast enough to keep things positive!</p><p>With that in mind, let&#39;s see the options.</p><p>The simplest trick is to change the solver tolerance. Reduce <code>abstol</code> (and maybe <code>reltol</code>) a bit. That can help reduce the error and thus keep the solution positive. For some more difficult equations, changing to a stiff ODE solver like <code>Rosenbrock23()</code> <code>QNDF</code>, or <code>TRBDF2()</code> can be helpful.</p><p>If those don&#39;t work, call out the big guns. One of them is <code>isoutofdomain</code>, where you can define a boolean function which will cause step rejections whenever it is not satisfied. For example, <code>isoutofdomain = (u,p,t)-&gt;any(x-&gt;x&lt;0,u)</code> will make the solver reject any step which cases any variable <code>u</code> to go negative. Now, using any pure-Julia solver with this option, it&#39;s impossible to get a negative in the result! One thing you may see though is:</p><blockquote><p>dt &lt;= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.</p></blockquote><p>or</p><blockquote><p>Interrupted. Larger maxiters is needed.</p></blockquote><p>What this means is that enforcing positivity is not possible. It keeps rejecting steps that go negative, reducing <code>dt</code>, taking another step, rejecting, reducing, repeat until <code>dt</code> hits <code>dtmin</code> or it hits maxiters. This means that even when trying to solve the problem with the most accurate infinitesimal <code>dt</code>, the solution still goes negative. Are you sure the true solution is supposed to be positive? If you see this, check for issues like a missing minus sign in your equations.</p><p>If that works but is a little slow, the domain handling callbacks in <a href="../../features/callback_library/#callback_library">the callback library</a> are designed to function similarly but in a way that gets better performance. Instead of repeating lots of steps through rejections, it interpolates back to still take a smaller step, always progressing forwards. However, this can be a bit less stable, so its applicability depends on the equation, and once again this requires that the solution is truly positive. If the true solution goes negative, it will repeatedly try interpolating backwards until it can no longer and end with a <code>dtmin</code> issue.</p><p>Finally, note that ODE solvers will not be more correct than tolerance, and so one should expect that if the solution is supposed to be positive but <code>abstol=1e-12</code>, you may end up with <code>u[i]=-1e-12</code>. That is okay, <a href="https://www.radford.edu/~thompson/RP/nonnegative.pdf">that is expected behavior of numerical solvers</a>, the ODE solver is still doing its job. If this is a major issue for your application, you may want to write your model to be robust to this behavior, such as changing <code>sqrt(u[i])</code> to <code>sqrt(max(0,u[i]))</code>. You should also consider transforming your values, like solving for <code>u^2</code> or <code>exp(u)</code> instead of <code>u</code>, which mathematically can only be positive. Look into using a tool like <a href="https://mtk.sciml.ai/dev/">ModelingToolkit.jl</a> for automatically transforming your equations.</p><h2 id="faq_performance"><a class="docs-heading-anchor" href="#faq_performance">Performance</a><a id="faq_performance-1"></a><a class="docs-heading-anchor-permalink" href="#faq_performance" title="Permalink"></a></h2><h4 id="GPUs,-multithreading-and-distributed-computation-support"><a class="docs-heading-anchor" href="#GPUs,-multithreading-and-distributed-computation-support">GPUs, multithreading and distributed computation support</a><a id="GPUs,-multithreading-and-distributed-computation-support-1"></a><a class="docs-heading-anchor-permalink" href="#GPUs,-multithreading-and-distributed-computation-support" title="Permalink"></a></h4><p>Yes. The <code>*</code>DiffEq.jl libraries (OrdinaryDiffEq.jl, StochasticDiffEq.jl, and DelayDiffEq.jl) are all written to be generic to the array and number types. This means they will adopt the implementation that is given by the array type. The in-place algorithms internally utilize Julia&#39;s broadcast (with some exceptions due to a Julia bug for now, see <a href="https://github.com/SciML/OrdinaryDiffEq.jl/issues/106">this issue</a>) and Julia&#39;s <code>mul!</code> in-place matrix multiplication function. The out-of-place algorithms utilize standard arithmetical functions. Both additionally utilize the user&#39;s norm specified via the common interface options and, if a stiff solver, ForwardDiff/DiffEqDiffTools for the Jacobian calculation, and Base linear factorizations for the linear solve. For your type, you may likely need to give a <a href="basics/@ref advanced_adaptive_stepsize_control">better form of the norm</a>, <a href="../../features/performance_overloads/#performance_overloads">Jacobian</a>, or <a href="../../features/linear_nonlinear/#linear_nonlinear">linear solve calculations</a> to fully utilize parallelism.</p><p>GPUArrays.jl (CuArrays.jl), ArrayFire.jl, DistributedArrays.jl have been tested and work in various forms, where the last one is still not recommended for common use yet.</p><p>The next question is whether it matters. Generally, your system has to be large for parallelism to matter. Using a multithreaded array for broadcast we find helpful around <code>N&gt;1000</code>, though the Sundials manual says <code>N&gt;100,000</code>. For high order Runge-Kutta methods it&#39;s likely lower than the Sundials estimate because of more operations packed into each internal step, but as always that will need more benchmarks to be precise and will depend on the problem being solved. GPUs generally require some intensive parallel operation in the user&#39;s <code>f</code> function to be viable, for example a matrix multiplication for a stencil computation in a PDE. If you&#39;re simply solving some ODE element-wise on a big array it likely won&#39;t do much or it will slow things down just due to how GPUs work. DistributedArrays require parallel linear solves to really matter, and thus are only recommended when you have a problem that cannot fit into memory or are using a stiff solver with a Krylov method for the linear solves.</p><h3 id="Note-About-Setting-Up-Your-Julia-Installation-for-Speed:-BLAS-Choices"><a class="docs-heading-anchor" href="#Note-About-Setting-Up-Your-Julia-Installation-for-Speed:-BLAS-Choices">Note About Setting Up Your Julia Installation for Speed: BLAS Choices</a><a id="Note-About-Setting-Up-Your-Julia-Installation-for-Speed:-BLAS-Choices-1"></a><a class="docs-heading-anchor-permalink" href="#Note-About-Setting-Up-Your-Julia-Installation-for-Speed:-BLAS-Choices" title="Permalink"></a></h3><p>Julia uses an underlying BLAS implementation for its matrix multiplications and factorizations. This library is automatically multithreaded and accelerates the internal linear algebra of DifferentialEquations.jl. However, for optimality, you should make sure that the number of BLAS threads that you are using matches the number of physical cores and not the number of logical cores. See <a href="https://github.com/JuliaLang/julia/issues/33409">this issue for more details</a>.</p><p>To check the number of BLAS threads, use:</p><pre><code class="language-julia hljs">ccall((:openblas_get_num_threads64_, Base.libblas_name), Cint, ())</code></pre><p>If I want to set this directly to 4 threads, I would use:</p><pre><code class="language-julia hljs">using LinearAlgebra
LinearAlgebra.BLAS.set_num_threads(4)</code></pre><p>Additionally, in some cases Intel&#39;s MKL might be a faster BLAS than the standard BLAS that ships with Julia (OpenBLAS). To switch your BLAS implementation, you can use <a href="https://github.com/JuliaComputing/MKL.jl">MKL.jl</a> which will accelerate the linear algebra routines. This is done via:</p><pre><code class="language-julia hljs">using MKL</code></pre><h4 id="My-ODE-is-solving-really-slow"><a class="docs-heading-anchor" href="#My-ODE-is-solving-really-slow">My ODE is solving really slow</a><a id="My-ODE-is-solving-really-slow-1"></a><a class="docs-heading-anchor-permalink" href="#My-ODE-is-solving-really-slow" title="Permalink"></a></h4><p>First, check for bugs. These solvers go through a ton of convergence tests and so if there&#39;s a solver issue, it&#39;s either just something to do with how numerical methods work or it&#39;s a user-error (generally the latter, though check the later part of the FAQ on normal numerical errors). User-errors in the <code>f</code> function causing a divergence of the solution is the most common reason for reported slow codes.</p><p>If you have no bugs, great! The standard tricks for optimizing Julia code then apply. Take a look at the <a href="https://tutorials.sciml.ai/html/introduction/03-optimizing_diffeq_code.html">Optimizing DiffEq Code tutorial</a> for some tips and pointers.</p><p>What you want to do first is make sure your function does not allocate. If your system is small (<code>&lt;=100</code> ODEs/SDEs/DDEs/DAEs?), then you should set your system up to use <a href="https://github.com/JuliaArrays/StaticArrays.jl">StaticArrays.jl</a>. This is demonstrated <a href="../../tutorials/ode_example/#ode_other_types">in the ODE tutorial</a> with static matrices. Static vectors/arrays are stack-allocated, and thus creating new arrays is free and the compiler doesn&#39;t have to heap-allocate any of the temporaries (that&#39;s the expensive part!). These have specialized super fast dispatches for arithmetic operations and extra things like LU-factorizations, and thus they are preferred when possible. However, they lose efficiency if they grow too large.</p><p>For anything larger, you should use the <code>in-place</code> syntax <code>f(du,u,p,t)</code> and make sure that your function doesn&#39;t allocate. Assuming you know of a <code>u0</code>, you should be able to do:</p><pre><code class="language-julia hljs">du = similar(u0)
@time f(du,u0,p,t)</code></pre><p>and see close to zero allocations and close to zero memory allocated. If you see more, then you might have a type-instability or have temporary arrays. To find type-instabilities, you should do:</p><pre><code class="language-julia hljs">@code_warntype f(du,u,p,t)</code></pre><p>and read the printout to see if there&#39;s any types that aren&#39;t inferred by the compiler, and fix them. If you have any global variables, you should make them <code>const</code>. As for allocations, some common things that allocate are:</p><ul><li>Array slicing, like <code>u[1:5]</code>. Instead, use <code>@view u[1:5]</code></li><li>Matrix multiplication with <code>*</code>. Instead of <code>A*b</code>, use <code>mul!(c,A,b)</code> for some pre-allocated cache vector <code>c</code>.</li><li>Non-broadcasted expressions. Every expression on arrays should <code>.=</code> into another array, or it should be re-written to loop and do computations with scalar (or static array) values.</li></ul><p>For an example of optimizing a function resulting from a PDE discretization, see <a href="http://www.stochasticlifestyle.com/solving-systems-stochastic-pdes-using-gpus-julia/">this blog post</a>.</p><h4 id="The-stiff-solver-takes-forever-to-take-steps-for-my-PDE-discretization"><a class="docs-heading-anchor" href="#The-stiff-solver-takes-forever-to-take-steps-for-my-PDE-discretization">The stiff solver takes forever to take steps for my PDE discretization</a><a id="The-stiff-solver-takes-forever-to-take-steps-for-my-PDE-discretization-1"></a><a class="docs-heading-anchor-permalink" href="#The-stiff-solver-takes-forever-to-take-steps-for-my-PDE-discretization" title="Permalink"></a></h4><p>The solvers for stiff solvers require solving a nonlinear equation each step. In order to do so, they have to do a few Newton steps. By default, these methods assume that the Jacobian is dense, automatically calculate the Jacobian for you, and do a dense factorization. However, in many cases you may want to use alternatives that are more tuned for your problem.</p><p>First of all, when available, it&#39;s recommended that you pass a function for computing your Jacobian. This is discussed in the <a href="../../features/performance_overloads/#performance_overloads">performance overloads</a> section. Jacobians are especially helpful for Rosenbrock methods.</p><p>Secondly, if your Jacobian isn&#39;t dense, you shouldn&#39;t use a dense Jacobian! Instead, if you&#39;re using  a <code>*DiffEq</code> library you should <a href="../../features/linear_nonlinear/#linear_nonlinear">specify a linear solver</a> and/or a <code>jac_prototype</code> for the matrix form, and for Sundials.jl, you should change the <code>linear_solver</code> option. See <a href="../../solvers/ode_solve/#ode_solve_sundials">the ODE solve Sundials portion</a> for details on that.</p><p>Right now, <code>QNDF</code> is the recommended method for stiff problems with large sparse Jacobians. You should specify <code>jac_prototype</code> as a special matrix, such as a banded or tridiagonal matrix, if it satisfies a special structure. If you only know the Jacobian is sparse, using automated sparsity detection can help with identifying the sparsity pattern. See the <a href="../../tutorials/advanced_ode_example/#stiff">stiff ODE tutorial</a> for more details. Lastly, using <code>LinSolveGMRES()</code> can help if a sparsity pattern cannot be obtained but the matrix is large, or if the sparsity cannot fit into memory. Once again, a good reference for how to handle PDE discretizations can be found <a href="http://www.stochasticlifestyle.com/solving-systems-stochastic-pdes-using-gpus-julia/">at this blog post</a>.</p><h4 id="My-Problem-Has-Discontinuities-and-is-Unstable-/-Slow"><a class="docs-heading-anchor" href="#My-Problem-Has-Discontinuities-and-is-Unstable-/-Slow">My Problem Has Discontinuities and is Unstable / Slow</a><a id="My-Problem-Has-Discontinuities-and-is-Unstable-/-Slow-1"></a><a class="docs-heading-anchor-permalink" href="#My-Problem-Has-Discontinuities-and-is-Unstable-/-Slow" title="Permalink"></a></h4><p><a href="https://discourse.julialang.org/t/handling-instability-when-solving-ode-problems/9019/5">This Discourse post</a> goes into detail for how to handle discontinuities in your ODE function and how to use that extra information to speed up the solver.</p><h2 id="Complicated-Models"><a class="docs-heading-anchor" href="#Complicated-Models">Complicated Models</a><a id="Complicated-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Complicated-Models" title="Permalink"></a></h2><h4 id="Switching-ODE-functions-in-the-middle-of-integration"><a class="docs-heading-anchor" href="#Switching-ODE-functions-in-the-middle-of-integration">Switching ODE functions in the middle of integration</a><a id="Switching-ODE-functions-in-the-middle-of-integration-1"></a><a class="docs-heading-anchor-permalink" href="#Switching-ODE-functions-in-the-middle-of-integration" title="Permalink"></a></h4><p>There are a few ways to do this. The simplest way is to just have a parameter to switch between the two. For example:</p><pre><code class="language-julia hljs">function f(du,u,p,t)
  if p == 0
    du[1] = 2u[1]
  else
    du[1] = -2u[1]
  end
  du[2] = -u[2]
end</code></pre><p>Then in a callback you can make the <code>affect!</code> function modify <code>integrator.prob.p</code>. For example, we can make it change when <code>u[2]&lt;0.5</code> via:</p><pre><code class="language-julia hljs">condition(t,u,integrator) = u[2] - 0.5
affect!(integrator) = integrator.p = 1</code></pre><p>Then it will change betweeen the two ODE choices for <code>du1</code> at that moment. Another way to do this is to make the ODE functions all be the same type via FunctionWrappers.jl, but that is unnecessary. With the way that modern processors work, there exists branch prediction and thus execution of a conditional is free if it&#39;s predictable which branch will be taken. In this case, almost every call to <code>f</code> takes the <code>p==0</code> route until the callback, at which point it is almost always the <code>else</code> route. Therefore the processor will effectively get rid of the computational cost associated with this, so you&#39;re likely over-optimizing if you&#39;re going further (unless this change happens every step, but even then this is probably the cheapest part of the computation...).</p><h2 id="Numerical-Error"><a class="docs-heading-anchor" href="#Numerical-Error">Numerical Error</a><a id="Numerical-Error-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-Error" title="Permalink"></a></h2><h4 id="What-does-tolerance-mean-and-how-much-error-should-I-expect"><a class="docs-heading-anchor" href="#What-does-tolerance-mean-and-how-much-error-should-I-expect">What does tolerance mean and how much error should I expect</a><a id="What-does-tolerance-mean-and-how-much-error-should-I-expect-1"></a><a class="docs-heading-anchor-permalink" href="#What-does-tolerance-mean-and-how-much-error-should-I-expect" title="Permalink"></a></h4><p>The most useful options are the tolerances <code>abstol</code> and <code>reltol</code>. These tell the internal adaptive time stepping engine how precise of a solution you want. Generally, <code>reltol</code> is the relative accuracy while <code>abstol</code> is the accuracy when <code>u</code> is near zero. <em>These tolerances are local tolerances and thus are not global guarantees</em>. However, a good rule of thumb is that the total solution accuracy is 1-2 digits less than the relative tolerances. Thus for the defaults <code>abstol=1e-6</code> and <code>reltol=1e-3</code>, you can expect a global accuracy of about 1-2 digits. This is standard across the board and applies to the native Julia methods, the wrapped Fortran and C++ methods, the calls to MATLAB/Python/R, etc.</p><h4 id="The-solver-doesn&#39;t-obey-physical-law-X-(e.g.-conservation-of-energy)"><a class="docs-heading-anchor" href="#The-solver-doesn&#39;t-obey-physical-law-X-(e.g.-conservation-of-energy)">The solver doesn&#39;t obey physical law X (e.g. conservation of energy)</a><a id="The-solver-doesn&#39;t-obey-physical-law-X-(e.g.-conservation-of-energy)-1"></a><a class="docs-heading-anchor-permalink" href="#The-solver-doesn&#39;t-obey-physical-law-X-(e.g.-conservation-of-energy)" title="Permalink"></a></h4><p>Yes, this is because the numerical solution of the ODE is not the exact solution. There are a few ways that you can handle this problem. One way is to get a more exact solution. Thus instead of</p><pre><code class="language-julia hljs">sol = solve(prob,alg)</code></pre><p>use</p><pre><code class="language-julia hljs">sol = solve(prob,alg,abstol=1e-10,reltol=1e-10)</code></pre><p>Of course, there&#39;s always a tradeoff between accuracy and efficiency, so play around to find out what&#39;s right for your problem.</p><p>Another thing you can do is use a callback. There are some <a href="../../features/callback_library/#callback_library">premade callbacks in the callback library</a> which handle these sorts of things like projecting to manifolds and preserving positivity.</p><h5 id="Symplectic-integrators-don&#39;t-conserve-energy"><a class="docs-heading-anchor" href="#Symplectic-integrators-don&#39;t-conserve-energy">Symplectic integrators don&#39;t conserve energy</a><a id="Symplectic-integrators-don&#39;t-conserve-energy-1"></a><a class="docs-heading-anchor-permalink" href="#Symplectic-integrators-don&#39;t-conserve-energy" title="Permalink"></a></h5><p>Yes, symplectic integrators do not exactly conserve energy. It is a common misconception that they do. What symplectic integrators actually do is solve for a trajectory which rests on a symplectic manifold that is perturbed from the true solution&#39;s manifold by the truncation error. This means that symplectic integrators do not experience (very much) long time drift, but their orbit is not exactly the same as the true solution in phase space and thus you will see differences in energy that tend to look periodic. There is a small drift which grows linearly and is related to floating point error, but this drift is much less than standard methods. This is why symplectic methods are recommended for long time integration.</p><p>For conserving energy, there are a few things you can do. First of all, the energy error is related to the integration error, so simply solving with higher accuracy will reduce the error. The results in the <a href="https://github.com/SciML/DiffEqBenchmarks.jl">DiffEqBenchmarks</a> show that using a <code>DPRKN</code> method with low tolerance can be a great choice. Another thing you can do is use <a href="../../features/callback_library/#callback_library">the ManifoldProjection callback from the callback library</a>.</p><h4 id="How-to-get-to-zero-error"><a class="docs-heading-anchor" href="#How-to-get-to-zero-error">How to get to zero error</a><a id="How-to-get-to-zero-error-1"></a><a class="docs-heading-anchor-permalink" href="#How-to-get-to-zero-error" title="Permalink"></a></h4><p>You can&#39;t. For floating point numbers, you shouldn&#39;t use below <code>abstol=1e-14</code> and <code>reltol=1e-14</code>. If you need lower than that, use arbitrary precision numbers like BigFloats or <a href="https://github.com/JuliaArbTypes/ArbFloats.jl">ArbFloats.jl</a>.</p><h2 id="Autodifferentiation-and-Dual-Numbers"><a class="docs-heading-anchor" href="#Autodifferentiation-and-Dual-Numbers">Autodifferentiation and Dual Numbers</a><a id="Autodifferentiation-and-Dual-Numbers-1"></a><a class="docs-heading-anchor-permalink" href="#Autodifferentiation-and-Dual-Numbers" title="Permalink"></a></h2><h4 id="Native-Julia-solvers-compatibility-with-autodifferentiation"><a class="docs-heading-anchor" href="#Native-Julia-solvers-compatibility-with-autodifferentiation">Native Julia solvers compatibility with autodifferentiation</a><a id="Native-Julia-solvers-compatibility-with-autodifferentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Native-Julia-solvers-compatibility-with-autodifferentiation" title="Permalink"></a></h4><p>Yes, they are compatible with automatic differentiation! Take a look at the <a href="basics/@ref sensitivity">sensitivity analysis</a> page for more details.</p><p>If the algorithm does not have differentiation of parameter-dependent events, then you simply need to make the initial condition have elements of Dual numbers. If the algorithm uses Dual numbers, you need to make sure that time is also given by Dual numbers.</p><p>To show this in action, let&#39;s say we want to find the Jacobian of solution of the Lotka-Volterra equation at <code>t=10</code> with respect to the parameters.</p><pre><code class="language-julia hljs">function func(du,u,p,t)
  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]
  du[2] = -3 * u[2] + u[1]*u[2]
end
function f(p)
  prob = ODEProblem(func,eltype(p).([1.0,1.0]),(0.0,10.0),p)
  # Lower tolerances to show the methods converge to the same value
  solve(prob,Tsit5(),save_everystep=false,abstol=1e-12,reltol=1e-12)[end]
end</code></pre><p>This function takes in new parameters and spits out the solution at the end. We make the inital condition <code>eltype(p).([1.0,1.0])</code> so that way it&#39;s typed to be Dual numbers whenever <code>p</code> is an array of <code>Dual</code> numbers, and we do the same for the timespan just to show what you&#39;d do if there was parameters-dependent events. Then we can take the Jacobian via ForwardDiff.jl:</p><pre><code class="language-julia hljs">using ForwardDiff
ForwardDiff.jacobian(f,[1.5,1.0])

2×2 Array{Float64,2}:
  2.16056   0.188569
 -6.25677  -0.697978</code></pre><p>and compare it to Calculus.jl:</p><pre><code class="language-julia hljs">Calculus.jacobian(f,[1.5,1.0],:central)

2×2 Array{Float64,2}:
  2.16056   0.188569
 -6.25677  -0.697978</code></pre><h4 id="I-get-Dual-number-errors-when-I-solve-my-ODE-with-Rosenbrock-or-SDIRK-methods"><a class="docs-heading-anchor" href="#I-get-Dual-number-errors-when-I-solve-my-ODE-with-Rosenbrock-or-SDIRK-methods">I get Dual number errors when I solve my ODE with Rosenbrock or SDIRK methods</a><a id="I-get-Dual-number-errors-when-I-solve-my-ODE-with-Rosenbrock-or-SDIRK-methods-1"></a><a class="docs-heading-anchor-permalink" href="#I-get-Dual-number-errors-when-I-solve-my-ODE-with-Rosenbrock-or-SDIRK-methods" title="Permalink"></a></h4><p>This is because you&#39;re using a cache which is not compatible with autodifferentiaion via ForwardDiff.jl. For example, if we use the ODE function:</p><pre><code class="language-julia hljs">using LinearAlgebra, OrdinaryDiffEq
function foo(du, u, (A, tmp), t)
    mul!(tmp, A, u)
    @. du = u + tmp
    nothing
end
prob = ODEProblem(foo, ones(5, 5), (0., 1.0), (ones(5,5), zeros(5,5)))
solve(prob, Rosenbrock23())</code></pre><p>Here we use a cached temporary array in order to avoid the allocations of matrix multiplication. When autodifferentiation occurs, the element type of <code>u</code> is <code>Dual</code> numbers, so <code>A*u</code> produces <code>Dual</code> numbers, so the error arises when it tries to write into <code>tmp</code>. There are two ways to avoid this. The first way, the easy way, is to just turn off autodifferentiation with the <code>autodiff=false</code> option in the solver. Every solver which uses autodifferentiation has this option. Thus we&#39;d solve this with:</p><pre><code class="language-julia hljs">prob = ODEProblem(f,ones(5, 5),(0.0,1.0))
sol = solve(prob,Rosenbrock23(autodiff=false))</code></pre><p>and it will use a numerical differentiation fallback (DiffEqDiffTools.jl) to calculate Jacobians.</p><p>We could use <code>get_tmp</code> and <code>dualcache</code> functions from <a href="https://github.com/SciML/PreallocationTools.jl">PreallocationTools.jl</a> to solve this issue, e.g.,</p><pre><code class="language-julia hljs">using LinearAlgebra, OrdinaryDiffEq, PreallocationTools
function foo(du, u, (A, tmp), t)
    tmp = get_tmp(tmp, first(u)*t)
    mul!(tmp, A, u)
    @. du = u + tmp
    nothing
end
prob = ODEProblem(foo, ones(5, 5), (0., 1.0), (ones(5,5), PreallocationTools.dualcache(zeros(5,5))))
solve(prob, TRBDF2()</code></pre><h2 id="Sparse-Jacobians"><a class="docs-heading-anchor" href="#Sparse-Jacobians">Sparse Jacobians</a><a id="Sparse-Jacobians-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Jacobians" title="Permalink"></a></h2><h4 id="I-get-errors-when-I-try-to-solve-my-problem-using-sparse-Jacobians"><a class="docs-heading-anchor" href="#I-get-errors-when-I-try-to-solve-my-problem-using-sparse-Jacobians">I get errors when I try to solve my problem using sparse Jacobians</a><a id="I-get-errors-when-I-try-to-solve-my-problem-using-sparse-Jacobians-1"></a><a class="docs-heading-anchor-permalink" href="#I-get-errors-when-I-try-to-solve-my-problem-using-sparse-Jacobians" title="Permalink"></a></h4><p>This is likely because you&#39;re using a Jacobian matrix with a sparsity structure that changes, which is incompatible with the default linear solver for sparse matrices.  If the linear solver catches the issue, you&#39;ll see the error message</p><pre><code class="nohighlight hljs">ERROR: ArgumentError: The pattern of the original matrix must match the pattern of the refactor.</code></pre><p>or</p><pre><code class="nohighlight hljs">ERROR: ArgumentError: pattern of the matrix changed</code></pre><p>though an <code>Error: SingularException</code> is also possible if the linear solver fails to detect that the sparsity structure changed. To address this issue, you&#39;ll need to disable caching the symbolic factorization, e.g., </p><pre><code class="language-julia hljs">solve(prob, Rodas4(linsolve=KLUFactorization(;reuse_symbolic=false))</code></pre><p>For more details about possible linear solvers, consult the <a href="http://linearsolve.sciml.ai/dev/">LinearSolve.jl documentation</a></p><h2 id="Odd-Error-Messages"><a class="docs-heading-anchor" href="#Odd-Error-Messages">Odd Error Messages</a><a id="Odd-Error-Messages-1"></a><a class="docs-heading-anchor-permalink" href="#Odd-Error-Messages" title="Permalink"></a></h2><h4 id="&quot;Error-Exception:-llvmcall-must-be-compiled-to-be-called&quot;-when-running-the-debugger?"><a class="docs-heading-anchor" href="#&quot;Error-Exception:-llvmcall-must-be-compiled-to-be-called&quot;-when-running-the-debugger?">&quot;Error Exception: <code>llvmcall</code> must be compiled to be called&quot; when running the debugger?</a><a id="&quot;Error-Exception:-llvmcall-must-be-compiled-to-be-called&quot;-when-running-the-debugger?-1"></a><a class="docs-heading-anchor-permalink" href="#&quot;Error-Exception:-llvmcall-must-be-compiled-to-be-called&quot;-when-running-the-debugger?" title="Permalink"></a></h4><p>The debugger is incompatible with <code>llvmcall</code> which is used in the <code>AutoSpecialize</code> form that is used to reduce the compile times. In order to make use of the debugger, make use of the <code>FullSpecialize</code> form. I.e., change <code>prob = ODEProblem(lorenz!,u0,tspan)</code> to <code>prob = ODEProblem{true, SciMLBase.FullSpecialize}(lorenz!,u0,tspan)</code>. We plan to have a fix for this but for now the workaround should be sufficient for all cases.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../problem/">« Problem Interface</a><a class="docs-footer-nextpage" href="../compatibility_chart/">Solver Compatibility Chart »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Saturday 10 December 2022 07:53">Saturday 10 December 2022</span>. Using Julia version 1.8.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
