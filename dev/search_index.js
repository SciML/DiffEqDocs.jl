var documenterSearchIndex = {"docs":
[{"location":"types/dynamical_types/#dynamical_prob","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"","category":"section"},{"location":"types/dynamical_types/#Solution-Type","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Solution Type","text":"Dynamical ODE solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"section"},{"location":"types/dynamical_types/#Hamiltonian-Problems","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Hamiltonian Problems","text":"HamiltonianProblems are provided by DiffEqPhysics.jl and provide an easy way to define equations of motion from the corresponding Hamiltonian. To define a HamiltonianProblem one only needs to specify the Hamiltonian:\n\nH(pq)\n\nand autodifferentiation (via ForwardDiff.jl) will create the appropriate equations.","category":"section"},{"location":"types/dynamical_types/#Constructors","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Constructors","text":"HamiltonianProblem{T}(H, p0, q0, tspan, param = nothing; kwargs...)","category":"section"},{"location":"types/dynamical_types/#Fields","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Fields","text":"H: The Hamiltonian H(p,q,params) which returns a scalar.\np0: The initial momentums.\nq0: The initial positions.\ntspan: The timespan for the problem.\nparam: Defaults to nothing. param will be passed to H's params.","category":"section"},{"location":"types/dynamical_types/#SciMLBase.DynamicalODEProblem","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"SciMLBase.DynamicalODEProblem","text":"Defines a dynamical ordinary differential equation (ODE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/dynamical_types/\n\nDynamical ordinary differential equations, such as those arising from the definition of a Hamiltonian system or a second order ODE, have a special structure that can be utilized in the solution of the differential equation. On this page, we describe how to define second order differential equations for their efficient numerical solution.\n\nMathematical Specification of a Dynamical ODE Problem\n\nThese algorithms require a Partitioned ODE of the form:\n\nbeginalign*\nfracdvdt = f_1(ut) \nfracdudt = f_2(v) \nendalign*\n\nThis is a Partitioned ODE partitioned into two groups, so the functions should be specified as f1(dv,v,u,p,t) and f2(du,v,u,p,t) (in the inplace form), where f1 is independent of v (unless specified by the solver), and f2 is independent of u and t. This includes discretizations arising from SecondOrderODEProblems where the velocity is not used in the acceleration function, and Hamiltonians where the potential is (or can be) time-dependent, but the kinetic energy is only dependent on v.\n\nNote that some methods assume that the integral of f2 is a quadratic form. That means that f2=v'*M*v, i.e. int f_2 = frac12 m v^2, giving du = v. This is equivalent to saying that the kinetic energy is related to v^2. The methods which require this assumption will lose accuracy if this assumption is violated. Methods listed make note of this requirement with \"Requires quadratic kinetic energy\".\n\nConstructor\n\nDynamicalODEProblem(f::DynamicalODEFunction,v0,u0,tspan,p=NullParameters();kwargs...)\nDynamicalODEProblem{isinplace}(f1,f2,v0,u0,tspan,p=NullParameters();kwargs...)\n\nDefines the ODE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFields\n\nf1 and f2: The functions in the ODE.\nv0 and u0: The initial conditions.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n\n\n","category":"type"},{"location":"types/dynamical_types/#SciMLBase.SecondOrderODEProblem","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"SciMLBase.SecondOrderODEProblem","text":"Defines a second order ordinary differential equation (ODE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/dynamical_types/\n\nMathematical Specification of a 2nd Order ODE Problem\n\nTo define a 2nd Order ODE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nu = f(uupt)\n\nf should be specified as f(du,u,p,t) (or in-place as f(ddu,du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nFrom this form, a dynamical ODE:\n\nbeginalign*\nv = f(vupt) \nu = v\nendalign*\n\nis generated.\n\nConstructors\n\nSecondOrderODEProblem{isinplace}(f,du0,u0,tspan,callback=CallbackSet())\n\nDefines the ODE with the specified functions.\n\nFields\n\nf: The function for the second derivative.\ndu0: The initial derivative.\nu0: The initial condition.\ntspan: The timespan for the problem.\ncallback: A callback to be applied to every solver which uses the problem. Defaults to nothing.\n\n\n\n\n\n","category":"type"},{"location":"types/dynamical_types/#SciMLBase.DynamicalODEFunction","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"SciMLBase.DynamicalODEFunction","text":"struct DynamicalODEFunction{iip, specialize, F1, F2, TMM, Ta, Tt, TJ, JVP, VJP, JP, SP, TW, TWt, TPJ, O, TCV, SYS, ID} <: SciMLBase.AbstractODEFunction{iip}\n\nA representation of an ODE function f, defined by:\n\nM fracdudt = f(upt)\n\nas a partitioned ODE:\n\nbeginalign*\nM_1 fracdudt = f_1(upt) \nM_2 fracdudt = f_2(upt)\nendalign*\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDynamicalODEFunction{iip,specialize}(f1,f2;\n                                    mass_matrix = __has_mass_matrix(f) ? f.mass_matrix : I,\n                                    analytic = __has_analytic(f) ? f.analytic : nothing,\n                                    tgrad= __has_tgrad(f) ? f.tgrad : nothing,\n                                    jac = __has_jac(f) ? f.jac : nothing,\n                                    jvp = __has_jvp(f) ? f.jvp : nothing,\n                                    vjp = __has_vjp(f) ? f.vjp : nothing,\n                                    jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                                    sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                                    paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n                                    colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n                                    sys = __has_sys(f) ? f.sys : nothing)\n\nNote that only the functions f_i themselves are required. These functions should be given as f_i!(du,u,p,t) or du = f_i(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M_i represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://docs.sciml.ai/DiffEqDocs/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator. Should be given as a tuple of mass matrices, i.e. (M_1, M_2) for the mass matrices of equations 1 and 2 respectively.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracf(upt)t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivative fracdfdu v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivative fracdfdu^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DynamicalODEFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"solvers/bvp_solve/#bvp_solve","page":"BVP Solvers","title":"BVP Solvers","text":"solve(prob::BVProblem, alg, dt; kwargs)\nsolve(prob::TwoPointBVProblem, alg, dt; kwargs)\nsolve(prob::SecondOrderBVProblem, alg, dt; kwargs)\nsolve(prob::SecondOrderTwoPointBVProblem, alg, dt; kwargs)\n\nSolves the BVP defined by prob using the algorithm alg. All algorithms except Shooting and MultipleShooting methods should specify a dt which is the step size for the discretized mesh.","category":"section"},{"location":"solvers/bvp_solve/#Recommended-Methods","page":"BVP Solvers","title":"Recommended Methods","text":"The MIRK methods are recommended in most scenarios given their improved stability properties over the other methods. They have adaptivty and sparsity handling which allows for them to handle large-scale and difficult problems. However, they are not compatible with callbacks / event handling (i.e. discontinuities), and in such cases Shooting methods are required. There are single shooting methods and multiple shooting methods available in BoundaryValueDiffEq.jl. Shooting methods should be used with an appropriate ODE solver such as Shooting(Tsit5()) or MultipleShooting(5, FBDF()). Additionally, in many cases, single shooting method Shooting may be faster than collocation methods if it converges, though it is a lot less numerically robust. Multiple shooting method MultipleShooting is more stable and robust than single shooting method Shooting.","category":"section"},{"location":"solvers/bvp_solve/#Full-List-of-Methods","page":"BVP Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/bvp_solve/#BoundaryValueDiffEq.jl","page":"BVP Solvers","title":"BoundaryValueDiffEq.jl","text":"","category":"section"},{"location":"solvers/bvp_solve/#Shooting-Methods","page":"BVP Solvers","title":"Shooting Methods","text":"Shooting(odealg()) - A wrapper over initial value problem solvers, it reduces BVP to an initial value problem and solves the IVP.\nMultipleShooting(N, odealg()) - A wrapper over initial value problem solvers, it reduces BVP to N initial value problems and solves these IVPs. Multiple Shooting usually maintains more numerical stability than Single Shooting.","category":"section"},{"location":"solvers/bvp_solve/#MIRK(Monotonic-Implicit-Runge-Kutta)-Methods","page":"BVP Solvers","title":"MIRK(Monotonic Implicit Runge-Kutta) Methods","text":"All MIRK methods have defect control adaptivity by default which adapts the mesh (dt) automatically. This can be turned off via the keyword argument adaptive = false.\n\nMIRK2 - A 2nd order collocation method using an implicit Runge-Kutta tableau with a sparse Jacobian.\nMIRK3 - A 3rd order collocation method using an implicit Runge-Kutta tableau with a sparse Jacobian.\nMIRK4 - A 4th order collocation method using an implicit Runge-Kutta tableau with a sparse Jacobian.\nMIRK5 - A 5th order collocation method using an implicit Runge-Kutta tableau with a sparse Jacobian.\nMIRK6 - A 6th order collocation method using an implicit Runge-Kutta tableau with a sparse Jacobian.","category":"section"},{"location":"solvers/bvp_solve/#FIRK(Fully-Implicit-Runge-Kutta)-methods","page":"BVP Solvers","title":"FIRK(Fully Implicit Runge-Kutta) methods","text":"Similar to MIRK methods, fully implicit Runge-Kutta methods construct nonlinear problems from the collocation equations of a BVP and solve such nonlinear systems to obtain numerical solutions of BVP. When solving large boundary value problems, choose a nested NonlinearSolve.jl solver by setting nested_nlsolve=true in FIRK solvers can achieve better performance.\n\nLobattoIIIa2 - A 2nd stage LobattoIIIa collocation method.\nLobattoIIIa3 - A 3rd stage LobattoIIIa collocation method.\nLobattoIIIa4 - A 4th stage LobattoIIIa collocation method.\nLobattoIIIa5 - A 5th stage LobattoIIIa collocation method.\nLobattoIIIb2 - A 2nd stage LobattoIIIa collocation method, doesn't support defect control adaptivity.\nLobattoIIIb3 - A 3rd stage LobattoIIIa collocation method.\nLobattoIIIb4 - A 4th stage LobattoIIIa collocation method.\nLobattoIIIb5 - A 5th stage LobattoIIIa collocation method.\nLobattoIIIc2 - A 2nd stage LobattoIIIa collocation method, doesn't support defect control adaptivity.\nLobattoIIIc3 - A 3rd stage LobattoIIIa collocation method.\nLobattoIIIc4 - A 4th stage LobattoIIIa collocation method.\nLobattoIIIc5 - A 5th stage LobattoIIIa collocation method.\nRadauIIa1 - A 1st stage Radau collocation method, doesn't support defect control adaptivity.\nRadauIIa2 - A 2nd stage Radau collocation method.\nRadauIIa3 - A 3rd stage Radau collocation method.\nRadauIIa5 - A 5th stage Radau collocation method.\nRadauIIa7 - A 7th stage Radau collocation method.","category":"section"},{"location":"solvers/bvp_solve/#Gauss-Legendre-collocation-methods","page":"BVP Solvers","title":"Gauss Legendre collocation methods","text":"The Ascher collocation methods are similar with MIRK and FIRK methods but have extension for BVDAE prblem solving, the error control is based on instead of defect control adaptivity.\n\nAscher1 - A 1st stage Gauss Legendre collocation method with Ascher's error control adaptivity.\nAscher2 - A 2nd stage Gauss Legendre collocation method with Ascher's error control adaptivity.\nAscher3 - A 3rd stage Gauss Legendre collocation method with Ascher's error control adaptivity.\nAscher4 - A 4th stage Gauss Legendre collocation method with Ascher's error control adaptivity.\nAscher5 - A 5th stage Gauss Legendre collocation method with Ascher's error control adaptivity.\nAscher6 - A 6th stage Gauss Legendre collocation method with Ascher's error control adaptivity.\nAscher7 - A 7th stage Gauss Legendre collocation method with Ascher's error control adaptivity.","category":"section"},{"location":"solvers/bvp_solve/#MIRKN(Monotonic-Implicit-Runge-Kutta-Nystöm)-methods","page":"BVP Solvers","title":"MIRKN(Monotonic Implicit Runge-Kutta-Nystöm) methods","text":"MIRKN4 - A 4th order collocation method using an implicit Runge-Kutta-Nyström tableau without defect control adaptivity.\nMIRKN6 - A 6th order collocation method using an implicit Runge-Kutta-Nyström tableau without defect control adaptivity.","category":"section"},{"location":"solvers/bvp_solve/#SimpleBoundaryValueDiffEq.jl","page":"BVP Solvers","title":"SimpleBoundaryValueDiffEq.jl","text":"SimpleMIRK4 - A simplified 4th order collocation method using an implicit Runge-Kutta tableau.\nSimpleMIRK5 - A simplified 5th order collocation method using an implicit Runge-Kutta tableau.\nSimpleMIRK6 - A simplified 6th order collocation method using an implicit Runge-Kutta tableau.\nSimpleShooting - A simplified single Shooting method.","category":"section"},{"location":"solvers/bvp_solve/#ODEInterface.jl","page":"BVP Solvers","title":"ODEInterface.jl","text":"ODEInterface.jl can be used seamlessly with BoundaryValueDiffEq.jl, after we define our model using BVProblem or TwoPointBVProblem, we can directly call the solvers from ODEInterface.jl.\n\nBVPM2 - FORTRAN code for solving two-point boundary value problems. BVPM2 is only compatible with TwoPointBVProblem.\nBVPSOL - FORTRAN77 code which solves highly nonlinear two point boundary value problems using a local linear solver (condensing algorithm) or a global sparse linear solver for the solution of the arising linear subproblems, by Peter Deuflhard, Georg Bader, Lutz Weimann. BVPSOL should be used with TwoPointBVProblem and initial guess.\nCOLNEW - A Fortran77 code solves a multi-points boundary value problems for a mixed order system of ODEs by Uri Ascher and Georg Bader. It incorporates a new basis representation replacing b-splines, and improvements for the linear and nonlinear algebraic equation solvers. COLNEW support TwoPointBVProblem by default. To solve multi-points BVP using COLNEW, special form of multi-points boundary conditions should be provided by COLNEW(bc_func, dbc_func, zeta) where bc_func(i, z, res) is the multi-points boundary conditions, dbc_func(i, z, dbc) is the i-th row of jacobian of boundary conditions.","category":"section"},{"location":"tutorials/bvp_example/#Boundary-Value-Problems","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.\n\nIn this example, we will solve the ODE that satisfies the boundary condition in the form of\n\nbeginaligned\nfracdudt = f(t u) \ng(u) = vec0\nendaligned","category":"section"},{"location":"tutorials/bvp_example/#Example-1:-Simple-Pendulum","page":"Boundary Value Problems","title":"Example 1: Simple Pendulum","text":"The concrete example that we are solving is the simple pendulum ddotu+fracgLsin(u)=0 on the time interval tin0fracpi2. First, we need to define the ODE\n\nimport BoundaryValueDiffEq as BVP\nimport Plots\nconst g = 9.81\nL = 1.0\ntspan = (0.0, pi / 2)\nfunction simplependulum!(du, u, p, t)\n    θ = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -(g / L) * sin(θ)\nend\n\nThere are two problem types available:\n\nA problem type for general boundary conditions BVProblem (including conditions that may be anywhere/ everywhere on the integration interval, aka multi-points BVP).\nA problem type for boundaries that are specified at the beginning and the end of the integration interval TwoPointBVProblem (aka two-points BVP)\n\nThe boundary conditions are specified by a function that calculates the residual in-place from the problem solution, such that the residual is vec0 when the boundary condition is satisfied.\n\nThere are collocation and shooting methods for addressing boundary value problems in DifferentialEquations.jl. We need to use appropriate available BVP solvers to solve BVProblem. In this example, we use MIRK4 to solve the simple pendulum example.\n\nfunction bc1!(residual, u, p, t)\n    residual[1] = u(pi / 4)[1] + pi / 2 # the solution at the middle of the time span should be -pi/2\n    residual[2] = u(pi / 2)[1] - pi / 2 # the solution at the end of the time span should be pi/2\nend\nbvp1 = BVP.BVProblem(simplependulum!, bc1!, [pi / 2, pi / 2], tspan)\nsol1 = BVP.solve(bvp1, BVP.MIRK4(); dt = 0.05)\nPlots.plot(sol1)\n\nThe third argument of BVProblem or TwoPointBVProblem is the initial guess of the solution, which can be specified as a Vector, a Function of t, or a solution object from previous solving. In this example, the initial guess is set as a Vector.\n\nimport OrdinaryDiffEq as ODE\nu₀_2 = [-1.6, -1.7] # the initial guess\nfunction bc3!(residual, sol, p, t)\n    residual[1] = sol(pi / 4)[1] + pi / 2 # use the interpolation here, since indexing will be wrong for adaptive methods\n    residual[2] = sol(pi / 2)[1] - pi / 2\nend\nbvp3 = BVP.BVProblem(simplependulum!, bc3!, u₀_2, tspan)\nsol3 = BVP.solve(bvp3, BVP.Shooting(ODE.Vern7()))\n\nThe initial guess can also be supplied via a function of t or a previous solution type, which is especially handy for parameter analysis. We changed u to sol to emphasize the fact that in this case, the boundary condition can be written on the solution object. Thus, all the features on the solution type such as interpolations are available when using both collocation and shooting methods (i.e., you can have a boundary condition saying that the maximum over the interval is 1 using an optimization function on the continuous output).\n\nPlots.plot(sol3)\n\nTwoPointBVProblem is operationally the same as BVProblem but allows for the solver to specialize on the common form of being a two-point BVP, i.e. a BVP which only has boundary conditions at the start and the finish of the time interval. Defining a similar problem as TwoPointBVProblem is shown in the following example:\n\nfunction bc2a!(resid_a, u_a, p) # u_a is at the beginning of the time span\n    resid_a[1] = u_a[1] + pi / 2 # the solution at the beginning of the time span should be -pi/2\nend\nfunction bc2b!(resid_b, u_b, p) # u_b is at the ending of the time span\n    resid_b[1] = u_b[1] - pi / 2 # the solution at the end of the time span should be pi/2\nend\nbvp2 = BVP.TwoPointBVProblem(simplependulum!, (bc2a!, bc2b!), [pi / 2, pi / 2], tspan;\n    bcresid_prototype = (zeros(1), zeros(1)))\nsol2 = BVP.solve(bvp2, BVP.MIRK4(); dt = 0.05)\nPlots.plot(sol2)\n\nNote here that bc2a! is a boundary condition for the first time point, and bc2b! is a boundary condition for the final time point. bcresid_prototype is a prototype array which is passed in order to know the size of resid_a and resid_b. In this case, we have one residual term for the start and one for the final time point, and thus we have bcresid_prototype = (zeros(1), zeros(1)).","category":"section"},{"location":"tutorials/bvp_example/#Example-2:-Directly-Solving-with-Second-Order-BVP","page":"Boundary Value Problems","title":"Example 2: Directly Solving with Second Order BVP","text":"Suppose we want to solve the second order BVP system which can be formulated as\n\nbegincases\nu_1(x)= u_2(x)\nε u_2(x)=-u_1(x)u_2(x)- u_3(x)u_3(x)\nε u_3(x)=u_1(x)u_3(x)- u_1(x) u_3 (x)\nendcases\n\nwith initial conditions:\n\nbeginalign*\nu_1(0) = u_1(0)= u_1(1)=u_1(1)=0 \nu_3(0) = -1 \nu_3(1) = 1\nendalign*\n\nThe common way of solving the second order BVP is to define intermediate variables and transform the second order system into first order one, however, DifferentialEquations.jl allows the direct solving of second order BVP system to achieve more efficiency and higher continuity of the numerical solution.\n\nfunction f!(ddu, du, u, p, t)\n    ε = 0.1\n    ddu[1] = u[2]\n    ddu[2] = (-u[1] * du[2] - u[3] * du[3]) / ε\n    ddu[3] = (du[1] * u[3] - u[1] * du[3]) / ε\nend\nfunction bc!(res, du, u, p, t)\n    res[1] = u(0.0)[1]\n    res[2] = u(1.0)[1]\n    res[3] = u(0.0)[3] + 1\n    res[4] = u(1.0)[3] - 1\n    res[5] = du(0.0)[1]\n    res[6] = du(1.0)[1]\nend\nu0 = [1.0, 1.0, 1.0]\ntspan = (0.0, 1.0)\nprob = BVP.SecondOrderBVProblem(f!, bc!, u0, tspan)\nsol = BVP.solve(prob, BVP.MIRKN4(;\n    jac_alg = BVP.BVPJacobianAlgorithm(BVP.AutoForwardDiff())); dt = 0.01)","category":"section"},{"location":"tutorials/bvp_example/#Example-3:-Semi-Explicit-Boundary-Value-Differential-Algebraic-Equations","page":"Boundary Value Problems","title":"Example 3: Semi-Explicit Boundary Value Differential-Algebraic Equations","text":"Consider a semi-explicit boundary value differential-algebraic equation formulated as\n\nbegincases\nx_1 = left(ε + x_2 - sin(t)right) y + cos(t) \nx_2 = cos(t) \nx_3 = y \n0 = left(x_1 - sin(t)right) left(y - e^tright)\nendcases\n\nwith boundary conditions\n\nbeginalign*\nx_1(0)=0 \nx_3(0)=1 \nx_2(1)=sin(1)\nendalign*\n\nWe need to choose the Ascher methods for solving BVDAEs.\n\nfunction f!(du, u, p, t)\n    du[1] = (1 + u[2] - sin(t)) * u[4] + cos(t)\n    du[2] = cos(t)\n    du[3] = u[4]\n    du[4] = (u[1] - sin(t)) * (u[4] - exp(t))\nend\nfunction bc!(res, u, p, t)\n    res[1] = u[1]\n    res[2] = u[3] - 1\n    res[3] = u[2] - sin(1.0)\nend\nu0 = [0.0, 0.0, 0.0, 0.0]\ntspan = (0.0, 1.0)\nfun = BVP.BVPFunction(f!, bc!, mass_matrix = [1 0 0 0; 0 1 0 0; 0 0 1 0; 0 0 0 0])\nprob = BVP.BVProblem(fun, u0, tspan)\nsolver = BVP.Ascher4(; zeta = [0.0, 0.0, 1.0], jac_alg = BVP.BVPJacobianAlgorithm(BVP.AutoForwardDiff()))\nsol = BVP.solve(prob, solver; dt = 0.01)","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#Basic-Nonstiff-Methods","page":"Basic Nonstiff Methods","title":"Basic Nonstiff Methods","text":"This page covers the fundamental explicit methods for solving SDEs. These methods are suitable for non-stiff problems and provide the foundation for more advanced algorithms.","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#Euler-Maruyama-Methods","page":"Basic Nonstiff Methods","title":"Euler-Maruyama Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#EM-Euler-Maruyama","page":"Basic Nonstiff Methods","title":"EM - Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#EulerHeun-Euler-Heun","page":"Basic Nonstiff Methods","title":"EulerHeun - Euler-Heun","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#LambaEM-Adaptive-Euler-Maruyama","page":"Basic Nonstiff Methods","title":"LambaEM - Adaptive Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#LambaEulerHeun-Adaptive-Euler-Heun","page":"Basic Nonstiff Methods","title":"LambaEulerHeun - Adaptive Euler-Heun","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#Milstein-Methods","page":"Basic Nonstiff Methods","title":"Milstein Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#RKMil-Runge-Kutta-Milstein","page":"Basic Nonstiff Methods","title":"RKMil - Runge-Kutta Milstein","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#Split-Methods","page":"Basic Nonstiff Methods","title":"Split Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#SplitEM-Split-Euler-Maruyama","page":"Basic Nonstiff Methods","title":"SplitEM - Split Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#When-to-Use-Basic-Methods","page":"Basic Nonstiff Methods","title":"When to Use Basic Methods","text":"Use EM when:\n\nComputational efficiency is most important\nProblem is not stiff\nAny noise type (most flexible)\nSimple implementation needed\n\nUse EulerHeun when:\n\nWorking in Stratonovich interpretation\nNeed slightly better accuracy than EM\nProblem has non-commutative noise\n\nUse LambaEM/LambaEulerHeun when:\n\nWant adaptive time stepping with basic methods\nNeed error control but not high accuracy\nGood balance of simplicity and adaptivity\n\nUse RKMil when:\n\nHigher accuracy than Euler methods\nProblem has diagonal or scalar noise\nStrong order 1.0 convergence required\n\nThese methods form the foundation of stochastic numerical analysis. While higher-order methods often provide better performance, the basic methods are essential for:\n\nInitial testing and prototyping\nProblems where simplicity is preferred\nEducational purposes\nFallback options when advanced methods fail","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#StochasticDiffEq.EM","page":"Basic Nonstiff Methods","title":"StochasticDiffEq.EM","text":"EM: Nonstiff Method The Euler-Maruyama method is the simplest and most fundamental numerical method for solving stochastic differential equations.\n\nMethod Properties\n\nStrong Order: 0.5 (in the Itô sense)\nWeak Order: 1.0\nTime stepping: Fixed time step only\nNoise types: All forms (diagonal, non-diagonal, scalar, additive, and colored noise)\nSDE interpretation: Itô\n\nParameters\n\nsplit::Bool = true: Controls step splitting for improved stability with large diffusion eigenvalues\n\nWhen to Use\n\nFirst choice for simple SDE problems\nWhen computational efficiency is more important than accuracy\nFor problems with all noise types including non-commutative noise\nWhen step splitting is needed for stability with large diffusion terms\n\nAlgorithm Description\n\nThe method discretizes the SDE:\n\ndu = f(u,p,t)dt + g(u,p,t)dW\n\nusing the scheme:\n\nu_{n+1} = u_n + f(u_n,p,t_n)Δt + g(u_n,p,t_n)ΔW_n\n\nWhen split=true, the method uses step splitting which can improve stability for problems with large diffusion eigenvalues.\n\nReferences\n\nKloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\", Springer (1992)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#StochasticDiffEq.EulerHeun","page":"Basic Nonstiff Methods","title":"StochasticDiffEq.EulerHeun","text":"EulerHeun()\n\nEulerHeun: Euler-Heun Method (Nonstiff)\n\nThe Euler-Heun method is the Stratonovich analog of the Euler-Maruyama method, providing strong order 0.5 convergence in the Stratonovich sense.\n\nMethod Properties\n\nStrong Order: 0.5 (in the Stratonovich sense)\nWeak Order: 1.0\nTime stepping: Fixed time step only\nNoise types: All forms (diagonal, non-diagonal, scalar, additive, and colored noise)\nSDE interpretation: Stratonovich\n\nWhen to Use\n\nWhen working with Stratonovich SDEs\nFor problems naturally formulated in Stratonovich interpretation\nWhen physical interpretation requires Stratonovich calculus\nAs the Stratonovich counterpart to Euler-Maruyama\n\nAlgorithm Description\n\nFor Stratonovich SDEs:\n\ndu = f(u,p,t)dt + g(u,p,t)∘dW\n\nThe method uses:\n\nu_{n+1} = u_n + f(u_n,p,t_n)Δt + g(u_n + 0.5*g(u_n,p,t_n)ΔW_n, p, t_n)ΔW_n\n\nStratonovich vs Itô\n\nEulerHeun: For Stratonovich SDEs\nEM: For Itô SDEs\nConversion between interpretations changes the drift term\n\nReferences\n\nKloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\", Springer (1992)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#StochasticDiffEq.LambaEM","page":"Basic Nonstiff Methods","title":"StochasticDiffEq.LambaEM","text":"LambaEM(split=true)\n\nLambaEM: Adaptive Euler-Maruyama Method (Nonstiff)\n\nAdaptive time-stepping version of the Euler-Maruyama method with error estimation based on the work of Lamba and Rackauckas.\n\nMethod Properties\n\nStrong Order: 0.5 (in the Itô sense)\nWeak Order: 1.0\nTime stepping: Adaptive with embedded error estimation\nNoise types: All forms (diagonal, non-diagonal, scalar, additive, and colored noise)\nSDE interpretation: Itô\n\nParameters\n\nsplit::Bool = true: Controls step splitting for improved stability\n\nWhen to Use\n\nWhen adaptive time stepping is needed with basic Euler-Maruyama\nFor problems requiring error control without high-order accuracy\nWhen computational efficiency and adaptivity are both important\nFor non-commutative noise where higher-order methods aren't applicable\n\nAlgorithm Description\n\nExtends EM with adaptive time stepping using error estimation. The method computes two approximations and uses their difference to estimate local error.\n\nError Control\n\nEmbedded error estimation for adaptive stepping\nAccepts standard tolerances (abstol, reltol)\nAutomatic step size adjustment\n\nReferences\n\nBased on error estimation work by Lamba and Rackauckas\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#StochasticDiffEq.LambaEulerHeun","page":"Basic Nonstiff Methods","title":"StochasticDiffEq.LambaEulerHeun","text":"LambaEulerHeun()\n\nLambaEulerHeun: Adaptive Euler-Heun Method (Nonstiff)\n\nAdaptive time-stepping version of the Euler-Heun method with error estimation for Stratonovich SDEs.\n\nMethod Properties\n\nStrong Order: 0.5 (in the Stratonovich sense)\nWeak Order: 1.0\nTime stepping: Adaptive with embedded error estimation\nNoise types: All forms (diagonal, non-diagonal, scalar, additive, and colored noise)\nSDE interpretation: Stratonovich\n\nWhen to Use\n\nWhen adaptive time stepping is needed for Stratonovich SDEs\nFor problems requiring error control in Stratonovich interpretation\nWhen computational efficiency and adaptivity are both important\nFor non-commutative noise in Stratonovich formulation\n\nAlgorithm Description\n\nAdaptive version of EulerHeun method with error estimation for automatic step size control.\n\nError Control\n\nEmbedded error estimation for adaptive stepping\nStandard tolerance control (abstol, reltol)\nAutomatic step size adjustment for Stratonovich problems\n\nReferences\n\nError estimation methodology by Lamba, adapted by Rackauckas\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#StochasticDiffEq.RKMil","page":"Basic Nonstiff Methods","title":"StochasticDiffEq.RKMil","text":"Kloeden, P.E., Platen, E., Numerical Solution of Stochastic Differential Equations. Springer. Berlin Heidelberg (2011)\n\nRKMil(;interpretation=AlgorithmInterpretation.Ito)\n\nRKMil: Runge-Kutta Milstein Method (Nonstiff)\n\nExplicit Runge-Kutta discretization of the Milstein method achieving strong order 1.0 convergence for diagonal and scalar noise.\n\nMethod Properties\n\nStrong Order: 1.0 (for diagonal/scalar noise)\nWeak Order: Depends on tableau\nTime stepping: Adaptive\nNoise types: Diagonal and scalar noise only\nSDE interpretation: Configurable (Itô or Stratonovich)\n\nParameters\n\ninterpretation: Choose AlgorithmInterpretation.Ito (default) or AlgorithmInterpretation.Stratonovich\n\nWhen to Use\n\nWhen higher accuracy than Euler methods is needed\nFor diagonal or scalar noise problems\nWhen strong order 1.0 convergence is required\nAlternative to SRI methods for simpler noise structures\n\nRestrictions\n\nOnly works with diagonal or scalar noise\nFor non-diagonal noise, use RKMilCommute or RKMilGeneral\nFor general noise, use SRI/SRA methods\n\nAlgorithm Description\n\nImplements the Milstein scheme using Runge-Kutta techniques:\n\ndu = f(u,t)dt + g(u,t)dW + 0.5*g(u,t)*g'(u,t)*(dW^2 - dt)\n\nwhere g'(u,t) is the derivative of g with respect to u.\n\nReferences\n\nKloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\", Springer (1992)\nMilstein, G.N., \"Numerical Integration of Stochastic Differential Equations\"\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/basic_methods/#StochasticDiffEq.SplitEM","page":"Basic Nonstiff Methods","title":"StochasticDiffEq.SplitEM","text":"SplitEM()\n\nSplitEM: Split-Step Euler-Maruyama Method (Nonstiff)\n\nSplit-step version of the Euler-Maruyama method that separates the drift and diffusion terms for improved stability.\n\nMethod Properties\n\nStrong Order: 0.5 (in the Itô sense)\nWeak Order: 1.0\nTime stepping: Fixed time step\nNoise types: All forms (diagonal, non-diagonal, scalar, additive, and colored noise)\nSDE interpretation: Itô\n\nWhen to Use\n\nWhen standard EM has stability issues with large diffusion terms\nAlternative to EM with split=true\nFor problems where operator splitting is natural\nWhen drift and diffusion have different timescales\n\nAlgorithm Description\n\nApplies operator splitting to treat drift and diffusion separately:\n\nStep 1: u* = u_n + f(u_n,t_n)Δt     (drift step)\nStep 2: u_{n+1} = u* + g(u*,t_n)ΔW_n (diffusion step)\n\nReferences\n\nOperator splitting methods for SDEs\n\n\n\n\n\n","category":"type"},{"location":"basics/faq/#faq","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This page is a compilation of frequently asked questions and answers.","category":"section"},{"location":"basics/faq/#faq_stability","page":"Frequently Asked Questions","title":"Stability and Divergence of ODE Solves","text":"For guidelines on debugging ODE solve issues, see PSA: How to help yourself debug differential equation solving issues.","category":"section"},{"location":"basics/faq/#My-model-is-reporting-unstable-results.-What-can-I-do?","page":"Frequently Asked Questions","title":"My model is reporting unstable results. What can I do?","text":"First of all, don't panic. You may have experienced one of the following warnings:\n\ndt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.NaN dt detected. Likely a NaN value in the state, parameters, or derivative value caused this outcome.Instability detected. Aborting\n\nThese are all pointing to a similar behavior: for some reason or another, the ODE solve is diverging to infinity. As it diverges to infinity, the dt of the integrator will drop (trying to control the speed and error), so it will either hit the minimum dt, hit dt=NaN, or have a value in the ODE hit Inf. Whichever one occurs first will throw the respective warning.\n\nHow to handle this? 99.99% of the time this has been debugged, it has turned out to be an error in the user's model! A missing minus sign, an incorrect term, etc. There are many other behaviors to watch out for. In some ODEs, increasing a parameter can cause a bifurcation so that the solution diverges. With u'=a*u, if a is negative then, it nicely falls to zero, but if a is positive the solution quickly diverges to infinity! This means, double-check your parameters are indexed correctly!\n\nNote: if you see these warnings during a parameter estimation process, this is likely the underlying problem. Simply check SciMLBase.successful_retcode(sol) and throw an Inf cost. Most optimizers will then reject steps in those parameter regimes!\n\nThere are a few other things to check as well. Often, the stability of an ODE solve improves as you decrease the tolerance, so you may want to try a smaller abstol and reltol. One behavior to watch out for is that if your model is a differential-algebraic equation and your DAE is of high index (say index>1), this can impact the numerical solution. In this case, you may want to use the ModelingToolkit.jl index reduction tools to improve the numerical stability of a solve. In addition, if it's a highly stiff ODE/DAE that is large, and you're using a matrix-free solver (such as GMRES), make sure the tolerance of the GMRES is well-tuned and an appropriate preconditioner is applied. Finally, try other solvers. They all have different stability, so try Tsit5(), Vern7(), QNDF(), Rodas5(), TRBDF2(), KenCarp4(), Sundials.CVODE_BDF(), etc. and see what works.\n\nIf none of this works out, double-check that your ODE truly has the behavior that you believe it should. This is one of the most common issues: your intuition may be deceiving. For example, u' = -sqrt(u) with u(0)=1 cannot hit zero because its derivative shrinks to zero, right? Wrong! It will hit zero in a finite time, after which the solution is undefined and does not have a purely real solution. u' = u^2 - 100u will “usually” go to zero, but if u(0)>10 then it will go to infinity. Plot out your diverging solution and see whether the asymptotics are correct: if u[i] gets big, do your equations make u'[i] positive and growing? That would be a problem!\n\nLet's say you don't believe you made an error at all, and you want to file a bug report. To do so, you'll first want to prove that it's isolated to a solver. If it's a solver issue, then you shouldn't see it happen with every single solver. Do you think it's an issue with the Julia solvers? Well fortunately, DifferentialEquations.jl offers direct unmodified wrappers to almost all previously built solvers, so if you think it's a Julia issue, try running your ODE through:\n\nSundials.jl, a wrapper for the C++ SUNDIALS library though CVODE_Adams, CVODE_BDF, IDA, and ARKODE.\nODEInterfaceDiffEq.jl, a wrapper for the classic Hairer Fortran codes like dorpi5, dop853, radau, rodas, etc.\nLSODA.jl, a wrapper for the classic lsoda algorithm.\nMATLABDiffEq.jl, a wrapper for the MATLAB ODE solvers ode45, ode15s, etc.\nSciPyDiffEq.jl, a wrapper for SciPy's odeint (LSODA) and other methods (LSODE, etc.).\ndeSolveDiffEq.jl, a wrapper for the commonly used R library.\n\nAnd many more. Testing this is as simple as changing solve(prob,Tsit5()) to solve(prob,lsoda()), so please give this a try. If you translated your code from another language, like Python or MATLAB, use the direct wrapper to double check the steps are the same. If they are not, then your ODE is not the same, because it's using a direct call to the solvers of those packages!\n\nIf your ODE diverges to infinity with every ODE solver ever made, the problem is most likely not the ODE solvers. Or rather, to put it in meme form:\n\n(Image: )\n\nDon't be like Patrick. If after trying these ideas, your ODE solve still seems to have issues, and you haven't narrowed it down, feel free to ask on the Julia Discourse to get some help diagnosing it. If you did find a solver issue, please open an issue on the GitHub repository.","category":"section"},{"location":"basics/faq/#A-larger-maxiters-seems-to-be-needed,-but-it's-already-high?","page":"Frequently Asked Questions","title":"A larger maxiters seems to be needed, but it's already high?","text":"If you see:\n\nInterrupted. Larger maxiters is needed.\n\nNote that it could quite possibly arise just from having a very long timespan. If you check sol.t from the returned object, and it looks like it's stepping at reasonable lengths, feel free to just pass maxiters=... into solve to bump it up from the default of Int(1e5).\n\nBut if your maxiters is already high, then the problem is likely that your model is stiff. A stiff ODE requires very small timesteps from many explicit solvers, such as Tsit5(), Vern7(), etc., and thus those methods are not appropriate for this kind of problem. You will want to change to a different method, like Rodas5(), Rosenbrock23(), TRBDF2(), KenCarp4(), or QNDF().","category":"section"},{"location":"basics/faq/#My-ODE-goes-negative-but-should-stay-positive,-what-tools-can-help?","page":"Frequently Asked Questions","title":"My ODE goes negative but should stay positive, what tools can help?","text":"There are many tools to help! However, let's first focus on one piece first: when you say “should” be positive, what do you mean by “should”? If you mean “mathematically you can prove that the ODE with these values and these initial conditions will have a solution that is positive for all time”, then yes, you're looking in the right place. If by “should” you mean “it's a model of biochemical reactions, so the concentration should always be positive”, then ask yourself first, did you write down a model where it will always be positive?\n\nThe following set of tools are designed to accuracy enforce positivity in ODE models, which mathematically should be positive in the true solution. If they encounter a model that is actually going negative, they will work really hard to get a positive but correct solution, which is impossible, so they will simply error out. This can be more subtle than you think. Solving u'=-sqrt(u) is not guaranteed to stay positive, even though the derivative goes to zero as u goes to zero (check the analytical solution if you're curious). Similarly, analyzing nonlinear models can showcase all sorts of behavior. A common cause for accidental negativity is Hill functions in systems biology models: just because derivatives go to zero doesn't mean they are going to zero fast enough to keep things positive!\n\nWith that in mind, let's see the options.\n\nThe simplest trick is to change the solver tolerance. Reduce abstol (and maybe reltol) a bit. That can help reduce the error and thus keep the solution positive. For some more difficult equations, changing to a stiff ODE solver like Rosenbrock23() QNDF, or TRBDF2() can be helpful.\n\nIf those don't work, call out the big guns. One of them is isoutofdomain, where you can define a boolean function which will cause step rejections whenever it is not satisfied. For example, isoutofdomain = (u,p,t)->any(x->x<0,u) will make the solver reject any step which cases any variable u to go negative. Now, using any pure-Julia solver with this option, it's impossible to get a negative in the result! One thing you may see though is:\n\ndt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.\n\nor\n\nInterrupted. Larger maxiters is needed.\n\nWhat this means is that enforcing positivity is not possible. It keeps rejecting steps that go negative, reducing dt, taking another step, rejecting, reducing, repeat until dt hits dtmin or it hits maxiters. This means that even when trying to solve the problem with the most accurate infinitesimal dt, the solution still goes negative. Are you sure the true solution is supposed to be positive? If you see this, check for issues like a missing minus sign in your equations.\n\nIf that works but is a little slow, the domain handling callbacks in the callback library are designed to function similarly but in a way that gets better performance. Instead of repeating lots of steps through rejections, it interpolates back to still take a smaller step, always progressing forwards. However, this can be a bit less stable, so its applicability depends on the equation, and once again this requires that the solution is truly positive. If the true solution goes negative, it will repeatedly try interpolating backwards until it can no longer and end with a dtmin issue.\n\nFinally, note that ODE solvers will not be more correct than tolerance, and so one should expect that if the solution is supposed to be positive but abstol=1e-12, you may end up with u[i]=-1e-12. That is okay, that is expected behavior of numerical solvers, the ODE solver is still doing its job. If this is a major issue for your application, you may want to write your model to be robust to this behavior, such as changing sqrt(u[i]) to sqrt(max(0,u[i])). You should also consider transforming your values, like solving for u^2 or exp(u) instead of u, which mathematically can only be positive. Look into using a tool like ModelingToolkit.jl for automatically transforming your equations.","category":"section"},{"location":"basics/faq/#I'm-trying-to-solve-DAEs-but-my-solver-is-unstable-and/or-slow,-what's-wrong-with-IDA-and-DFBDF?","page":"Frequently Asked Questions","title":"I'm trying to solve DAEs but my solver is unstable and/or slow, what's wrong with IDA and DFBDF?","text":"Fully implicit DAEs f(duupt) = 0 are extremely difficult to numerically handle for many reasons. The linearly implicit form Mu=f(u) where M is a singular mass matrix is much simpler numerically and thus results in much better performance. This is seen in many instances with the SciMLBenchmarks. Thus it is recommended that in almost all or most situations, one should use the mass matrix form of the DAE solver.\n\nHowever, it is generally recommended that if you are solving a DAE that you use ModelingToolkit.jl because it has many utilities for pre-processing DAEs to make them more numerically stable. For example, if your algebraic conditions are not uniquely matching to algebraic variables (i.e. you have at least one unique algebraic variable per algebraic condition), then the system is what is known as high index and thus the numerical DAE solvers will not be able to accurately solve the equation without rewriting the equations. ModelingToolkit is able to automatically detect this kind of condition and perform the equation transformation automatically. As such, if you are having difficulties with a DAE system, it is highly recommended to try modelingtookitize to transform the system to MTK's formulation and running mtkcompile to see how it would change the equations, simply convert the model to MTK.","category":"section"},{"location":"basics/faq/#faq_performance","page":"Frequently Asked Questions","title":"Performance","text":"","category":"section"},{"location":"basics/faq/#GPUs,-multithreading-and-distributed-computation-support","page":"Frequently Asked Questions","title":"GPUs, multithreading and distributed computation support","text":"Yes. The *DiffEq.jl libraries (OrdinaryDiffEq.jl, StochasticDiffEq.jl, and DelayDiffEq.jl) are all written to be generic to the array and number types. This means they will adopt the implementation that is given by the array type. The in-place algorithms internally utilize Julia's broadcast and Julia's mul! in-place matrix multiplication function. The out-of-place algorithms utilize standard arithmetical functions. Both additionally utilize the user's norm specified via the common interface options and, if a stiff solver, ForwardDiff/DiffEqDiffTools for the Jacobian calculation, and Base linear factorizations for the linear solve. For your type, you may likely need to give a better form of the norm, Jacobian, or linear solve calculations to fully utilize parallelism.\n\nGPUArrays.jl (CuArrays.jl), ArrayFire.jl, DistributedArrays.jl have been tested and work in various forms, where the last one is still not recommended for common use yet.\n\nThe next question is whether it matters. Generally, your system has to be large for parallelism to matter. Using a multithreaded array for broadcast we find helpful around N>1000, though the Sundials manual says N>100,000. For high order Runge-Kutta methods it's likely lower than the Sundials estimate because of more operations packed into each internal step, but as always, that will need more benchmarks to be precise and will depend on the problem being solved. GPUs generally require some intensive parallel operation in the user's f function to be viable, for example, a matrix multiplication for a stencil computation in a PDE. If you're simply solving some ODE element-wise on a big array, it likely won't do much, or it will slow things down just due to how GPUs work. DistributedArrays require parallel linear solves to really matter, and thus are only recommended when you have a problem that cannot fit into memory or are using a stiff solver with a Krylov method for the linear solves.","category":"section"},{"location":"basics/faq/#Note-About-Setting-Up-Your-Julia-Installation-for-Speed:-BLAS-Choices","page":"Frequently Asked Questions","title":"Note About Setting Up Your Julia Installation for Speed: BLAS Choices","text":"Julia uses an underlying BLAS implementation for its matrix multiplications and factorizations. This library is automatically multithreaded and accelerates the internal linear algebra of DifferentialEquations.jl. However, for optimality, you should make sure that the number of BLAS threads that you are using matches the number of physical cores and not the number of logical cores. See this issue for more details.\n\nTo check the number of BLAS threads, use:\n\nccall((:openblas_get_num_threads64_, Base.libblas_name), Cint, ())\n\nIf I want to set this directly to 4 threads, I would use:\n\nimport LinearAlgebra\nLinearAlgebra.BLAS.set_num_threads(4)\n\nAdditionally, sometimes Intel's MKL might be a faster BLAS than the standard BLAS that ships with Julia (OpenBLAS). To switch your BLAS implementation, you can use MKL.jl, which will accelerate the linear algebra routines. This is done via:\n\nimport MKL","category":"section"},{"location":"basics/faq/#My-ODE-is-solving-really-slow","page":"Frequently Asked Questions","title":"My ODE is solving really slow","text":"First, check for bugs. These solvers go through a ton of convergence tests and so if there's a solver issue, it's either just something to do with how numerical methods work or it's a user-error (generally the latter, though check the later part of the FAQ on normal numerical errors). User-errors in the f function causing a divergence of the solution is the most common reason for reported slow codes.\n\nIf you have no bugs, great! The standard tricks for optimizing Julia code then apply. Take a look at the Optimizing DiffEq Code tutorial for some tips and pointers.\n\nWhat you want to do first is make sure your function does not allocate. If your system is small (<=100 ODEs/SDEs/DDEs/DAEs?), then you should set your system up to use StaticArrays.jl. This is demonstrated in the ODE tutorial with static matrices. Static vectors/arrays are stack-allocated, and thus creating new arrays is free, and the compiler doesn't have to heap-allocate any of the temporaries (that's the expensive part!). These have specialized superfast dispatches for arithmetic operations and extra things like LU-factorizations, and thus they are preferred when possible. However, they lose efficiency if they grow too large.\n\nFor anything larger, you should use the in-place syntax f(du,u,p,t) and make sure that your function doesn't allocate. Assuming you know of a u0, you should be able to do:\n\ndu = similar(u0)\n@time f(du, u0, p, t)\n\nand see close to zero allocations and close to zero memory allocated. If you see more, then you might have a type-instability or have temporary arrays. To find type-instabilities, you should do:\n\n@code_warntype f(du, u, p, t)\n\nand read the printout to see if there are any types that aren't inferred by the compiler, and fix them. If you have any global variables, you should make them const. As for allocations, some common things that allocate are:\n\nArray slicing, like u[1:5]. Instead, use @view u[1:5]\nMatrix multiplication with *. Instead of A*b, use mul!(c,A,b) for some pre-allocated cache vector c.\nNon-broadcasted expressions. Every expression on arrays should .= into another array, or it should be re-written to loop and do computations with scalar (or static array) values.\n\nFor an example of optimizing a function resulting from a PDE discretization, see this blog post.","category":"section"},{"location":"basics/faq/#The-stiff-solver-takes-forever-to-take-steps-for-my-PDE-discretization","page":"Frequently Asked Questions","title":"The stiff solver takes forever to take steps for my PDE discretization","text":"The solvers for stiff solvers require solving a nonlinear equation each step. To do so, they have to do a few Newton steps. By default, these methods assume that the Jacobian is dense, automatically calculate the Jacobian for you, and do a dense factorization. However, in many cases you may want to use alternatives that are more tuned for your problem.\n\nFirst of all, when available, it's recommended that you pass a function for computing your Jacobian. This is discussed in the performance overloads section. Jacobians are especially helpful for Rosenbrock methods.\n\nSecondly, if your Jacobian isn't dense, you shouldn't use a dense Jacobian! Instead, if you're using  a *DiffEq library you should specify a linear solver and/or a jac_prototype for the matrix form, and for Sundials.jl, you should change the linear_solver option. See the ODE solve Sundials portion for details on that.\n\nRight now, QNDF is the recommended method for stiff problems with large sparse Jacobians. You should specify jac_prototype as a special matrix, such as a banded or tridiagonal matrix, if it satisfies a special structure. If you only know the Jacobian is sparse, using automated sparsity detection can help with identifying the sparsity pattern. See the stiff ODE tutorial for more details. Lastly, using LinSolveGMRES() can help if a sparsity pattern cannot be obtained, but the matrix is large, or if the sparsity cannot fit into memory. Once again, a good reference for how to handle PDE discretizations can be found at this blog post.","category":"section"},{"location":"basics/faq/#My-Problem-Has-Discontinuities-and-is-Unstable-/-Slow","page":"Frequently Asked Questions","title":"My Problem Has Discontinuities and is Unstable / Slow","text":"This Discourse post goes into detail for how to handle discontinuities in your ODE function and how to use that extra information to speed up the solver.","category":"section"},{"location":"basics/faq/#Complicated-Models","page":"Frequently Asked Questions","title":"Complicated Models","text":"","category":"section"},{"location":"basics/faq/#Switching-ODE-functions-in-the-middle-of-integration","page":"Frequently Asked Questions","title":"Switching ODE functions in the middle of integration","text":"There are a few ways to do this. The simplest way is to just have a parameter to switch between the two. For example:\n\nfunction f(du, u, p, t)\n    if p == 0\n        du[1] = 2u[1]\n    else\n        du[1] = -2u[1]\n    end\n    du[2] = -u[2]\nend\n\nThen in a callback, you can make the affect! function modify integrator.prob.p. For example, we can make it change when u[2]<0.5 via:\n\ncondition(t, u, integrator) = u[2] - 0.5\naffect!(integrator) = integrator.p = 1\n\nThen it will change between the two ODE choices for du1 at that moment. Another way to do this is to make the ODE functions all be the same type via FunctionWrappers.jl, but that is unnecessary. With the way that modern processors work, there exists branch prediction and thus execution of a conditional is free if it's predictable which branch will be taken. In this case, almost every call to f takes the p==0 route until the callback, at which point it is almost always the else route. Therefore, the processor will effectively get rid of the computational cost associated with this, so you're likely over-optimizing if you're going further (unless this change happens every step, but even then, this is probably the cheapest part of the computation…).","category":"section"},{"location":"basics/faq/#Numerical-Error","page":"Frequently Asked Questions","title":"Numerical Error","text":"","category":"section"},{"location":"basics/faq/#What-does-tolerance-mean-and-how-much-error-should-I-expect","page":"Frequently Asked Questions","title":"What does tolerance mean and how much error should I expect","text":"The most useful options are the tolerances abstol and reltol. These tell the internal adaptive time stepping engine how precise of a solution you want. Generally, reltol is the relative accuracy while abstol is the accuracy when u is near zero. These tolerances are local tolerances and thus are not global guarantees. However, a good rule of thumb is that the total solution accuracy is 1-2 digits less than the relative tolerances. Thus, for the defaults abstol=1e-6 and reltol=1e-3, you can expect a global accuracy of about 1-2 digits. This is standard across the board and applies to the native Julia methods, the wrapped Fortran and C++ methods, the calls to MATLAB/Python/R, etc.","category":"section"},{"location":"basics/faq/#The-solver-doesn't-obey-physical-law-X-(e.g.-conservation-of-energy)","page":"Frequently Asked Questions","title":"The solver doesn't obey physical law X (e.g. conservation of energy)","text":"Yes, this is because the numerical solution of the ODE is not the exact solution. There are a few ways that you can handle this problem. One way is to get a more exact solution. Thus instead of\n\nsol = solve(prob, alg)\n\nuse\n\nsol = solve(prob, alg, abstol = 1e-10, reltol = 1e-10)\n\nOf course, there's always a tradeoff between accuracy and efficiency, so play around to find out what's right for your problem.\n\nAnother thing you can do is use a callback. There are some premade callbacks in the callback library which handle these sorts of things like projecting to manifolds and preserving positivity.","category":"section"},{"location":"basics/faq/#Symplectic-integrators-don't-conserve-energy","page":"Frequently Asked Questions","title":"Symplectic integrators don't conserve energy","text":"Yes, symplectic integrators do not exactly conserve energy. It is a common misconception that they do. What symplectic integrators actually do is solve for a trajectory which rests on a symplectic manifold that is perturbed from the true solution's manifold by the truncation error. This means that symplectic integrators do not experience (very much) longtime drift, but their orbit is not exactly the same as the true solution in phase space, and thus you will see differences in energy that tend to look periodic. There is a small drift which grows linearly and is related to floating-point error, but this drift is much less than standard methods. This is why symplectic methods are recommended for longtime integration.\n\nFor conserving energy, there are a few things you can do. First of all, the energy error is related to the integration error, so simply solving with higher accuracy will reduce the error. The results in the SciMLBenchmarks show that using a DPRKN method with low tolerance can be a great choice. Another thing you can do is use the ManifoldProjection callback from the callback library.","category":"section"},{"location":"basics/faq/#How-to-get-to-zero-error","page":"Frequently Asked Questions","title":"How to get to zero error","text":"You can't. For floating-point numbers, you shouldn't use below abstol=1e-14 and reltol=1e-14. If you need lower than that, use arbitrary precision numbers like BigFloats or ArbNumerics.jl.","category":"section"},{"location":"basics/faq/#Autodifferentiation-and-Dual-Numbers","page":"Frequently Asked Questions","title":"Autodifferentiation and Dual Numbers","text":"","category":"section"},{"location":"basics/faq/#Native-Julia-solvers-compatibility-with-autodifferentiation","page":"Frequently Asked Questions","title":"Native Julia solvers compatibility with autodifferentiation","text":"Yes, they are compatible with automatic differentiation! Take a look at the sensitivity analysis page for more details.\n\nIf the algorithm does not have differentiation of parameter-dependent events, then you simply need to make the initial condition have elements of Dual numbers. If the algorithm uses Dual numbers, you need to make sure that time is also given by Dual numbers.\n\nTo show this in action, let's say we want to find the Jacobian of solution of the Lotka-Volterra equation at t=10 with respect to the parameters.\n\nimport DifferentialEquations as DE\nfunction func(du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2]\n    du[2] = -3 * u[2] + u[1] * u[2]\nend\nfunction f(p)\n    prob = DE.ODEProblem(func, eltype(p).([1.0, 1.0]), (0.0, 10.0), p)\n    # Lower tolerances to show the methods converge to the same value\n    DE.solve(prob, DE.Tsit5(), save_everystep = false, abstol = 1e-12, reltol = 1e-12)[end]\nend\n\nThis function takes in new parameters and spits out the solution at the end. We make the initial condition eltype(p).([1.0,1.0]) so that way it's typed to be Dual numbers whenever p is an array of Dual numbers, and we do the same for the timespan just to show what you'd do if there were parameters-dependent events. Then we can take the Jacobian via ForwardDiff.jl:\n\nimport ForwardDiff\nForwardDiff.jacobian(f, [1.5, 1.0])\n\nand compare it to FiniteDiff.jl:\n\nimport FiniteDiff\nFiniteDiff.finite_difference_jacobian(f, [1.5, 1.0])","category":"section"},{"location":"basics/faq/#I-get-Dual-number-errors-when-I-solve-my-ODE-with-Rosenbrock-or-SDIRK-methods","page":"Frequently Asked Questions","title":"I get Dual number errors when I solve my ODE with Rosenbrock or SDIRK methods","text":"This is because you're using a cache which is incompatible with autodifferentiation via ForwardDiff.jl. For example, if we use the ODE function:\n\nimport LinearAlgebra, OrdinaryDiffEq as ODE, DifferentialEquations\nfunction foo(du, u, (A, tmp), t)\n    mul!(tmp, A, u)\n    @. du = u + tmp\n    nothing\nend\nprob = DE.ODEProblem(foo, ones(5, 5), (0.0, 1.0), (ones(5, 5), zeros(5, 5)))\nDE.solve(prob, ODE.Rosenbrock23())\n\nHere we use a cached temporary array to avoid the allocations of matrix multiplication. When autodifferentiation occurs, the element type of u is Dual numbers, so A*u produces Dual numbers, so the error arises when it tries to write into tmp. There are two ways to avoid this. The first way, the easy way, is to just turn off autodifferentiation with the autodiff=false option in the solver. Every solver which uses autodifferentiation has this option. Thus, we'd solve this with:\n\nimport DifferentialEquations as DE, OrdinaryDiffEq as ODE\nprob = DE.ODEProblem(f, ones(5, 5), (0.0, 1.0))\nsol = DE.solve(prob, ODE.Rosenbrock23(autodiff = false))\n\nand it will use a numerical differentiation fallback (DiffEqDiffTools.jl) to calculate Jacobians.\n\nWe could use get_tmp and dualcache functions from PreallocationTools.jl to solve this issue, e.g.,\n\nimport LinearAlgebra, OrdinaryDiffEq as ODE, PreallocationTools, DifferentialEquations\nfunction foo(du, u, (A, tmp), t)\n    tmp = PreallocationTools.get_tmp(tmp, first(u) * t)\n    mul!(tmp, A, u)\n    @. du = u + tmp\n    nothing\nend\nprob = DE.ODEProblem(foo, ones(5, 5), (0.0, 1.0),\n    (ones(5, 5), PreallocationTools.dualcache(zeros(5, 5))))\nDE.solve(prob, ODE.TRBDF2())","category":"section"},{"location":"basics/faq/#Sparse-Jacobians","page":"Frequently Asked Questions","title":"Sparse Jacobians","text":"","category":"section"},{"location":"basics/faq/#I-get-errors-when-I-try-to-solve-my-problem-using-sparse-Jacobians","page":"Frequently Asked Questions","title":"I get errors when I try to solve my problem using sparse Jacobians","text":"This is likely because you're using a Jacobian matrix with a sparsity structure that changes, which is incompatible with the default linear solver for sparse matrices.  If the linear solver catches the issue, you'll see the error message\n\nERROR: ArgumentError: The pattern of the original matrix must match the pattern of the refactor.\n\nor\n\nERROR: ArgumentError: pattern of the matrix changed\n\nthough, an Error: SingularException is also possible if the linear solver fails to detect that the sparsity structure changed. To address this issue, you'll need to disable caching the symbolic factorization, e.g.,\n\nimport DifferentialEquations as DE, OrdinaryDiffEq as ODE, LinearSolve\nDE.solve(prob, ODE.Rodas4(linsolve = LinearSolve.KLUFactorization(;\n    reuse_symbolic = false)))\n\nFor more details about possible linear solvers, consult the LinearSolve.jl documentation","category":"section"},{"location":"basics/faq/#Odd-Error-Messages","page":"Frequently Asked Questions","title":"Odd Error Messages","text":"","category":"section"},{"location":"basics/faq/#“Error-Exception:-llvmcall-must-be-compiled-to-be-called”-when-running-the-debugger?","page":"Frequently Asked Questions","title":"“Error Exception: llvmcall must be compiled to be called” when running the debugger?","text":"The debugger is incompatible with llvmcall which is used in the AutoSpecialize form that is used to reduce the compile times. In order to make use of the debugger, make use of the FullSpecialize form. I.e., change prob = ODEProblem(lorenz!,u0,tspan) to prob = ODEProblem{true, SciMLBase.FullSpecialize}(lorenz!,u0,tspan). We plan to have a fix for this, but for now, the workaround should be sufficient for all cases.","category":"section"},{"location":"basics/problem/#Problem-Interface","page":"Problem Interface","title":"Problem Interface","text":"This page defines the common problem interface. There are certain rules that can be applied to any function definition, and this page defines those behaviors.","category":"section"},{"location":"basics/problem/#In-place-vs-Out-of-Place-Function-Definition-Forms","page":"Problem Interface","title":"In-place vs Out-of-Place Function Definition Forms","text":"Every problem definition has an in-place and out-of-place form, commonly referred throughout DiffEq as IIP (isinplace) and OOP (out of place). The in-place form is a mutating form. For example, on ODEs, we have that f!(du,u,p,t) is the in-place form which, as its output, mutates du. Whatever is returned is simply ignored. Similarly, for OOP we have the form du=f(u,p,t) which uses the return.\n\nEach of the problem types has the first argument as the optional mutating argument. The SciMLBase system will automatically determine the functional form and place a specifier isinplace on the function to carry as type information whether the function defined for this DEProblem is in-place. However, every constructor allows for manually specifying the in-placeness of the function. For example, this can be done at the problem level like:\n\nODEProblem{true}(f, u0, tspan, p)\n\nwhich declares that isinplace=true. Similarly, this can be done at the DEFunction level. For example:\n\nODEFunction{true}(f, jac = myjac)","category":"section"},{"location":"basics/problem/#Type-Specifications","page":"Problem Interface","title":"Type Specifications","text":"Throughout DifferentialEquations.jl, the types that are given in a problem are the types used for the solution. If an initial value u0 is needed for a problem, then the state variable u will match the type of that u0. Similarly, if time exists in a problem the type for t will be derived from the types of the tspan. Parameters p can be any type, and the type will be matching how it's defined in the problem.\n\nFor internal matrices, such as Jacobians and Brownian caches, these also match the type specified by the user. jac_prototype and rand_prototype can thus be any Julia matrix type which is compatible with the operations that will be performed.","category":"section"},{"location":"basics/problem/#Functional-and-Condensed-Problem-Inputs","page":"Problem Interface","title":"Functional and Condensed Problem Inputs","text":"Note that the initial condition can be written as a function of parameters and initial time:\n\nu0(p, t0)\n\nand be resolved before going to the solver. Additionally, the initial condition can be a distribution from Distributions.jl, in which case a sample initial condition will be taken each time init or solve is called.\n\nIn addition, tspan supports the following forms. The single value form t is equivalent to (zero(t),t). The functional form is allowed:\n\ntspan(p)\n\nwhich outputs a tuple.","category":"section"},{"location":"basics/problem/#Examples","page":"Problem Interface","title":"Examples","text":"import DifferentialEquations as DE\nprob = DE.ODEProblem((u, p, t) -> u, (p, t0) -> p[1], (p) -> (0.0, p[2]), (2.0, 1.0))\n\nimport Distributions\nprob = DE.ODEProblem((u, p, t) -> u, (p, t) -> Distributions.Normal(p, 1), (0.0, 1.0), 1.0)","category":"section"},{"location":"basics/problem/#Lower-Level-__init-and-__solve","page":"Problem Interface","title":"Lower Level __init and __solve","text":"At the high level, known problematic problems will emit warnings before entering the solver to better clarify the error to the user. The following cases are checked if the solver is adaptive:\n\nInteger times warn\nDual numbers must be in the initial conditions and timespans\nMeasurements.jl values must be in the initial conditions and timespans\n\nIf there is an exception to these rules, please file an issue. If one wants to go around the high level solve interface and its warnings, one can call __init or __solve instead.","category":"section"},{"location":"basics/problem/#Modification-of-problem-types","page":"Problem Interface","title":"Modification of problem types","text":"Problem-related types in DifferentialEquations.jl are immutable.  This helps, e.g., parallel solvers to efficiently handle problem types.\n\nHowever, you may want to modify the problem after it is created.  For example, to simulate it for longer timespan.  It can be done by the remake function:\n\nprob1 = DE.ODEProblem((u, p, t) -> u / 2, 1.0, (0.0, 1.0))\nprob2 = DE.remake(prob1; tspan = (0.0, 2.0))\n\nA general syntax of remake is\n\nmodified_problem = remake(original_problem;\n    field_1 = value_1,\n    field_2 = value_2,\n    ...)\n\nwhere field_N and value_N are renamed to appropriate field names and new desired values.","category":"section"},{"location":"solvers/discrete_solve/#Discrete-Solvers","page":"Discrete Solvers","title":"Discrete Solvers","text":"","category":"section"},{"location":"solvers/discrete_solve/#DiscreteProblems","page":"Discrete Solvers","title":"DiscreteProblems","text":"solve(prob::DiscreteProblem,alg;kwargs)\n\nSolves the discrete function map defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"section"},{"location":"solvers/discrete_solve/#Recommended-Methods","page":"Discrete Solvers","title":"Recommended Methods","text":"The implementation for solving discrete equations is the FunctionMap algorithm in OrdinaryDiffEq.jl. It allows the full common interface (including events/callbacks) to solve function maps, along with everything else like plot recipes, while completely ignoring the ODE functionality related to continuous equations (except for a tiny bit of initialization). However, the SimpleFunctionMap from SimpleDiffEq.jl can be more efficient if the mapping function is sufficiently cheap, but it doesn't have all the extras like callbacks and saving support (but does have an integrator interface).","category":"section"},{"location":"solvers/discrete_solve/#Full-List-of-Methods","page":"Discrete Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/discrete_solve/#OrdinaryDiffEq.jl","page":"Discrete Solvers","title":"OrdinaryDiffEq.jl","text":"FunctionMap: A basic function map which implements the full common interface.\n\nOrdinaryDiffEq.jl also contains the FunctionMap algorithm which lets you It has a piecewise constant interpolation and allows for all the callback/event handling capabilities (of course, with rootfind=false. If a ContinuousCallback is given, it's always assumed rootfind=false).\n\nThe constructor is:\n\nFunctionMap()\nFunctionMap{scale_by_time}()\n\nEvery step is the update\n\nu_n+1 = f(t_n+1u_n)\n\nIf in addition scale_by_time is marked true (default is false), then every step is the update:\n\nu_n+1 = u_n + dtf(t_n+1u_n)\n\nNotice that this is the same as updates from the Euler method, except in this case we assume that it's a discrete change and thus the interpolation is piecewise constant.","category":"section"},{"location":"solvers/discrete_solve/#SimpleDiffEq.jl","page":"Discrete Solvers","title":"SimpleDiffEq.jl","text":"SimpleFunctionMap: A bare-bones implementation of a function map. Is optimally-efficient and has an integrator interface version, but does not support callbacks or saving controls.","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#OrdinaryDiffEqExtrapolation","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation","text":"Explicit extrapolation methods that achieve high accuracy through Richardson extrapolation of basic integration schemes. These methods provide adaptive order capabilities and natural parallelism, though they are generally outclassed by modern Runge-Kutta methods for most non-stiff problems.","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Key-Properties","page":"OrdinaryDiffEqExtrapolation","title":"Key Properties","text":"Extrapolation methods provide:\n\nAdaptive order capability allowing arbitrarily high orders\nNatural parallelism across different substep sequences\nHigh accuracy potential for very smooth problems\nRichardson extrapolation to eliminate lower-order error terms\nAutomatic stepsize and order control\nTheoretical appeal but often practical limitations","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#When-to-Use-Extrapolation-Methods","page":"OrdinaryDiffEqExtrapolation","title":"When to Use Extrapolation Methods","text":"These methods are recommended for:\n\nVery smooth problems where high-order accuracy is beneficial\nExtremely low tolerance requirements where adaptive order helps\nParallel computing environments that can exploit the natural parallelism\nResearch applications exploring adaptive order techniques\nProblems where other high-order methods struggle with accuracy","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Important-Limitations","page":"OrdinaryDiffEqExtrapolation","title":"Important Limitations","text":"Generally outclassed by modern explicit RK methods (Tsit5, Verner methods)\nHigher computational overhead compared to optimized RK methods\nBest suited for very smooth functions - poor performance on non-smooth problems\nParallel efficiency gains often don't compensate for increased work","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Mathematical-Background","page":"OrdinaryDiffEqExtrapolation","title":"Mathematical Background","text":"Extrapolation methods use sequences of basic integrators (like Euler or midpoint) with different stepsizes, then apply Richardson extrapolation to achieve higher-order accuracy. The adaptive order capability comes from using longer extrapolation sequences.","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Solver-Selection-Guide","page":"OrdinaryDiffEqExtrapolation","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Explicit-extrapolation-methods","page":"OrdinaryDiffEqExtrapolation","title":"Explicit extrapolation methods","text":"AitkenNeville: Euler extrapolation using Aitken-Neville algorithm\nExtrapolationMidpointDeuflhard: Midpoint extrapolation with barycentric coordinates\nExtrapolationMidpointHairerWanner: Midpoint extrapolation following ODEX algorithm","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#When-to-consider-these-methods","page":"OrdinaryDiffEqExtrapolation","title":"When to consider these methods","text":"Very low tolerances (< 1e-12) where adaptive order might help\nExtremely smooth problems with analytic solutions\nParallel computing scenarios with many available cores\nComparison studies with other high-order methods","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Better-alternatives-for-most-problems","page":"OrdinaryDiffEqExtrapolation","title":"Better alternatives for most problems","text":"For high accuracy: Use Verner methods (Vern7, Vern8, Vern9)\nFor general problems: Use Tsit5 or appropriate RK method\nFor stiff problems: Consider implicit extrapolation methods","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Performance-Notes","page":"OrdinaryDiffEqExtrapolation","title":"Performance Notes","text":"Consider stiff extrapolation methods which can perform very well for sufficiently stiff problems\nTest against Verner methods before choosing extrapolation for high accuracy\nParallelism benefits are problem and hardware dependent\nMost effective on very smooth, well-behaved problems\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqExtrapolation\", \"ExtrapolationMidpointDeuflhard\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#Full-list-of-solvers","page":"OrdinaryDiffEqExtrapolation","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#OrdinaryDiffEqExtrapolation.AitkenNeville","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation.AitkenNeville","text":"AitkenNeville(; max_order::Int = 10,\n                min_order::Int = 1,\n                init_order = 3,\n                thread = OrdinaryDiffEq.False())\n\nParallelized Explicit Extrapolation Method. Euler extrapolation using Aitken-Neville with the Romberg Sequence.\n\nKeyword Arguments\n\nmax_order: maximum order of the adaptive order algorithm.\nmin_order: minimum order of the adaptive order algorithm.\ninit_order: initial order of the adaptive order algorithm.\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@inproceedings{elrod2022parallelizing,   title={Parallelizing explicit and implicit extrapolation methods for ordinary differential equations},   author={Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher and others},   booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},   pages={1–9},   year={2022},   organization={IEEE}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#OrdinaryDiffEqExtrapolation.ExtrapolationMidpointDeuflhard","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation.ExtrapolationMidpointDeuflhard","text":"ExtrapolationMidpointDeuflhard(; max_order = 10,\n                                 min_order = 1,\n                                 init_order = 5,\n                                 thread = OrdinaryDiffEq.True(),\n                                 sequence = :harmonic,\n                                 sequence_factor = 2)\n\nParallelized Explicit Extrapolation Method. Midpoint extrapolation using Barycentric coordinates.\n\nKeyword Arguments\n\nmax_order: maximum order of the adaptive order algorithm.\nmin_order: minimum order of the adaptive order algorithm.\ninit_order: initial order of the adaptive order algorithm.\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch.\nsequence_factor: denotes which even multiple of sequence to take while evaluating internal discretizations.\n\nReferences\n\n@inproceedings{elrod2022parallelizing,   title={Parallelizing explicit and implicit extrapolation methods for ordinary differential equations},   author={Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher and others},   booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},   pages={1–9},   year={2022},   organization={IEEE}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Extrapolation/#OrdinaryDiffEqExtrapolation.ExtrapolationMidpointHairerWanner","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation.ExtrapolationMidpointHairerWanner","text":"ExtrapolationMidpointHairerWanner(; max_order = 10,\n                                    min_order = 2,\n                                    init_order = 5,\n                                    thread = OrdinaryDiffEq.True(),\n                                    sequence = :harmonic,\n                                    sequence_factor = 2)\n\nParallelized Explicit Extrapolation Method. Midpoint extrapolation using Barycentric coordinates,     following Hairer's ODEX in the adaptivity behavior.\n\nKeyword Arguments\n\nmax_order: maximum order of the adaptive order algorithm.\nmin_order: minimum order of the adaptive order algorithm.\ninit_order: initial order of the adaptive order algorithm.\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch.\nsequence_factor: denotes which even multiple of sequence to take while evaluating internal discretizations.\n\nReferences\n\n@inproceedings{elrod2022parallelizing,   title={Parallelizing explicit and implicit extrapolation methods for ordinary differential equations},   author={Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher and others},   booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},   pages={1–9},   year={2022},   organization={IEEE}}\n\n\n\n\n\n","category":"type"},{"location":"types/split_ode_types/#split_ode_prob","page":"Split ODE Problems","title":"Split ODE Problems","text":"","category":"section"},{"location":"types/split_ode_types/#Solution-Type","page":"Split ODE Problems","title":"Solution Type","text":"SplitODEProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"section"},{"location":"types/split_ode_types/#SciMLBase.SplitODEProblem","page":"Split ODE Problems","title":"SciMLBase.SplitODEProblem","text":"Defines a split ordinary differential equation (ODE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/split_ode_types/\n\nMathematical Specification of a Split ODE Problem\n\nTo define a SplitODEProblem, you simply need to give two functions f_1 and f_2 along with an initial condition u_0 which define an ODE:\n\nfracdudt =  f_1(upt) + f_2(upt)\n\nf should be specified as f(u,p,t) (or in-place as f(du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nMany splits are at least partially linear. That is the equation:\n\nfracdudt =  Au + f_2(upt)\n\nFor how to define a linear function A, see the documentation for the AbstractSciMLOperator.\n\nConstructors\n\nSplitODEProblem(f::SplitFunction,u0,tspan,p=NullParameters();kwargs...)\nSplitODEProblem{isinplace}(f1,f2,u0,tspan,p=NullParameters();kwargs...)\n\nThe isinplace parameter can be omitted and will be determined using the signature of f2. Note that both f1 and f2 should support the in-place style if isinplace is true or they should both support the out-of-place style if isinplace is false. You cannot mix up the two styles.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nUnder the hood, a SplitODEProblem is just a regular ODEProblem whose f is a SplitFunction. Therefore, you can solve a SplitODEProblem using the same solvers for ODEProblem. For solvers dedicated to split problems, see Split ODE Solvers.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf1, f2: The functions in the ODE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n\n\n","category":"type"},{"location":"types/split_ode_types/#SciMLBase.SplitFunction","page":"Split ODE Problems","title":"SciMLBase.SplitFunction","text":"struct SplitFunction{iip, specialize, F1, F2, TMM, C, Ta, Tt, TJ, JVP, VJP, JP, WP, SP, TW, TWt, TPJ, O, TCV, SYS, ID<:Union{Nothing, SciMLBase.OverrideInitData}, NLP<:Union{Nothing, SciMLBase.ODENLStepData}} <: SciMLBase.AbstractODEFunction{iip}\n\nA representation of a split ODE function f, defined by:\n\nM fracdudt = f_1(upt) + f_2(upt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nGenerally, for ODE integrators the f_1 portion should be considered the \"stiff portion of the model\" with larger timescale separation, while the f_2 portion should be considered the \"non-stiff portion\". This interpretation is directly used in integrators like IMEX (implicit-explicit integrators) and exponential integrators.\n\nConstructor\n\nSplitFunction{iip,specialize}(f1,f2;\n                             mass_matrix = __has_mass_matrix(f1) ? f1.mass_matrix : I,\n                             analytic = __has_analytic(f1) ? f1.analytic : nothing,\n                             tgrad= __has_tgrad(f1) ? f1.tgrad : nothing,\n                             jac = __has_jac(f1) ? f1.jac : nothing,\n                             jvp = __has_jvp(f1) ? f1.jvp : nothing,\n                             vjp = __has_vjp(f1) ? f1.vjp : nothing,\n                             jac_prototype = __has_jac_prototype(f1) ? f1.jac_prototype : nothing,\n                             W_prototype = __has_W_prototype(f1) ? f1.W_prototype : nothing,\n                             sparsity = __has_sparsity(f1) ? f1.sparsity : jac_prototype,\n                             paramjac = __has_paramjac(f1) ? f1.paramjac : nothing,\n                             colorvec = __has_colorvec(f1) ? f1.colorvec : nothing,\n                             sys = __has_sys(f1) ? f1.sys : nothing)\n\nNote that only the functions f_i themselves are required. These functions should be given as f_i!(du,u,p,t) or du = f_i(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of the SplitFunction. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://docs.sciml.ai/DiffEqDocs/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracf_1(upt)t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdf_1du\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivative fracdf_1du v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivative fracdf_1du^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nW_prototype: a prototype matrix matching the type that matches the W matrix. For example, if the Jacobian is tridiagonal, and the mass_matrix is diagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the W matrix. The default is nothing, which means a W of appropriate type for the jacobian and linear solver\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdf_1dp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\nNote on the Derivative Definition\n\nThe derivatives, such as the Jacobian, are only defined on the f1 portion of the split ODE. This is used to treat the f1 implicit while keeping the f2 portion explicit.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the SplitFunction type directly match the names of the inputs.\n\nSymbolically Generating the Functions\n\nSee the modelingtoolkitize function from ModelingToolkit.jl for automatically symbolically generating the Jacobian and more from the numerically-defined functions. See ModelingToolkit.SplitODEProblem for information on generating the SplitFunction from this symbolic engine.\n\n\n\n\n\n","category":"type"},{"location":"solvers/rode_solve/#RODE-Solvers","page":"RODE Solvers","title":"RODE Solvers","text":"","category":"section"},{"location":"solvers/rode_solve/#Recommended-Methods","page":"RODE Solvers","title":"Recommended Methods","text":"Currently, the only implemented method is the RandomEM method in StochasticDiffEq.jl. It is strong order alpha for an alpha-Holder continuous noise process.","category":"section"},{"location":"solvers/rode_solve/#Full-List-of-Methods","page":"RODE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/rode_solve/#StochasticDiffEq.jl","page":"RODE Solvers","title":"StochasticDiffEq.jl","text":"Each of the StochasticDiffEq.jl solvers come with a linear interpolation.\n\nRandomEM- The Euler-Maruyama method for RODEs. Strong order matching Holder continuity.\n\nExample usage:\n\nsol = solve(prob, RandomEM(), dt = 1 / 100)","category":"section"},{"location":"solvers/dde_solve/#DDE-Solvers","page":"DDE Solvers","title":"DDE Solvers","text":"solve(prob::AbstractDDEProblem, alg; kwargs)\n\nSolves the DDE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"section"},{"location":"solvers/dde_solve/#Recommended-Methods","page":"DDE Solvers","title":"Recommended Methods","text":"The recommended method for DDE problems are the MethodOfSteps algorithms. These are constructed from an OrdinaryDiffEq.jl algorithm as follows:\n\nMethodOfSteps(alg; constrained = false, fpsolve = NLFunctional(; max_iter = 10))\n\nwhere alg is an OrdinaryDiffEq.jl algorithm. Most algorithms should work.","category":"section"},{"location":"solvers/dde_solve/#Nonstiff-DDEs","page":"DDE Solvers","title":"Nonstiff DDEs","text":"The standard algorithm choice is MethodOfSteps(Tsit5()). This is a highly efficient FSAL 5th order algorithm with free interpolants which should handle most problems. For fast solving where non-strict error control is needed, choosing MethodOfSteps(BS3()) can do well. Using BS3 is similar to the MATLAB dde23. For algorithms where strict error control is needed, it is recommended that one uses MethodOfSteps(Vern6()). Benchmarks show that going to higher order methods like MethodOfSteps(DP8()) may not be beneficial.","category":"section"},{"location":"solvers/dde_solve/#Stiff-DDEs-and-Differential-Algebraic-Delay-Equations-(DADEs)","page":"DDE Solvers","title":"Stiff DDEs and Differential-Algebraic Delay Equations (DADEs)","text":"For stiff DDEs, the SDIRK and Rosenbrock methods are very efficient, as they will reuse the Jacobian in the unconstrained stepping iterations. One should choose from the methods which have stiff-aware interpolants for better stability. MethodOfSteps(Rosenbrock23()) is a good low order method choice. Additionally, the Rodas methods like MethodOfSteps(Rodas4()) are good choices because of their higher order stiff-aware interpolant.\n\nAdditionally, DADEs can be solved by specifying the problem in mass matrix form. The Rosenbrock methods are good choices in these situations.","category":"section"},{"location":"solvers/dde_solve/#Lag-Handling","page":"DDE Solvers","title":"Lag Handling","text":"Lags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can be more efficient and accurate. Constant delays are propagated until the order is higher than the order of the integrator. If state-dependent delays are declared, the algorithm will detect discontinuities arising from these delays and adjust the step size such that these discontinuities are included in the mesh, if steps are rejected. This way, all discontinuities are treated exactly.\n\nIf there are undeclared lags, the discontinuities due to delays are not tracked. In this case, one should only use residual control methods like MethodOfSteps(RK4()), which is the current best choice, as these will step more accurately. Still, residual control is an error-prone method. We recommend setting the tolerances lower in order to get accurate results, though this may be costly since it will use a rejection-based approach to adapt to the delay discontinuities.","category":"section"},{"location":"solvers/dde_solve/#Special-Keyword-Arguments","page":"DDE Solvers","title":"Special Keyword Arguments","text":"discontinuity_interp_points - Number of interpolation points used to track discontinuities arising from dependent delays. Defaults to 10. Only relevant if dependent delays are declared.\ndiscontinuity_abstol and discontinuity_reltol - These are absolute and relative tolerances used by the check whether the time point at the beginning of the current step is a discontinuity arising from dependent delays. Defaults to 1/10^12 and 0. Only relevant if dependent delays are declared.","category":"section"},{"location":"solvers/dde_solve/#Note","page":"DDE Solvers","title":"Note","text":"If the method is having trouble, one may want to adjust the fixed-point iteration. Decreasing the absolute tolerance and the relative tolerance by specifying the keyword arguments abstol and reltol when solving the DDE problem, and increasing the maximal number of iterations by specifying the keyword argument max_iter in the MethodOfSteps algorithm, can help ensure that the steps are correct. If the problem still is not correctly converging, one should lower dtmax. For problems with only constant delays, in the worst-case scenario, one may need to set constrained = true which will constrain timesteps to at most the size of the minimal lag and hence forces more stability at the cost of smaller timesteps.","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK","text":"Strong Stability Preserving Runge-Kutta (SSPRK) methods are specialized explicit Runge-Kutta methods designed to preserve important stability properties of the underlying spatial discretization when applied to hyperbolic partial differential equations and conservation laws.","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Key-Properties","page":"OrdinaryDiffEqSSPRK","title":"Key Properties","text":"SSPRK methods provide:\n\nStrong stability preservation for convex functionals (total variation, maximum norm, entropy)\nOptimal SSP coefficients allowing larger stable timesteps\nNon-oscillatory behavior crucial for hyperbolic PDEs and conservation laws\nHigh-order accuracy while maintaining monotonicity properties\nSpecialized variants for different orders and storage requirements","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#When-to-Use-SSPRK-Methods","page":"OrdinaryDiffEqSSPRK","title":"When to Use SSPRK Methods","text":"SSPRK methods are essential for:\n\nHyperbolic partial differential equations (Euler equations, shallow water, etc.)\nConservation laws where preserving physical bounds is critical\nDiscontinuous Galerkin methods and other high-order spatial discretizations\nProblems requiring monotonicity preservation or total variation stability\nShock-capturing schemes where spurious oscillations must be avoided\nAstrophysical simulations and computational fluid dynamics","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#SSP-Coefficient-and-CFL-Conditions","page":"OrdinaryDiffEqSSPRK","title":"SSP Coefficient and CFL Conditions","text":"The SSP coefficient determines the maximum allowable timestep for stability preservation. Use OrdinaryDiffEqCore.ssp_coefficient(alg) to query this value for step size calculations. The timestep must satisfy dt ≤ CFL * dx / max_wavespeed where CFL ≤ SSP coefficient.","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqSSPRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Second-order-methods","page":"OrdinaryDiffEqSSPRK","title":"Second-order methods","text":"SSPRK22: Two-stage, second-order (SSP coefficient = 1)\nSSPRKMSVS32: Three-step multistep variant","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Third-order-methods","page":"OrdinaryDiffEqSSPRK","title":"Third-order methods","text":"SSPRK33: Three-stage, third-order, optimal (SSP coefficient = 1)\nSSPRK53: Five-stage, third-order, higher SSP coefficient\nSSPRK63, SSPRK73, SSPRK83: More stages for larger SSP coefficients\nSSPRK43: Four-stage with embedded error estimation\nSSPRK432: Low-storage variant","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Fourth-order-methods","page":"OrdinaryDiffEqSSPRK","title":"Fourth-order methods","text":"SSPRK54: Five-stage, fourth-order\nSSPRK104: Ten-stage, fourth-order, large SSP coefficient","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Low-storage-variants","page":"OrdinaryDiffEqSSPRK","title":"Low-storage variants","text":"SSPRK53_2N1, SSPRK53_2N2, SSPRK53_H: Two-register storage schemes","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Discontinuous-Galerkin-optimized","page":"OrdinaryDiffEqSSPRK","title":"Discontinuous Galerkin optimized","text":"KYKSSPRK42: Optimized for DG spatial discretizations\nKYK2014DGSSPRK_3S2: Specialized DG method","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Adaptive-methods","page":"OrdinaryDiffEqSSPRK","title":"Adaptive methods","text":"SSPRK432: Third-order with error control\nSSPRK932: High-stage adaptive method\nSSPRKMSVS43: Multistep adaptive variant\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqSSPRK\", \"SSPRK22\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#Full-list-of-solvers","page":"OrdinaryDiffEqSSPRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK22","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK22","text":"SSPRK22(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A second-order, two-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nShu, Chi-Wang, and Stanley Osher.     Efficient implementation of essentially non-oscillatory shock-capturing schemes.     Journal of Computational Physics 77.2 (1988): 439-471.     https://doi.org/10.1016/0021-9991(88)90177-5\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK33","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK33","text":"SSPRK33(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, three-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nShu, Chi-Wang, and Stanley Osher.     Efficient implementation of essentially non-oscillatory shock-capturing schemes.     Journal of Computational Physics 77.2 (1988): 439-471.     https://doi.org/10.1016/0021-9991(88)90177-5\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK53","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK53","text":"SSPRK53(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, five-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRuuth, Steven.     Global optimization of explicit strong-stability-preserving Runge-Kutta methods.     Mathematics of Computation 75.253 (2006): 183-207\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.KYKSSPRK42","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.KYKSSPRK42","text":"KYKSSPRK42(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Optimal strong-stability-preserving Runge-Kutta time discretizations for discontinuous Galerkin methods\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kubatko2014optimal,     title={Optimal strong-stability-preserving Runge–Kutta time discretizations for discontinuous Galerkin methods},     author={Kubatko, Ethan J and Yeager, Benjamin A and Ketcheson, David I},     journal={Journal of Scientific Computing},     volume={60},     pages={313–344},     year={2014},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.KYK2014DGSSPRK_3S2","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.KYK2014DGSSPRK_3S2","text":"KYK2014DGSSPRK_3S2(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                     step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                     thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Optimal strong-stability-preserving Runge-Kutta time discretizations for discontinuous Galerkin methods\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kubatko2014optimal, title={Optimal strong-stability-preserving Runge–Kutta time discretizations for discontinuous Galerkin methods}, author={Kubatko, Ethan J and Yeager, Benjamin A and Ketcheson, David I}, journal={Journal of Scientific Computing}, volume={60}, pages={313–344}, year={2014}, publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK53_2N1","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK53_2N1","text":"SSPRK53_2N1(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, five-stage explicit strong stability preserving (SSP) low-storage method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nHigueras and T. Roldán.     New third order low-storage SSP explicit Runge–Kutta methods     arXiv:1809.04807v1.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK53_2N2","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK53_2N2","text":"SSPRK53_2N2(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, five-stage explicit strong stability preserving (SSP) low-storage method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nHigueras and T. Roldán.     New third order low-storage SSP explicit Runge–Kutta methods     arXiv:1809.04807v1.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK53_H","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK53_H","text":"SSPRK53_H(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, five-stage explicit strong stability preserving (SSP) low-storage method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nHigueras and T. Roldán.     New third order low-storage SSP explicit Runge–Kutta methods     arXiv:1809.04807v1.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK63","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK63","text":"SSPRK63(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, six-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRuuth, Steven.     Global optimization of explicit strong-stability-preserving Runge-Kutta methods.     Mathematics of Computation 75.253 (2006): 183-207\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK73","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK73","text":"SSPRK73(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, seven-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRuuth, Steven.     Global optimization of explicit strong-stability-preserving Runge-Kutta methods.     Mathematics of Computation 75.253 (2006): 183-207\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK83","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK83","text":"SSPRK83(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, eight-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRuuth, Steven.     Global optimization of explicit strong-stability-preserving Runge-Kutta methods.     Mathematics of Computation 75.253 (2006): 183-207\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK43","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK43","text":"SSPRK43(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, four-stage explicit strong stability preserving (SSP) method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nOptimal third-order explicit SSP method with four stages discovered by\n\nJ. F. B. M. Kraaijevanger. \"Contractivity of Runge-Kutta methods.\" In: BIT Numerical Mathematics 31.3 (1991), pp. 482–528. DOI: 10.1007/BF01933264.\n\nEmbedded method constructed by\n\nSidafa Conde, Imre Fekete, John N. Shadid. \"Embedded error estimation and adaptive step-size control for optimal explicit strong stability preserving Runge–Kutta methods.\" arXiv: 1806.08693\n\nEfficient implementation (and optimized controller) developed by\n\nHendrik Ranocha, Lisandro Dalcin, Matteo Parsani, David I. Ketcheson (2021) Optimized Runge-Kutta Methods with Automatic Step Size Control for Compressible Computational Fluid Dynamics arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK432","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK432","text":"SSPRK432(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, four-stage explicit strong stability preserving (SSP) method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nGottlieb, Sigal, David I. Ketcheson, and Chi-Wang Shu.     Strong stability preserving Runge-Kutta and multistep time discretizations.     World Scientific, 2011.     Example 6.1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRKMSVS43","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRKMSVS43","text":"SSPRKMSVS43(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, four-step explicit strong stability preserving (SSP) linear multistep method. This method does not come with an error estimator and requires a fixed time step size.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nShu, Chi-Wang.     Total-variation-diminishing time discretizations.     SIAM Journal on Scientific and Statistical Computing 9, no. 6 (1988): 1073-1084.     DOI: 10.1137/0909073\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRKMSVS32","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRKMSVS32","text":"SSPRKMSVS32(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A second-order, three-step explicit strong stability preserving (SSP) linear multistep method. This method does not come with an error estimator and requires a fixed time step size.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nShu, Chi-Wang.     Total-variation-diminishing time discretizations.     SIAM Journal on Scientific and Statistical Computing 9, no. 6 (1988): 1073-1084.     DOI: 10.1137/0909073\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK932","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK932","text":"SSPRK932(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, nine-stage explicit strong stability preserving (SSP) method.\n\nConsider using SSPRK43 instead, which uses the same main method and an improved embedded method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nGottlieb, Sigal, David I. Ketcheson, and Chi-Wang Shu.     Strong stability preserving Runge-Kutta and multistep time discretizations.     World Scientific, 2011.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK54","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK54","text":"SSPRK54(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A fourth-order, five-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRuuth, Steven.     Global optimization of explicit strong-stability-preserving Runge-Kutta methods.     Mathematics of Computation 75.253 (2006): 183-207.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/SSPRK/#OrdinaryDiffEqSSPRK.SSPRK104","page":"OrdinaryDiffEqSSPRK","title":"OrdinaryDiffEqSSPRK.SSPRK104","text":"SSPRK104(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A fourth-order, ten-stage explicit strong stability preserving (SSP) method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nKetcheson, David I.     Highly efficient strong stability-preserving Runge–Kutta methods with     low-storage implementations.     SIAM Journal on Scientific Computing 30.4 (2008): 2113-2136.\n\n\n\n\n\n","category":"type"},{"location":"types/sdde_types/#sdde_prob","page":"SDDE Problems","title":"SDDE Problems","text":"","category":"section"},{"location":"types/sdde_types/#Solution-Type","page":"SDDE Problems","title":"Solution Type","text":"SDDEProblem solutions return an RODESolution. For more information, see the RODE problem definition page for the RODESolution docstring.","category":"section"},{"location":"types/sdde_types/#Alias-Specifier","page":"SDDE Problems","title":"Alias Specifier","text":"","category":"section"},{"location":"types/sdde_types/#SciMLBase.SDDEProblem","page":"SDDE Problems","title":"SciMLBase.SDDEProblem","text":"Defines a stochastic delay differential equation (SDDE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/sdde_types/\n\nMathematical Specification of a Stochastic Delay Differential Equation (SDDE) Problem\n\nTo define a SDDE Problem, you simply need to give the drift function f, the diffusion function g, the initial condition u_0 at time point t_0, and the history function h which together define a SDDE:\n\nbeginalign*\ndu(t)  = f(uhpt)  dt + g(uhpt)  dW_t  (t geq t_0) \nu(t_0) = u_0 \nu(t)   = h(t)  (t  t_0)\nendalign*\n\nf should be specified as f(u, h, p, t) (or in-place as f(du, u, h, p, t)) (and g should match). u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u, and h should be specified as described below. The history function h is accessed for all delayed values. Note that we are not limited to numbers or vectors for u_0; one is allowed to provide u_0 as arbitrary matrices / higher dimension tensors as well.\n\nNote that this functionality should be considered experimental.\n\nFunctional Forms of the History Function\n\nThe history function h can be called in the following ways:\n\nh(p, t): out-of-place calculation\nh(out, p, t): in-place calculation\nh(p, t, deriv::Type{Val{i}}): out-of-place calculation of the ith derivative\nh(out, p, t, deriv::Type{Val{i}}): in-place calculation of the ith derivative\nh(args...; idxs): calculation of h(args...) for indices idxs\n\nNote that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function f.\n\nDeclaring Lags\n\nLags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can more efficiently be more accurate and thus this is recommended.\n\nNeutral, Retarded, and Algebraic Stochastic Delay Differential Equations\n\nNote that the history function specification can be used to specify general retarded arguments, i.e. h(p,α(u,t)). Neutral delay differential equations can be specified by using the deriv value in the history interpolation. For example, h(p,t-τ, Val{1}) returns the first derivative of the history values at time t-τ.\n\nNote that algebraic equations can be specified by using a singular mass matrix.\n\nProblem Type\n\nConstructors\n\nSDDEProblem(f,g[, u0], h, tspan[, p]; <keyword arguments>)\nSDDEProblem{isinplace,specialize}(f,g[, u0], h, tspan[, p]; <keyword arguments>)\n\nisinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nArguments\n\nf: The drift function in the SDDE.\ng: The diffusion function in the SDDE.\nu0: The initial condition. Defaults to the value h(p, first(tspan)) of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise, it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n\n\n","category":"type"},{"location":"types/sdde_types/#SciMLBase.SDDEFunction","page":"SDDE Problems","title":"SciMLBase.SDDEFunction","text":"struct SDDEFunction{iip, specialize, F, G, TMM, Ta, Tt, TJ, JVP, VJP, JP, SP, TW, TWt, TPJ, GG, O, TCV, SYS, ID} <: SciMLBase.AbstractSDDEFunction{iip}\n\nA representation of a SDDE function f, defined by:\n\nM  du = f(uhpt)  dt + g(uhpt)  dW_t\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nSDDEFunction{iip,specialize}(f,g;\n                 mass_matrix = __has_mass_matrix(f) ? f.mass_matrix : I,\n                 analytic = __has_analytic(f) ? f.analytic : nothing,\n                 tgrad= __has_tgrad(f) ? f.tgrad : nothing,\n                 jac = __has_jac(f) ? f.jac : nothing,\n                 jvp = __has_jvp(f) ? f.jvp : nothing,\n                 vjp = __has_vjp(f) ? f.vjp : nothing,\n                 jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                 sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                 paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n                 colorvec = __has_colorvec(f) ? f.colorvec : nothing\n                 sys = __has_sys(f) ? f.sys : nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,h,p,t) or du = f(u,h,p,t). See the section on iip for more details on in-place vs out-of-place handling. The history function h acts as an interpolator over time, i.e. h(t) with options matching the solution interface, i.e. h(t; save_idxs = 2).\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://docs.sciml.ai/DiffEqDocs/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,h,p,t) or dT=tgrad(u,p,t): returns fracf(upt)t\njac(J,u,h,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,h,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivative fracdfdu v\nvjp(Jv,v,h,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivative fracdfdu^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,h,u,p,t): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DDEFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"types/sdde_types/#SciMLBase.SDDEAliasSpecifier","page":"SDDE Problems","title":"SciMLBase.SDDEAliasSpecifier","text":"SDDEAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = nothing, alias_du0 = nothing, alias_tstops = nothing, alias = nothing)\n\nHolds information on what variables to alias when solving an SDDEProblem. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias_jumps::Union{Bool, Nothing}: alias jump process if wrapped in a JumpProcess\nalias::Union{Bool, Nothing}: sets all fields of the SDDEAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"types/sdae_types/#SDAE-Problems","page":"SDAE Problems","title":"SDAE Problems","text":"","category":"section"},{"location":"types/sdae_types/#Mathematical-Specification-of-a-Stochastic-Differential-Algebraic-Equation-(SDAE)-Problem","page":"SDAE Problems","title":"Mathematical Specification of a Stochastic Differential-Algebraic Equation (SDAE) Problem","text":"To define an SDAE, you simply define an SDE Problem with the forcing function f, the noise function g, a mass matrix M and the initial condition u₀ which define the SDAE in mass matrix form:\n\nM du = f(upt)dt + Σgᵢ(upt)dWⁱ\n\nf and g should be specified as f(u,p,t) and  g(u,p,t) respectively, and u₀ should be an AbstractArray whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. A vector of gs can also be defined to determine an SDE of higher Ito dimension.\n\nNonsingular mass matrices correspond to constraint equations and thus a stochastic DAE.","category":"section"},{"location":"types/sdae_types/#Example","page":"SDAE Problems","title":"Example","text":"const mm_A = [-2.0 1 4\n              4 -2 1\n              0 0 0]\nfunction f!(du, u, p, t)\n    du[1] = u[1]\n    du[2] = u[2]\n    du[3] = u[1] + u[2] + u[3] - 1\nend\n\nfunction g!(du, u, p, t)\n    @. du = 0.1\nend\n\nprob = SDEProblem(SDEFunction(f!, g!; mass_matrix = mm_A),\n    ones(3), (0.0, 1.0))","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK","text":"Exponential Runge-Kutta methods are specialized integrators for semi-linear differential equations of the form du/dt = Au + f(u,t), where A is a linear operator (often representing diffusion or dispersion) and f represents nonlinear terms. These methods are particularly effective for stiff linear parts combined with non-stiff nonlinear terms. Important: The nonlinear term f(u,t) must be non-stiff for these methods to be effective.","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Key-Properties","page":"OrdinaryDiffEqExponentialRK","title":"Key Properties","text":"Exponential RK methods provide:\n\nExact treatment of linear parts using matrix exponential functions\nHigh-order accuracy for both linear and nonlinear components\nExcellent stability properties for problems with stiff linear operators\nEfficient handling of semi-linear PDEs after spatial discretization\nReduced timestep restrictions compared to traditional explicit methods\nPreservation of qualitative behavior for many physical systems","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#When-to-Use-Exponential-RK-Methods","page":"OrdinaryDiffEqExponentialRK","title":"When to Use Exponential RK Methods","text":"These methods are recommended for:\n\nSemi-linear PDEs with stiff diffusion/dispersion and moderate non-stiff nonlinearity\nReaction-diffusion systems with fast diffusion and slower non-stiff reactions\nNonlinear Schrödinger equations and other dispersive wave equations with non-stiff nonlinear terms\nPattern formation problems (Turing patterns, phase field models) where nonlinearity is non-stiff\nQuantum dynamics with linear Hamiltonian and non-stiff nonlinear interactions\nProblems with strong linear damping or oscillatory linear parts combined with non-stiff nonlinear terms\nSpatially discretized PDEs where the linear part dominates stiffness but the nonlinear part remains non-stiff","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Mathematical-Background","page":"OrdinaryDiffEqExponentialRK","title":"Mathematical Background","text":"For problems du/dt = Au + f(u,t), exponential methods compute the exact solution of the linear part Au using exp(A*dt) and treat the nonlinear part f(u,t) with Runge-Kutta-like stages. This approach is particularly effective when A represents well-understood physics (diffusion, dispersion, linear oscillations).","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqExponentialRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Basic-exponential-time-differencing-(ETD)","page":"OrdinaryDiffEqExponentialRK","title":"Basic exponential time differencing (ETD)","text":"LawsonEuler: First-order exponential Euler method\nNorsettEuler / ETD1: Alternative first-order scheme\nETDRK2: Second-order exponential RK\nETDRK3: Third-order exponential RK\nETDRK4: Fourth-order exponential RK, popular choice\nETD2: Second-order exponential time differencing (in development)","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#High-order-specialized-methods","page":"OrdinaryDiffEqExponentialRK","title":"High-order specialized methods","text":"HochOst4: Fourth-order exponential RK with enhanced stability\nExp4: Fourth-order EPIRK scheme","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Adaptive-exponential-Rosenbrock","page":"OrdinaryDiffEqExponentialRK","title":"Adaptive exponential Rosenbrock","text":"Exprb32: Third-order adaptive method with error control\nExprb43: Fourth-order adaptive method","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#EPIRK-(Exponential-Propagation-Iterative-RK)-methods","page":"OrdinaryDiffEqExponentialRK","title":"EPIRK (Exponential Propagation Iterative RK) methods","text":"EPIRK4s3A: Fourth-order with stiff order 4\nEPIRK4s3B: Alternative fourth-order variant\nEPIRK5s3: Fifth-order method (note: marked as broken)\nEXPRB53s3: Fifth-order with stiff order 5\nEPIRK5P1, EPIRK5P2: Fifth-order variants","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Performance-Recommendations","page":"OrdinaryDiffEqExponentialRK","title":"Performance Recommendations","text":"For most semi-linear problems: ETDRK4\nFor adaptive stepsize: Exprb43\nFor high stiffness in linear part: EPIRK4s3A or EPIRK4s3B\nFor maximum accuracy: EXPRB53s3","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Implementation-Requirements","page":"OrdinaryDiffEqExponentialRK","title":"Implementation Requirements","text":"These methods require:\n\nComputation of matrix exponentials exp(A*dt) and related functions\nKrylov subspace methods for large systems (automatic in most cases)\nProper problem formulation with identified linear and nonlinear parts","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Installation","page":"OrdinaryDiffEqExponentialRK","title":"Installation","text":"To be able to access the solvers in OrdinaryDiffEqLinear, you must first install them use the Julia package manager:\n\nusing Pkg\nPkg.add(\"OrdinaryDiffEqExponentialRK\")\n\nThis will only install the solvers listed at the bottom of this page. If you want to explore other solvers for your problem, you will need to install some of the other libraries listed in the navigation bar on the left.","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Example-usage","page":"OrdinaryDiffEqExponentialRK","title":"Example usage","text":"first_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqExponentialRK\", \"EPIRK5s3\")","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Full-list-of-solvers","page":"OrdinaryDiffEqExponentialRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Adaptive-Exponential-Rosenbrock-Methods","page":"OrdinaryDiffEqExponentialRK","title":"Adaptive Exponential Rosenbrock Methods","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#Exponential-Propagation-Iterative-Runge-Kutta-Methods-(EPIRK)","page":"OrdinaryDiffEqExponentialRK","title":"Exponential Propagation Iterative Runge-Kutta Methods (EPIRK)","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.LawsonEuler-api-ordinarydiffeq-semiimplicit-ExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.LawsonEuler","text":"LawsonEuler(; krylov = false,\n              m = 30,\n              iop = 0)\n\nSemilinear ODE solver First order exponential Euler scheme (fixed timestepping)\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.NorsettEuler-api-ordinarydiffeq-semiimplicit-ExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.NorsettEuler","text":"NorsettEuler(; krylov = false,\n               m = 30,\n               iop = 0)\n\nSemilinear ODE solver First order exponential-RK scheme. Alias: ETD1\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETD2-api-ordinarydiffeq-semiimplicit-ExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETD2","text":"ETD2: Exponential Runge-Kutta Method Second order Exponential Time Differencing method (in development).\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETDRK2-api-ordinarydiffeq-semiimplicit-ExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETDRK2","text":"ETDRK2(; krylov = false,\n         m = 30,\n         iop = 0)\n\nSemilinear ODE solver 2nd order exponential-RK scheme.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETDRK3-api-ordinarydiffeq-semiimplicit-ExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETDRK3","text":"ETDRK3(; krylov = false,\n         m = 30,\n         iop = 0)\n\nSemilinear ODE solver 3rd order exponential-RK scheme.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETDRK4-api-ordinarydiffeq-semiimplicit-ExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETDRK4","text":"ETDRK4(; krylov = false,\n         m = 30,\n         iop = 0)\n\nSemilinear ODE solver 4th order exponential-RK scheme (fixed timestepping)\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.HochOst4-api-ordinarydiffeq-semiimplicit-ExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.HochOst4","text":"HochOst4(; krylov = false,\n           m = 30,\n           iop = 0)\n\nSemilinear ODE solver 4th order exponential-RK scheme with stiff order 4.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.Exprb32","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.Exprb32","text":"Exprb32(; m = 30,\n          iop = 0)\n\nSemilinear ODE solver 3rd order adaptive Exponential-Rosenbrock scheme.\n\nKeyword Arguments\n\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).   Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, M., & Ostermann, A. (2010). Exponential integrators. Acta Numerica, 19, 209-286. (https://doi.org/10.1017/S0962492910000048)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/ExponentialRK/#OrdinaryDiffEqExponentialRK.Exprb43","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.Exprb43","text":"Exprb43(; m = 30,\n          iop = 0)\n\nSemilinear ODE solver 4th order adaptive Exponential-Rosenbrock scheme.\n\nKeyword Arguments\n\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).   Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, M., & Ostermann, A. (2010). Exponential integrators. Acta Numerica, 19, 209-286. (https://doi.org/10.1017/S0962492910000048)\n\n\n\n\n\n","category":"type"},{"location":"types/rode_types/#rode_problem","page":"RODE Problems","title":"RODE Problems","text":"","category":"section"},{"location":"types/rode_types/#Solution-Type","page":"RODE Problems","title":"Solution Type","text":"","category":"section"},{"location":"types/rode_types/#Alias-Specifier","page":"RODE Problems","title":"Alias Specifier","text":"","category":"section"},{"location":"types/rode_types/#SciMLBase.RODEProblem","page":"RODE Problems","title":"SciMLBase.RODEProblem","text":"Defines a random ordinary differential equation (RODE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/rode_types/\n\nMathematical Specification of a RODE Problem\n\nTo define a RODE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nfracdudt = f(uptW(t))\n\nwhere W(t) is a random process. f should be specified as f(u,p,t,W) (or in-place as f(du,u,p,t,W)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nConstructors\n\nRODEProblem(f::RODEFunction,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing)\nRODEProblem{isinplace,specialize}(f,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing,mass_matrix=I) : Defines the RODE with the specified functions. The default noise is WHITE_NOISE. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The drift function in the SDE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The optional parameters for the problem. Defaults to NullParameters.\nnoise: The noise process applied to the noise upon generation. Defaults to Gaussian white noise. For information on defining different noise processes, see the noise process documentation page\nrand_prototype: A prototype type instance for the noise vector. It defaults to nothing, which means the problem should be interpreted as having a noise vector whose size matches u0.\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n\n\n","category":"type"},{"location":"types/rode_types/#SciMLBase.RODEFunction","page":"RODE Problems","title":"SciMLBase.RODEFunction","text":"struct RODEFunction{iip, specialize, F, TMM, Ta, Tt, TJ, JVP, VJP, JP, SP, TW, TWt, TPJ, O, TCV, SYS, ID} <: SciMLBase.AbstractRODEFunction{iip}\n\nA representation of a RODE function f, defined by:\n\nM fracdudt = f(uptW)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nRODEFunction{iip,specialize}(f;\n                           mass_matrix = __has_mass_matrix(f) ? f.mass_matrix : I,\n                           analytic = __has_analytic(f) ? f.analytic : nothing,\n                           tgrad= __has_tgrad(f) ? f.tgrad : nothing,\n                           jac = __has_jac(f) ? f.jac : nothing,\n                           jvp = __has_jvp(f) ? f.jvp : nothing,\n                           vjp = __has_vjp(f) ? f.vjp : nothing,\n                           jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                           sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                           paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n                           colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n                           sys = __has_sys(f) ? f.sys : nothing,\n                           analytic_full = __has_analytic_full(f) ? f.analytic_full : false)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p,t,W) or du = f(u,p,t,W). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the RODE function. Can be used to determine that the equation is actually a random differential-algebraic equation (RDAE) if M is singular.\nanalytic: (u0,p,t,W) or analytic(sol): used to pass an analytical solution function for the analytical solution of the RODE. Generally only used for testing and development of the solvers. The exact form depends on the field analytic_full.\nanalytic_full: a boolean to indicate whether to use the form analytic(u0,p,t,W) (if false) or the form analytic!(sol) (if true). The former is expected to return the solution u(t) of the equation, given the initial condition u0, the parameter p, the current time t and the value W=W(t) of the noise at the given time t. The latter case is useful when the solution of the RODE depends on the whole history of the noise, which is available in sol.W.W, at times sol.W.t. In this case, analytic(sol) must mutate explicitly the field sol.u_analytic with the corresponding expected solution at sol.W.t or sol.t.\ntgrad(dT,u,p,t,W) or dT=tgrad(u,p,t,W): returns fracf(uptW)t\njac(J,u,p,t,W) or J=jac(u,p,t,W): returns fracdfdu\njvp(Jv,v,u,p,t,W) or Jv=jvp(v,u,p,t,W): returns the directional derivative fracdfdu v\nvjp(Jv,v,u,p,t,W) or Jv=vjp(v,u,p,t,W): returns the adjoint derivative fracdfdu^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t,W): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the RODEFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"types/rode_types/#SciMLBase.RODESolution","page":"RODE Problems","title":"SciMLBase.RODESolution","text":"struct RODESolution{T, N, uType, uType2, DType, tType, randType, discType, P, A, IType, S, AC<:Union{Nothing, Vector{Int64}}, V} <: SciMLBase.AbstractRODESolution{T, N, uType}\n\nRepresentation of the solution to an stochastic differential equation defined by an SDEProblem, or of a random ordinary differential equation defined by an RODEProblem.\n\nDESolution Interface\n\nFor more information on interacting with DESolution types, check out the Solution Handling page of the DifferentialEquations.jl documentation.\n\nhttps://docs.sciml.ai/DiffEqDocs/stable/basics/solution\n\nFields\n\nu: the representation of the SDE or RODE solution. Given as an array of solutions, where u[i] corresponds to the solution at time t[i]. It is recommended in most cases one does not access sol.u directly and instead use the array interface described in the Solution Handling page of the DifferentialEquations.jl documentation.\nt: the time points corresponding to the saved values of the ODE solution.\nW: the representation of the saved noise process from the solution. See the Noise Processes page of the DifferentialEquations.jl. Note that this noise is only saved in full if save_noise=true in the solver.\nprob: the original SDEProblem/RODEProblem that was solved.\nalg: the algorithm type used by the solver.\nstats: statistics of the solver, such as the number of function evaluations required, number of Jacobians computed, and more.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully, whether it terminated early due to a user-defined callback, or whether it exited due to an error. For more details, see the return code documentation.\n\n\n\n\n\n","category":"type"},{"location":"types/rode_types/#SciMLBase.RODEAliasSpecifier","page":"RODE Problems","title":"SciMLBase.RODEAliasSpecifier","text":"RODEAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = false, alias_du0 = false, alias_tstops = false, alias = nothing)\n\nHolds information on what variables to alias when solving an RODEProblem. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false .\nalias_du0::Union{Bool, Nothing}: alias the du0 array for DAEs. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias_noise::Union{Bool,Nothing}: alias the noise process\nalias_jumps::Union{Bool, Nothing}: alias jump process if wrapped in a JumpProcess\nalias::Union{Bool, Nothing}: sets all fields of the RODEAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#callbacks","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"DifferentialEquations.jl allows for using callback functions to inject user code into the solver algorithms. It allows for safely and accurately applying events and discontinuities. Multiple callbacks can be chained together, and these callback types can be used to build libraries of extension behavior.","category":"section"},{"location":"features/callback_functions/#The-Callback-Types","page":"Event Handling and Callback Functions","title":"The Callback Types","text":"The callback types are defined as follows. There are three primitive callback types: the ContinuousCallback, DiscreteCallback and the VectorContinuousCallback:\n\nThe ContinuousCallback is applied when a given continuous condition function hits zero. This hitting can happen even within an integration step, and the solver must be able to detect it and adjust the integration step accordingly. This type of callback implements what is known in other problem-solving environments as an Event.\nThe DiscreteCallback is applied when its condition function is true, but the condition is only evaluated at the end of every integration step.\nThe VectorContinuousCallback works like a vector of ContinuousCallbacks and lets the user specify a vector of continuous callbacks, each with simultaneous rootfinding equations. The effect that is applied is the effect corresponding to the first (earliest) condition that is satisfied. A VectorContinuousCallback is more efficient than a CallbackSet of ContinuousCallbacks as the number of callbacks grows. As such, it's a slightly more involved definition which gives better scaling.","category":"section"},{"location":"features/callback_functions/#ContinuousCallback","page":"Event Handling and Callback Functions","title":"ContinuousCallback","text":"","category":"section"},{"location":"features/callback_functions/#discrete_callback","page":"Event Handling and Callback Functions","title":"DiscreteCallback","text":"","category":"section"},{"location":"features/callback_functions/#CallbackSet","page":"Event Handling and Callback Functions","title":"CallbackSet","text":"","category":"section"},{"location":"features/callback_functions/#VectorContinuousCallback","page":"Event Handling and Callback Functions","title":"VectorContinuousCallback","text":"","category":"section"},{"location":"features/callback_functions/#Using-Callbacks","page":"Event Handling and Callback Functions","title":"Using Callbacks","text":"The callback type is then sent to the solver (or the integrator) via the callback keyword argument:\n\nsol = solve(prob, alg, callback = cb)\n\nYou can supply nothing, a single DiscreteCallback or ContinuousCallback or VectorContinuousCallback, or a CallbackSet.","category":"section"},{"location":"features/callback_functions/#Note-About-Saving","page":"Event Handling and Callback Functions","title":"Note About Saving","text":"When a callback is supplied, the default saving behavior is turned off. This is because otherwise, events would “double save” one of the values. To re-enable the standard saving behavior, one must have the first save_positions value be true for at least one callback.","category":"section"},{"location":"features/callback_functions/#Modifying-the-Stepping-Within-A-Callback","page":"Event Handling and Callback Functions","title":"Modifying the Stepping Within A Callback","text":"A common issue with callbacks is that they cause a large discontinuous change, and so it may be wise to pull down dt after such a change. To control the timestepping from a callback, please see the timestepping controls in the integrator interface. Specifically, set_proposed_dt! is used to set the next stepsize, and terminate! can be used to cause the simulation to stop.","category":"section"},{"location":"features/callback_functions/#DiscreteCallback-Examples","page":"Event Handling and Callback Functions","title":"DiscreteCallback Examples","text":"","category":"section"},{"location":"features/callback_functions/#Example-1:-Interventions-at-Preset-Times","page":"Event Handling and Callback Functions","title":"Example 1: Interventions at Preset Times","text":"Assume we have a patient whose internal drug concentration follows exponential decay, i.e. the linear ODE with a negative coefficient:\n\nimport DifferentialEquations as DE\nfunction f(du, u, p, t)\n    du[1] = -u[1]\nend\nu0 = [10.0]\nconst V = 1\nprob = DE.ODEProblem(f, u0, (0.0, 10.0))\nsol = DE.solve(prob, DE.Tsit5())\nimport Plots;\nPlots.plot(sol);\n\nNow assume we wish to give the patient a dose of 10 at time t==4. For this, we can use a DiscreteCallback which will only be true at t==4:\n\ncondition(u, t, integrator) = t == 4\naffect!(integrator) = integrator.u[1] += 10\ncb = DE.DiscreteCallback(condition, affect!)\n\nIf we then solve with this callback enabled, we see no change:\n\nsol = DE.solve(prob, DE.Tsit5(), callback = cb)\nPlots.plot(sol)\n\nThe reason there is no change is because the DiscreteCallback only applies at a specific time, and the integrator never hit that time. Thus we would like to force the ODE solver to step exactly at t=4 so that the condition can be applied. We can do that with the tstops argument:\n\nsol = DE.solve(prob, DE.Tsit5(), callback = cb, tstops = [4.0])\nPlots.plot(sol)\n\nand thus we achieve the desired result.\n\nPerforming multiple doses then just requires that we have multiple points which are hit. For example, to dose at time t=4 and t=8, we can do the following:\n\ndosetimes = [4.0, 8.0]\ncondition(u, t, integrator) = t ∈ dosetimes\naffect!(integrator) = integrator.u[1] += 10\ncb = DE.DiscreteCallback(condition, affect!)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb, tstops = dosetimes)\nPlots.plot(sol)\n\nWe can then use this mechanism to make the model arbitrarily complex. For example, let's say there's now 3 dose times, but the dose only triggers if the current concentration is below 1.0. Additionally, the dose is now 10t instead of just 10. This model is implemented as simply:\n\ndosetimes = [4.0, 6.0, 8.0]\ncondition(u, t, integrator) = t ∈ dosetimes && (u[1] < 1.0)\naffect!(integrator) = integrator.u[1] += 10 * integrator.t\ncb = DE.DiscreteCallback(condition, affect!)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb, tstops = dosetimes)\nPlots.plot(sol)","category":"section"},{"location":"features/callback_functions/#PresetTimeCallback","page":"Event Handling and Callback Functions","title":"PresetTimeCallback","text":"Because events at preset times is a very common occurrence, DifferentialEquations.jl provides a pre-built callback in the Callback Library. The PresetTimeCallback(tstops,affect!) takes an array of times and an affect! function to apply. Thus to do the simple 2 dose example with this callback, we could do the following:\n\ndosetimes = [4.0, 8.0]\naffect!(integrator) = integrator.u[1] += 10\ncb = DE.PresetTimeCallback(dosetimes, affect!)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb)\nPlots.plot(sol)\n\nNotice that this version will automatically set the tstops for you.","category":"section"},{"location":"features/callback_functions/#Example-2:-A-Control-Problem","page":"Event Handling and Callback Functions","title":"Example 2: A Control Problem","text":"Another example of a DiscreteCallback is a control problem switching parameters. Our parameterized ODE system is as follows:\n\nOur ODE function will use this field as follows:\n\nfunction f(du, u, p, t)\n    du[1] = -0.5 * u[1] + p\n    du[2] = -0.5 * u[2]\nend\n\nNow we will setup our control mechanism. It will be a simple setup which uses set timepoints at which we will change p. At t=5.0 we will want to increase the value of p, and at t=8.0 we will want to decrease the value of p. Using the DiscreteCallback interface, we code these conditions as follows:\n\nconst tstop1 = [5.0]\nconst tstop2 = [8.0]\n\nfunction condition(u, t, integrator)\n    t in tstop1\nend\n\nfunction condition2(u, t, integrator)\n    t in tstop2\nend\n\nNow we have to apply an effect when these conditions are reached. When condition is hit (at t=5.0), we will increase p to 1.5. When condition2 is reached, we will decrease p to -1.5. This is done via the functions:\n\nfunction affect!(integrator)\n    integrator.p = 1.5\nend\n\nfunction affect2!(integrator)\n    integrator.p = -1.5\nend\n\nWith these functions we can build our callbacks:\n\nimport DifferentialEquations as DE\nsave_positions = (true, true)\n\ncb = DE.DiscreteCallback(condition, affect!, save_positions = save_positions)\n\nsave_positions = (false, true)\n\ncb2 = DE.DiscreteCallback(condition2, affect2!, save_positions = save_positions)\n\ncbs = DE.CallbackSet(cb, cb2)\n\nNow we define our initial condition. We will start at [10.0;10.0] with p=0.0.\n\nu0 = [10.0; 10.0]\np = 0.0\nprob = DE.ODEProblem(f, u0, (0.0, 10.0), p)\n\nLastly we solve the problem. Note that we must pass tstop values of 5.0 and 8.0 to ensure the solver hits those timepoints exactly:\n\nconst tstop = [5.0; 8.0]\nsol = DE.solve(prob, DE.Tsit5(), callback = cbs, tstops = tstop)\nimport Plots;\nPlots.plot(sol);\n\nIt's clear from the plot how the controls affected the outcome.","category":"section"},{"location":"features/callback_functions/#Example-3:-AutoAbstol","page":"Event Handling and Callback Functions","title":"Example 3: AutoAbstol","text":"MATLAB's Simulink has the option for an automatic absolute tolerance. In this example we will implement a callback which will add this behavior to any JuliaDiffEq solver which implements the integrator and callback interface.\n\nThe algorithm is as follows. The default value is set to start at 1e-6, though we will give the user an option for this choice. Then as the simulation progresses, at each step the absolute tolerance is set to the maximum value that has been reached so far times the relative tolerance. This is the behavior that we will implement in affect!.\n\nSince the effect is supposed to occur every timestep, we use the trivial condition:\n\ncondition = function (u, t, integrator)\n    true\nend\n\nwhich always returns true. For our effect we will overload the call on a type. This type will have a value for the current maximum. By doing it this way, we can store the current state for the running maximum. The code is as follows:\n\nmutable struct AutoAbstolAffect{T}\n    curmax::T\nend\n# Now make `affect!` for this:\nfunction (p::AutoAbstolAffect)(integrator)\n    p.curmax = max(p.curmax, integrator.u)\n    integrator.opts.abstol = p.curmax * integrator.opts.reltol\n    DE.u_modified!(integrator, false)\nend\n\nThis makes affect!(integrator) use an internal mutating value curmax to update the absolute tolerance of the integrator as the algorithm states.\n\nLastly, we can wrap it in a nice little constructor:\n\nfunction AutoAbstol(save = true; init_curmax = 1e-6)\n    affect! = AutoAbstolAffect(init_curmax)\n    condition = (u, t, integrator) -> true\n    save_positions = (save, false)\n    DE.DiscreteCallback(condition, affect!, save_positions = save_positions)\nend\n\nThis creates the DiscreteCallback from the affect! and condition functions that we implemented. Now\n\nimport DifferentialEquations as DE\ncb = AutoAbstol(true; init_curmax = 1e-6)\n\nreturns the callback that we created. We can then solve an equation using this by simply passing it with the callback keyword argument. Using the integrator interface rather than the solve interface, we can step through one by one to watch the absolute tolerance increase:\n\nfunction g(u, p, t)\n    -u[1]\nend\nu0 = 10.0\nconst V = 1\nprob = DE.ODEProblem(g, u0, (0.0, 10.0))\nintegrator = DE.init(prob, DE.BS3(), callback = cb)\nat1 = integrator.opts.abstol\nDE.step!(integrator)\nat2 = integrator.opts.abstol\nat1 <= at2\n\nDE.step!(integrator)\nat3 = integrator.opts.abstol\nat2 <= at3\n\nNote that this example is contained in the Callback Library, a library of useful callbacks for JuliaDiffEq solvers.","category":"section"},{"location":"features/callback_functions/#ContinuousCallback-Examples","page":"Event Handling and Callback Functions","title":"ContinuousCallback Examples","text":"","category":"section"},{"location":"features/callback_functions/#Example-1:-Bouncing-Ball","page":"Event Handling and Callback Functions","title":"Example 1: Bouncing Ball","text":"Let's look at the bouncing ball. Let the first variable y is the height which changes by v the velocity, where the velocity is always changing at -g which is the gravitational constant. This is the equation:\n\nfunction f(du, u, p, t)\n    du[1] = u[2]\n    du[2] = -p\nend\n\nAll we have to do in order to specify the event is to have a function which should always be positive, with an event occurring at 0. We thus want to check if the ball's height ever hits zero:\n\nfunction condition(u, t, integrator) # Event when condition(u,t,integrator) == 0\n    u[1]\nend\n\nNotice that here we used the values u instead of the value from the integrator. This is because the values u,t will be appropriately modified at the interpolation points, allowing for the rootfinding behavior to occur.\n\nNow we have to say what to do when the event occurs. In this case, we just flip the velocity (the second variable)\n\nfunction affect!(integrator)\n    integrator.u[2] = -integrator.u[2]\nend\n\nThe callback is thus specified by:\n\nimport DifferentialEquations as DE\ncb = DE.ContinuousCallback(condition, affect!)\n\nThen you can solve and plot:\n\nu0 = [50.0, 0.0]\ntspan = (0.0, 15.0)\np = 9.8\nprob = DE.ODEProblem(f, u0, tspan, p)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb)\nimport Plots;\nPlots.plot(sol);\n\nAs you can see from the resulting image, DifferentialEquations.jl is smart enough to use the interpolation to hone in on the time of the event and apply the event back at the correct time. Thus, one does not have to worry about the adaptive timestepping “overshooting” the event, as this is handled for you. Notice that the event macro will save the value(s) at the discontinuity.\n\nThe callback is robust to having multiple discontinuities occur. For example, we can integrate for long time periods and get the desired behavior:\n\nu0 = [50.0, 0.0]\ntspan = (0.0, 100.0)\nprob = DE.ODEProblem(f, u0, tspan, p)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb)\nPlots.plot(sol)","category":"section"},{"location":"features/callback_functions/#Handling-Changing-Dynamics-and-Exactness","page":"Event Handling and Callback Functions","title":"Handling Changing Dynamics and Exactness","text":"Let's make a version of the bouncing ball where the ball sticks to the ground. We can do this by introducing a parameter p to send the velocity to zero on the bounce. This looks as follows:\n\nfunction dynamics!(du, u, p, t)\n    du[1] = u[2]\n    du[2] = p[1] * -9.8\nend\nfunction floor_aff!(int)\n    int.p[1] = 0\n    int.u[2] = 0\n    @show int.u[1], int.t\nend\nfloor_event = DE.ContinuousCallback(condition, floor_aff!)\nu0 = [1.0, 0.0]\np = [1.0]\nprob = DE.ODEProblem{true}(dynamics!, u0, (0.0, 1.75), p)\nsol = DE.solve(prob, DE.Tsit5(), callback = floor_event)\nPlots.plot(sol)\n\nNotice that at the end, the ball is not at 0.0 like the condition would let you believe, but instead it's at 4.329177480185359e-16. From the printing inside the affect function, we can see that this is the value it had at the event time t=0.4517539514526232. Why did the event handling not make it exactly zero? If you instead had run the simulation to nextfloat(0.4517539514526232) = 0.45175395145262326, we would see that the value of u[1] = -1.2647055847076505e-15. You can see this by changing the rootfind argument of the callback:\n\nfloor_event = DE.ContinuousCallback(condition, floor_aff!, rootfind = DE.SciMLBase.RightRootFind)\nu0 = [1.0, 0.0]\np = [1.0]\nprob = DE.ODEProblem{true}(dynamics!, u0, (0.0, 1.75), p)\nsol = DE.solve(prob, DE.Tsit5(), callback = floor_event)\nsol[end] # [-1.2647055847076505e-15, 0.0]\n\nWhat this means is that there is no 64-bit floating-point number t such that the condition is zero! By default, if there is no t such that condition=0, then the rootfinder defaults to choosing the floating-point number exactly before the event (LeftRootFind). This way manifold constraints are preserved by default (i.e. the ball never goes below the floor). However, if you require that the condition is exactly satisfied after the event, you will want to add such a change to the affect! function. For example, the error correction in this case is to add int.u[1] = 0 to the affect!, i.e.:\n\nfunction floor_aff!(int)\n    int.p[1] = 0\n    int.u[1] = 0\n    int.u[2] = 0\n    @show int.u[1], int.t\nend\nfloor_event = DE.ContinuousCallback(condition, floor_aff!)\nu0 = [1.0, 0.0]\np = [1.0]\nprob = DE.ODEProblem{true}(dynamics!, u0, (0.0, 1.75), p)\nsol = DE.solve(prob, DE.Tsit5(), callback = floor_event)\nsol[end] # [0.0,0.0]\n\nand now the sticky behavior is perfect to the floating-point.","category":"section"},{"location":"features/callback_functions/#Handling-Accumulation-Points","page":"Event Handling and Callback Functions","title":"Handling Accumulation Points","text":"Now let's take a look at the bouncing ball with friction. After the bounce, we will send the velocity to -v/2. Since the velocity is halving each time, we should have Zeno-like behavior and see an accumulation point of bounces. We will use some extra parameters to count the number of bounces (to infinity) and find the accumulation point. Let's watch!\n\nfunction dynamics!(du, u, p, t)\n    du[1] = u[2]\n    du[2] = -9.8\nend\nfunction floor_aff!(int)\n    int.u[2] *= -0.5\n    int.p[1] += 1\n    int.p[2] = int.t\nend\nfloor_event = DE.ContinuousCallback(condition, floor_aff!)\nu0 = [1.0, 0.0]\np = [0.0, 0.0]\nprob = DE.ODEProblem{true}(dynamics!, u0, (0.0, 2.0), p)\nsol = DE.solve(prob, DE.Tsit5(), callback = floor_event)\nPlots.plot(sol)\n\nFrom the readout, we can see the ball only bounced 8 times before it went below the floor, what happened? What happened is floating-point error. Because one cannot guarantee that floating-point numbers exist to make the condition=0, a heuristic is used to ensure that a zero is not accidentally detected at nextfloat(t) after the simulation restarts (otherwise it would repeatedly find the same event!). However, sooner or later, the ability to detect minute floating point differences will crash, and what should be infinitely many bounces finally misses a bounce.\n\nThis leads to two questions:\n\nHow can you improve the accuracy of an accumulation calculation?\nHow can you make it gracefully continue?\n\nFor (1), note that floating-point accuracy is dependent on the current dt. If you know that an accumulation point is coming, one can use set_proposed_dt! to shrink the dt value and help find the next bounce point. You can use t - tprev to know the length of the previous interval for this calculation. For this example, we can set the proposed dt to (t - tprev)/10 to ensure an ever-increasing accuracy of the check.\n\nHowever, at some point we will hit machine epsilon, the value where t + eps(t) == t, so we cannot measure infinitely many bounces and instead will be limited by the floating-point accuracy of our number representation. Using alternative number types like ArbNumerics.jl can allow for this to be done at very high accuracy, but still not infinite. Thus, what we need to do is determine a tolerance after which we assume the accumulation has been reached and define the exit behavior. In this case, we will say when the dt<1e-12, we are almost at the edge of Float64 accuracy (eps(1.0) = 2.220446049250313e-16), so we will change the position and velocity to exactly zero.\n\nWith these floating-point corrections in mind, the accumulation calculation looks as follows:\n\nfunction dynamics!(du, u, p, t)\n    du[1] = u[2]\n    du[2] = p[1] * -9.8\nend\nfunction floor_aff!(int)\n    int.u[2] *= -0.5\n    if int.dt > 1e-12\n        DE.set_proposed_dt!(int, (int.t - int.tprev) / 100)\n    else\n        int.u[1] = 0\n        int.u[2] = 0\n        int.p[1] = 0\n    end\n    int.p[2] += 1\n    int.p[3] = int.t\nend\nfloor_event = DE.ContinuousCallback(condition, floor_aff!)\nu0 = [1.0, 0.0]\np = [1.0, 0.0, 0.0]\nprob = DE.ODEProblem{true}(dynamics!, u0, (0.0, 2.0), p)\nsol = DE.solve(prob, DE.Tsit5(), callback = floor_event)\nPlots.plot(sol)\n\nWith this corrected version, we see that after 41 bounces, the accumulation point is reached at t = 1.355261854357056. To really see the accumulation, let's zoom in:\n\np1 = Plots.plot(sol, idxs = 1, tspan = (1.25, 1.40))\np2 = Plots.plot(sol, idxs = 1, tspan = (1.35, 1.36))\np3 = Plots.plot(sol, idxs = 1, tspan = (1.354, 1.35526))\np4 = Plots.plot(sol, idxs = 1, tspan = (1.35526, 1.35526185))\nPlots.plot(p1, p2, p3, p4)\n\nI think Zeno would be proud of our solution.","category":"section"},{"location":"features/callback_functions/#Example-2:-Terminating-an-Integration","page":"Event Handling and Callback Functions","title":"Example 2: Terminating an Integration","text":"Often, you might want to terminate an integration when some condition is satisfied. To terminate an integration, use terminate!(integrator) as the affect! in a callback.\n\nIn this example, we will solve the differential equation:\n\nimport DifferentialEquations as DE\nu0 = [1.0, 0.0]\nfunction fun2(du, u, p, t)\n    du[2] = -u[1]\n    du[1] = u[2]\nend\ntspan = (0.0, 10.0)\nprob = DE.ODEProblem(fun2, u0, tspan)\n\nwhich has cosine and -sine as the solutions respectively. We wish to solve until the sine part, u[2] becomes positive. There are two things we may be looking for.\n\nA DiscreteCallback will cause this to halt at the first step such that the condition is satisfied. For example, we could use:\n\ncondition(u, t, integrator) = u[2] > 0\naffect!(integrator) = DE.terminate!(integrator)\ncb = DE.DiscreteCallback(condition, affect!)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb)\n\nHowever, we often wish to halt exactly at the point of time that the condition is satisfied. To achieve that, we use a continuous callback. The condition must thus be a function which is zero at the point we want to halt. Thus, we use the following:\n\ncondition(u, t, integrator) = u[2]\naffect!(integrator) = DE.terminate!(integrator)\ncb = DE.ContinuousCallback(condition, affect!)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb)\nimport Plots;\nPlots.plot(sol);\n\nNote that this uses rootfinding to approximate the “exact” moment of the crossing. Analytically we know the value is pi, and here the integration terminates at\n\nsol.t[end] # 3.1415902502224307\n\nUsing a more accurate integration increases the accuracy of this prediction:\n\nsol = DE.solve(prob, DE.Vern8(), callback = cb, reltol = 1e-12, abstol = 1e-12)\n#π = 3.141592653589703...\nsol.t[end] # 3.1415926535896035\n\nNow say we wish to find when the first period is over, i.e. we want to ignore the upcrossing and only stop on the downcrossing. We do this by ignoring the affect! and only passing an affect! for the second:\n\ncondition(u, t, integrator) = u[2]\naffect!(integrator) = DE.terminate!(integrator)\ncb = DE.ContinuousCallback(condition, nothing, affect!)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb)\nPlots.plot(sol)\n\nNotice that passing only one affect! is the same as ContinuousCallback(condition,affect!,affect!), i.e. both upcrossings and downcrossings will activate the event. Using ContinuousCallback(condition,affect!,nothing)will thus be the same as above because the first event is an upcrossing.","category":"section"},{"location":"features/callback_functions/#Example-3:-Growing-Cell-Population","page":"Event Handling and Callback Functions","title":"Example 3: Growing Cell Population","text":"Another interesting issue is with models of changing sizes. The ability to handle such events is a unique feature of DifferentialEquations.jl! The problem we would like to tackle here is a cell population. We start with 1 cell with a protein X which increases linearly with time with rate parameter α. Since we will be changing the size of the population, we write the model in the general form:\n\nconst α = 0.3\nfunction f(du, u, p, t)\n    for i in 1:length(u)\n        du[i] = α * u[i]\n    end\nend\n\nOur model is that, whenever the protein X gets to a concentration of 1, it triggers a cell division. So we check to see if any concentrations hit 1:\n\nfunction condition(u, t, integrator) # Event when condition(u,t,integrator) == 0\n    1 - maximum(u)\nend\n\nAgain, recall that this function finds events as when condition==0, so 1-maximum(u) is positive until a cell has a concentration of X which is 1, which then triggers the event. At the event, we have that the cell splits into two cells, giving a random amount of protein to each one. We can do this by resizing the cache (adding 1 to the length of all of the caches) and setting the values of these two cells at the time of the event:\n\nfunction affect!(integrator)\n    u = integrator.u\n    maxidx = findmax(u)[2]\n    DE.resize!(integrator, length(u) + 1)\n    Θ = rand()\n    u[maxidx] = Θ\n    u[end] = 1 - Θ\n    nothing\nend\n\nAs noted in the Integrator Interface, resize!(integrator,length(integrator.u)+1) is used to change the length of all of the internal caches (which includes u) to be their current length + 1, growing the ODE system. Then the following code sets the new protein concentrations. Now we can solve:\n\nimport DifferentialEquations as DE\ncallback = DE.ContinuousCallback(condition, affect!)\nu0 = [0.2]\ntspan = (0.0, 10.0)\nprob = DE.ODEProblem(f, u0, tspan)\nsol = DE.solve(prob, callback = callback)\n\nThe plot recipes do not have a way of handling the changing size, but we can plot from the solution object directly. For example, let's make a plot of how many cells there are at each time. Since these are discrete values, we calculate and plot them directly:\n\nimport Plots\nPlots.plot(sol.t, map((x) -> length(x), sol[:]), lw = 3,\n    ylabel = \"Number of Cells\", xlabel = \"Time\")\n\nNow let's check in on a cell. We can still use the interpolation to get a nice plot of the concentration of cell 1 over time. This is done with the command:\n\nts = range(0, stop = 10, length = 100)\nPlots.plot(ts, map((x) -> x[1], sol.(ts)), lw = 3,\n    ylabel = \"Amount of X in Cell 1\", xlabel = \"Time\")\n\nNotice that every time it hits 1 the cell divides, giving cell 1 a random amount of X which then grows until the next division.\n\nNote that one macro which was not shown in this example is deleteat! on the caches. For example, to delete the second cell, we could use:\n\nDE.deleteat!(integrator, 2)\n\nThis allows you to build sophisticated models of populations with births and deaths.","category":"section"},{"location":"features/callback_functions/#VectorContinuousCallback-Example","page":"Event Handling and Callback Functions","title":"VectorContinuousCallback Example","text":"","category":"section"},{"location":"features/callback_functions/#Example-1:-Bouncing-Ball-with-multiple-walls","page":"Event Handling and Callback Functions","title":"Example 1: Bouncing Ball with multiple walls","text":"This is similar to the above Bouncing Ball example, but now we have two more vertical walls, at x = 0 and x = 10.0. We have our ODEFunction as -\n\nfunction f(du, u, p, t)\n    du[1] = u[2]\n    du[2] = -p\n    du[3] = u[4]\n    du[4] = 0.0\nend\n\nwhere u[1] denotes y-coordinate, u[2] denotes velocity in y-direction, u[3] denotes x-coordinate and u[4] denotes velocity in x-direction. We will make a VectorContinuousCallback of length 2 - one for x axis collision, one for walls parallel to y axis.\n\nfunction condition(out, u, t, integrator) # Event when condition(out,u,t,integrator) == 0\n    out[1] = u[1]\n    out[2] = (u[3] - 10.0)u[3]\nend\n\nfunction affect!(integrator, idx)\n    if idx == 1\n        integrator.u[2] = -0.9integrator.u[2]\n    elseif idx == 2\n        integrator.u[4] = -0.9integrator.u[4]\n    end\nend\nimport DifferentialEquations as DE\ncb = DE.VectorContinuousCallback(condition, affect!, 2)\n\nIt is evident that out[2] will be zero when u[3] (x-coordinate) is either 0.0 or 10.0. And when that happens, we flip the velocity with some coefficient of restitution (0.9).\n\nCompleting the rest of the code\n\nu0 = [50.0, 0.0, 0.0, 2.0]\ntspan = (0.0, 15.0)\np = 9.8\nprob = DE.ODEProblem(f, u0, tspan, p)\nsol = DE.solve(prob, DE.Tsit5(), callback = cb, dt = 1e-3, adaptive = false)\nimport Plots;\nPlots.plot(sol, idxs = (1, 3));","category":"section"},{"location":"features/callback_functions/#SciMLBase.ContinuousCallback","page":"Event Handling and Callback Functions","title":"SciMLBase.ContinuousCallback","text":"ContinuousCallback(condition, affect!, affect_neg!;\n    initialize = INITIALIZE_DEFAULT,\n    finalize = FINALIZE_DEFAULT,\n    idxs = nothing,\n    rootfind = LeftRootFind,\n    save_positions = (true, true),\n    interp_points = 10,\n    abstol = 10eps(), reltol = 0, repeat_nudge = 1 // 100,\n    initializealg = nothing)\n\nContinuousCallback(condition, affect!;\n    initialize = INITIALIZE_DEFAULT,\n    finalize = FINALIZE_DEFAULT,\n    idxs = nothing,\n    rootfind = LeftRootFind,\n    save_positions = (true, true),\n    affect_neg! = affect!,\n    interp_points = 10,\n    abstol = 10eps(), reltol = 0, repeat_nudge = 1 // 100,\n    initializealg = nothing)\n\nContains a single callback whose condition is a continuous function. The callback is triggered when this function evaluates to 0.\n\nArguments\n\ncondition: This is a function condition(u,t,integrator) for declaring when the callback should be used. A callback is initiated if the condition hits 0 within the time interval. See the Integrator Interface documentation for information about integrator.\naffect!: This is the function affect!(integrator) where one is allowed to modify the current state of the integrator. If you do not pass an affect_neg! function, it is called when condition is found to be 0 (at a root) and the cross is either an upcrossing (from negative to positive) or a downcrossing (from positive to negative). You need to explicitly pass nothing as the affect_neg! argument if it should only be called at upcrossings, e.g. ContinuousCallback(condition, affect!, nothing). For more information on what can be done, see the Integrator Interface manual page. Modifications to u are safe in this function.\naffect_neg!=affect!: This is the function affect_neg!(integrator) where one is allowed to modify the current state of the integrator. This is called when condition is found to be 0 (at a root) and the cross is a downcrossing (from positive to negative). For more information on what can be done, see the Integrator Interface manual page. Modifications to u are safe in this function.\nrootfind=LeftRootFind: This is a flag to specify the type of rootfinding to do for finding event location. If this is set to LeftRootfind, the solution will be backtracked to the point where condition==0 and if the solution isn't exact, the left limit of root is used. If set to RightRootFind, the solution would be set to the right limit of the root. Otherwise, the systems and the affect! will occur at t+dt. Note that these enums are not exported, and thus one needs to reference them as SciMLBase.LeftRootFind, SciMLBase.RightRootFind, or SciMLBase.NoRootFind.\ninterp_points=10: The number of interpolated points to check the condition. The condition is found by checking whether any interpolation point / endpoint has a different sign. If interp_points=0, then conditions will only be noticed if the sign of condition is different at t than at t+dt. This behavior is not robust when the solution is oscillatory, and thus it's recommended that one use some interpolation points (they're cheap to compute!). 0 within the time interval.\nsave_positions=(true,true): Boolean tuple for whether to save before and after the affect!. This saving will occur just before and after the event, only at event times, and does not depend on options like saveat, save_everystep, etc. (i.e. if saveat=[1.0,2.0,3.0], this can still add a save point at 2.1 if true). For discontinuous changes like a modification to u to be handled correctly (without error), one should set save_positions=(true,true).\nidxs=nothing: The components which will be interpolated into the condition. Defaults to nothing which means u will be all components.\ninitialize: This is a function (c,u,t,integrator) which can be used to initialize the state of the callback c. It should modify the argument c and the return is ignored.\nfinalize: This is a function (c,u,t,integrator) which can be used to finalize the state of the callback c. It can modify the argument c and the return is ignored.\nabstol=1e-14 & reltol=0: These are used to specify a tolerance from zero for the rootfinder: if the starting condition is less than the tolerance from zero, then no root will be detected. This is to stop repeat events happening immediately after a rootfinding event.\nrepeat_nudge = 1//100: This is used to set the next testing point after a previously found zero. Defaults to 1//100, which means after a callback, the next sign check will take place at t + dt*1//100 instead of at t to avoid repeats.\ninitializealg = nothing: In the context of a DAE, this is the algorithm that is used to run initialization after the effect. The default of nothing defers to the initialization algorithm provided in the solve.\n\nwarning: Warning\nThe effect of using a callback with a DAE needs to be done with care because the solution u needs to satisfy the algebraic constraints before taking the next step. For this reason, a consistent initialization calculation must be run after running the callback. If the chosen initialization alg is BrownFullBasicInit() (the default for solve), then the initialization will change the algebraic variables to satisfy the conditions. Thus if x is an algebraic variable and the callback performs x+=1, the initialization may \"revert\" the change to satisfy the constraints. This behavior can be removed by setting initializealg = CheckInit(), which simply checks that the state u is consistent, but requires that the result of the affect! satisfies the constraints (or else errors). It is not recommended that NoInit() is used as that will lead to an unstable step following initialization. This warning can be ignored for non-DAE ODEs.\n\nExtended help\n\nsaved_clock_partitions: An iterable of clock partition indices to save after the callback triggers. MTK-only API\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#SciMLBase.DiscreteCallback","page":"Event Handling and Callback Functions","title":"SciMLBase.DiscreteCallback","text":"DiscreteCallback(condition, affect!;\n    initialize = INITIALIZE_DEFAULT,\n    finalize = FINALIZE_DEFAULT,\n    save_positions = (true, true),\n    initializealg = nothing)\n\nArguments\n\ncondition: This is a function condition(u,t,integrator) for declaring when the callback should be used. A callback is initiated if the condition evaluates to true. See the Integrator Interface documentation for information about integrator.\naffect!: This is the function affect!(integrator) where one is allowed to modify the current state of the integrator. For more information on what can be done, see the Integrator Interface manual page.\nsave_positions: Boolean tuple for whether to save before and after the affect!. This saving will occur just before and after the event, only at event times, and does not depend on options like saveat, save_everystep, etc. (i.e. if saveat=[1.0,2.0,3.0], this can still add a save point at 2.1 if true). For discontinuous changes like a modification to u to be handled correctly (without error), one should set save_positions=(true,true).\ninitialize: This is a function (c,u,t,integrator) which can be used to initialize the state of the callback c. It should modify the argument c and the return is ignored.\nfinalize: This is a function (c,u,t,integrator) which can be used to finalize the state of the callback c. It should can the argument c and the return is ignored.\ninitializealg = nothing: In the context of a DAE, this is the algorithm that is used to run initialization after the effect. The default of nothing defers to the initialization algorithm provided in the solve.\n\nwarning: Warning\nThe effect of using a callback with a DAE needs to be done with care because the solution u needs to satisfy the algebraic constraints before taking the next step. For this reason, a consistent initialization calculation must be run after running the callback. If the chosen initialization alg is BrownFullBasicInit() (the default for solve), then the initialization will change the algebraic variables to satisfy the conditions. Thus if x is an algebraic variable and the callback performs x+=1, the initialization may \"revert\" the change to satisfy the constraints. This behavior can be removed by setting initializealg = CheckInit(), which simply checks that the state u is consistent, but requires that the result of the affect! satisfies the constraints (or else errors). It is not recommended that NoInit() is used as that will lead to an unstable step following initialization. This warning can be ignored for non-DAE ODEs.\n\nExtended help\n\nsaved_clock_partitions: An iterable of clock partition indices to save after the callback triggers. MTK-only API\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#SciMLBase.CallbackSet","page":"Event Handling and Callback Functions","title":"SciMLBase.CallbackSet","text":"struct CallbackSet{T1<:Tuple, T2<:Tuple} <: SciMLBase.DECallback\n\nMultiple callbacks can be chained together to form a CallbackSet. A CallbackSet is constructed by passing the constructor ContinuousCallback, DiscreteCallback, VectorContinuousCallback or other CallbackSet instances:\n\nCallbackSet(cb1,cb2,cb3)\n\nYou can pass as many callbacks as you like. When the solvers encounter multiple callbacks, the following rules apply:\n\nContinuousCallbacks and VectorContinuousCallbacks are applied before DiscreteCallbacks. (This is because they often implement event-finding that will backtrack the timestep to smaller than dt).\nFor ContinuousCallbacks and VectorContinuousCallbacks, the event times are found by rootfinding and only the first ContinuousCallback or VectorContinuousCallback affect is applied.\nThe DiscreteCallbacks are then applied in order. Note that the ordering only matters for the conditions: if a previous callback modifies u in such a way that the next callback no longer evaluates condition to true, its affect will not be applied.\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#SciMLBase.VectorContinuousCallback","page":"Event Handling and Callback Functions","title":"SciMLBase.VectorContinuousCallback","text":"VectorContinuousCallback(condition, affect!, affect_neg!, len;\n    initialize = INITIALIZE_DEFAULT,\n    finalize = FINALIZE_DEFAULT,\n    idxs = nothing,\n    rootfind = LeftRootFind,\n    save_positions = (true, true),\n    interp_points = 10,\n    abstol = 10eps(), reltol = 0, repeat_nudge = 1 // 100,\n    initializealg = nothing)\n\nVectorContinuousCallback(condition, affect!, len;\n    initialize = INITIALIZE_DEFAULT,\n    finalize = FINALIZE_DEFAULT,\n    idxs = nothing,\n    rootfind = LeftRootFind,\n    save_positions = (true, true),\n    affect_neg! = affect!,\n    interp_points = 10,\n    abstol = 10eps(), reltol = 0, repeat_nudge = 1 // 100,\n    initializealg = nothing)\n\nThis is also a subtype of AbstractContinuousCallback. CallbackSet is not feasible when you have many callbacks, as it doesn't scale well. For this reason, we have VectorContinuousCallback - it allows you to have a single callback for multiple events.\n\nArguments\n\ncondition: This is a function condition(out, u, t, integrator) which should save the condition value in the array out at the right index. Maximum index of out should be specified in the len property of callback. So, this way you can have a chain of len events, which would cause the ith event to trigger when out[i] = 0.\naffect!: This is a function affect!(integrator, event_index) which lets you modify integrator and it tells you about which event occurred using event_idx i.e. gives you index i for which out[i] came out to be zero.\nlen: Number of callbacks chained. This is compulsory to be specified.\n\nRest of the arguments have the same meaning as in ContinuousCallback.\n\nExtended help\n\nsaved_clock_partitions: An iterable of len elements, where the ith element is an iterable of clock partition indices to save when the ith event triggers. MTK-only API.\n\n\n\n\n\n","category":"type"},{"location":"solvers/ode_solve/#ode_solve","page":"ODE Solvers","title":"ODE Solvers","text":"solve(prob::ODEProblem,alg;kwargs)\n\nSolves the ODE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"section"},{"location":"solvers/ode_solve/#Recommended-Methods","page":"ODE Solvers","title":"Recommended Methods","text":"It is suggested that you try choosing an algorithm using the alg_hints keyword argument. However, in some cases you may want something specific, or you may just be curious. This guide is to help you choose the right algorithm.","category":"section"},{"location":"solvers/ode_solve/#Unknown-Stiffness-Problems","page":"ODE Solvers","title":"Unknown Stiffness Problems","text":"When the stiffness of the problem is unknown, it is recommended you use a stiffness detection and auto-switching algorithm. These methods are multi-paradigm and allow for efficient solution of both stiff and non-stiff problems. The cost for auto-switching is very minimal, but the choices are restrained. They are a good go-to method when applicable.\n\nFor default tolerances, AutoTsit5(Rosenbrock23()) is a good choice. For lower tolerances, using AutoVern7 or AutoVern9 with Rodas4, KenCarp4, or Rodas5P can all be good choices depending on the problem. For very large systems (>1000 ODEs?), consider using lsoda.","category":"section"},{"location":"solvers/ode_solve/#Non-Stiff-Problems","page":"ODE Solvers","title":"Non-Stiff Problems","text":"For non-stiff problems, the native OrdinaryDiffEq.jl algorithms are vastly more efficient than the other choices. For most non-stiff problems, we recommend Tsit5. When more robust error control is required, BS5 is a good choice. If at moderate tolerances and the interpolation error is very important, consider the OwrenZen5 method. For fast solving at higher tolerances, we recommend BS3 (or OwrenZen3 if the interpolation error is important). For high accuracy but with the range of Float64 (~1e-8-1e-12), we recommend Vern6, Vern7, or Vern8 as efficient choices. For very small non-stiff ODEs, SimpleATsit5(), GPUVern7(), or GPUVern9() (available in the SimpleDiffEq package) is a simplified implementation of Tsit5 that can cut out extra overhead and is recommended in those scenarios.\n\nFor high accuracy non-stiff solving (BigFloat and tolerances like <1e-12), we recommend the Vern9 method. If a high-order method is needed with a high order interpolant, then you should choose Vern9 which is Order 9 with an Order 9 interpolant. If you require extremely high accuracy (<1e-30?) and do not need an interpolant, try the Feagin12 or Feagin14 methods. Note that the Feagin methods are the only high-order optimized methods which do not include a high-order interpolant (they do include a 3rd order Hermite interpolation if needed). Note that these high order RK methods are more robust than the high order Adams-Bashforth methods to discontinuities and achieve very high precision, and are much more efficient than the extrapolation methods. However, the VCABM method can be a good choice for high accuracy when the system of equations is very large (>1,000 ODEs?), the function calculation is very expensive, or the solution is very smooth.\n\nIf strict error bounds are needed, then adaptive methods with defect controls are required. Defect controls use an error measurement on the interpolating polynomial to make the error estimate better capture the error over the full interval. For medium accuracy calculations, RK4 is a good choice.","category":"section"},{"location":"solvers/ode_solve/#Stiff-Problems","page":"ODE Solvers","title":"Stiff Problems","text":"For stiff problems at high tolerances (>1e-2?) it is recommended that you use Rosenbrock23 or TRBDF2. These are robust to oscillations and massive stiffness, though are only efficient when low accuracy is needed. Rosenbrock23 is more efficient for small systems where re-evaluating and re-factorizing the Jacobian is not too costly, and for sufficiently large systems TRBDF2 will be more efficient. QNDF or FBDF can be the most efficient for the largest systems or most expensive f.\n\nAt medium tolerances (>1e-8?) it is recommended you use Rodas5P, Rodas4P (the former is more efficient, but the latter is more reliable), Kvaerno5, or KenCarp4. As native DifferentialEquations.jl solvers, many Julia numeric types (such as BigFloats, ArbNumerics.jl, or DecFP) will work. When the equation is defined via the @ode_def macro, these will be the most efficient.\n\nFor faster solving at low tolerances (<1e-9) but when Vector{Float64} is used, use radau.\n\nFor asymptotically large systems of ODEs (N>1000?) where f is very costly, and the complex eigenvalues are minimal (low oscillations), in that case QNDF or FBDF will be the most efficient. QNDF and FBDF will also do surprisingly well if the solution is smooth. However, this method can handle less stiffness than other methods and its Newton iterations may fail at low accuracy situations. Other choices to consider in this regime are CVODE_BDF and lsoda.","category":"section"},{"location":"solvers/ode_solve/#Special-Properties-of-Stiff-Integrators","page":"ODE Solvers","title":"Special Properties of Stiff Integrators","text":"ImplicitMidpoint is a symmetric and symplectic integrator. Trapezoid is a symmetric (almost symplectic) integrator with adaptive timestepping. ImplicitEuler is an extension to the common algorithm with adaptive timestepping, and efficient quasi-Newton Jacobian re-usage which is fully strong-stability preserving (SSP) for hyperbolic PDEs.\n\nNotice that Rodas4 loses accuracy on discretizations of nonlinear parabolic PDEs, and thus it's suggested you replace it with Rodas4P in those situations which are 3rd order. Similarly, between Rodas5 and Rodas5P. ROS3P is only third order and achieves 3rd order on such problems and can thus be more efficient in this case.","category":"section"},{"location":"solvers/ode_solve/#Translations-from-MATLAB/Python/R","page":"ODE Solvers","title":"Translations from MATLAB/Python/R","text":"For users familiar with MATLAB/Python/R, good translations of the standard library methods are as follows:\n\node23 –> BS3()\node45/dopri5 –> DP5(), though in most cases Tsit5() is more efficient\node23s –> Rosenbrock23(), though in most cases Rodas5P() is more efficient\node113 –> VCABM(), though in many cases Vern7() is more efficient\ndop853 –> DP8(), though in most cases Vern7() is more efficient\node15s/vode –> QNDF() or FBDF(), though in many cases Rodas5P(), KenCarp4(), TRBDF2(), or RadauIIA5() are more efficient\node23t –> Trapezoid()\node23tb –> TRBDF2()\nlsoda –> lsoda(), though AutoTsit5(Rosenbrock23()) or AutoVern7(Rodas5()) may be more efficient. Note that lsoda() requires the LSODA.jl extension, which can be added via ]add LSODA; using LSODA.\node15i –> IDA() or DFBDF(), though in many cases Rodas5P() can handle the DAE and is significantly more efficient.","category":"section"},{"location":"solvers/ode_solve/#Full-List-of-Methods","page":"ODE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/ode_solve/#OrdinaryDiffEq.jl-for-Non-Stiff-Equations","page":"ODE Solvers","title":"OrdinaryDiffEq.jl for Non-Stiff Equations","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a “free” interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).\n\nThe OrdinaryDiffEq.jl algorithms achieve the highest performance for non-stiff equations while being the most generic: accepting the most Julia-based types, allow for sophisticated event handling, etc. On stiff ODEs, these algorithms again consistently among the top. OrdinaryDiffEq.jl is recommended for most ODE problems.","category":"section"},{"location":"solvers/ode_solve/#Explicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Explicit Runge-Kutta Methods","text":"Euler- The canonical forward Euler method. Fixed timestep only.\nMidpoint - The second order midpoint method. Uses embedded Euler method for adaptivity.\nHeun - The second order Heun's method. Uses embedded Euler method for adaptivity.\nRalston - The optimized second order midpoint method. Uses embedded Euler method for adaptivity.\nRK4 - The canonical Runge-Kutta Order 4 method. Uses a defect control for adaptive stepping using maximum error over the whole interval.\nBS3 - Bogacki-Shampine 3/2 method.\nOwrenZen3 - Owren-Zennaro optimized interpolation 3/2 method (free 3rd order interpolant).\nOwrenZen4 - Owren-Zennaro optimized interpolation 4/3 method (free 4th order interpolant).\nOwrenZen5 - Owren-Zennaro optimized interpolation 5/4 method (free 5th order interpolant).\nDP5 - Dormand-Prince's 5/4 Runge-Kutta method. (free 4th order interpolant).\nTsit5 - Tsitouras 5/4 Runge-Kutta method. (free 4th order interpolant).\nAnas5(w) - 4th order Runge-Kutta method designed for periodic problems. Requires a periodicity estimate w which when accurate the method becomes 5th order (and is otherwise 4th order with less error for better estimates).\nFRK65(w=0) - Zero Dissipation Runge-Kutta of 6th order. Takes an optional argument w to for the periodicity phase, in which case this method results in zero numerical dissipation.\nPFRK87(w=0) - Phase-fitted Runge-Kutta of 8th order. Takes an optional argument w to for the periodicity phase, in which case this method results in zero numerical dissipation.\nRKO65 - Tsitouras' Runge-Kutta-Oliver 6 stage 5th order method. This method is robust on problems which have a singularity at t=0.\nTanYam7 - Tanaka-Yamashita 7 Runge-Kutta method.\nDP8 - Hairer's 8/5/3 adaption of the Dormand-Prince Runge-Kutta method. (7th order interpolant).\nTsitPap8 - Tsitouras-Papakostas 8/7 Runge-Kutta method.\nFeagin10 - Feagin's 10th-order Runge-Kutta method.\nFeagin12 - Feagin's 12th-order Runge-Kutta method.\nFeagin14 - Feagin's 14th-order Runge-Kutta method.\nMSRK5 - Stepanov 5th-order Runge-Kutta method.\nMSRK6 - Stepanov 6th-order Runge-Kutta method.\nStepanov5 - Stepanov adaptive 5th-order Runge-Kutta method.\nSIR54 - 5th order explicit Runge-Kutta method suited for SIR-type epidemic models.\nAlshina2 - Alshina 2nd-order Runge-Kutta method.\nAlshina3 - Alshina 3rd-order Runge-Kutta method.\nAlshina6 - Alshina 6th-order Runge-Kutta method.\n\nExample usage:\n\nalg = Tsit5()\nsolve(prob, alg)\n\nAdditionally, the following algorithms have a lazy interpolant:\n\nBS5 - Bogacki-Shampine 5/4 Runge-Kutta method. (lazy 5th order interpolant).\nVern6 - Verner's “Most Efficient” 6/5 Runge-Kutta method. (lazy 6th order interpolant).\nVern7 - Verner's “Most Efficient” 7/6 Runge-Kutta method. (lazy 7th order interpolant).\nVern8 - Verner's “Most Efficient” 8/7 Runge-Kutta method. (lazy 8th order interpolant)\nVern9 - Verner's “Most Efficient” 9/8 Runge-Kutta method. (lazy 9th order interpolant)\n\nThese methods require a few extra steps in order to compute the high order interpolation, but these steps are only taken when the interpolation is used. These methods when lazy assume that the parameter vector p will be unchanged between the moment of the interval solving and the interpolation. If p is changed in a ContinuousCallback, or in a DiscreteCallback and the continuous solution is used after the full solution, then set lazy=false.\n\nExample:\n\nsolve(prob, Vern7()) # lazy by default\nsolve(prob, Vern7(lazy = false))","category":"section"},{"location":"solvers/ode_solve/#Parallel-Explicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Parallel Explicit Runge-Kutta Methods","text":"KuttaPRK2p5 - A 5 parallel, 2 processor explicit Runge-Kutta method of 5th order.\n\nThese methods utilize multithreading on the f calls to parallelize the problem. This requires that simultaneous calls to f are thread-safe.","category":"section"},{"location":"solvers/ode_solve/#Explicit-Strong-Stability-Preserving-Runge-Kutta-Methods-for-Hyperbolic-PDEs-(Conservation-Laws)","page":"ODE Solvers","title":"Explicit Strong-Stability Preserving Runge-Kutta Methods for Hyperbolic PDEs (Conservation Laws)","text":"SSPRK22 - The two-stage, second order strong stability preserving (SSP) method of Shu and Osher (SSP coefficient 1, free 2nd order SSP interpolant). Fixed timestep only.\nSSPRK33 - The three-stage, third order strong stability preserving (SSP) method of Shu and Osher (SSP coefficient 1, free 2nd order SSP interpolant). Fixed timestep only.\nSSPRK53 - The five-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 2.65, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK63 - The six-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 3.518, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK73 - The seven-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 4.2879, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK83 - The eight-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 5.107, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK432 - A  3/2 adaptive strong stability preserving (SSP) method with five stages (SSP coefficient 2, free 2nd order SSP interpolant).\nSSPRK43 - A  3/2 adaptive strong stability preserving (SSP) method with five stages (SSP coefficient 2, free 2nd order SSP interpolant). The main method is the same as SSPRK432, but the embedded method has a larger stability region.\nSSPRK932 - A  3/2 adaptive strong stability preserving (SSP) method with nine stages (SSP coefficient 6, free 3rd order Hermite interpolant).\nSSPRK54 - The five-stage, fourth order strong stability preserving (SSP) method of Spiteri and Ruuth (SSP coefficient 1.508, 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK104 - The ten-stage, fourth order strong stability preserving method of Ketcheson (SSP coefficient 6, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRKMSVS32 - 3-stage, 2nd order SSP-optimal linear multistep method. (SSP coefficient 0.5, 3rd order Hermite interpolant). Fixed timestep only.\nSSPRKMSVS43 - 4-stage, 3rd order SSP-optimal linear multistep method. (SSP coefficient 0.33, 3rd order Hermite interpolant). Fixed timestep only.\n\nThe SSP coefficients of the methods can be queried as ssp_coefficient(alg). All explicit SSP methods take two optional arguments SSPXY(stage_limiter!, step_limiter!), where stage_limiter! and step_limiter are functions taking arguments of the form limiter!(u, integrator, p, t). Here, u is the new solution value (updated inplace) after an explicit Euler stage / the whole time step, integrator the ODE integrator, and t the current time. These limiters can be used to enforce physical constraints, e.g., the positivity preserving limiters of Zhang and Shu (Zhang, Xiangxiong, and Chi-Wang Shu. \"Maximum-principle-satisfying and positivity-preserving high-order schemes for conservation laws: survey and new developments.\" Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. The Royal Society, 2011.).","category":"section"},{"location":"solvers/ode_solve/#Low-Storage-Methods","page":"ODE Solvers","title":"Low-Storage Methods","text":"ORK256 - 5-stage, second order low-storage method for wave propagation equations. Fixed timestep only. Like SSPRK methods, ORK256 also takes optional arguments stage_limiter!, step_limiter!, where stage_limiter! and step_limiter! are functions of the form limiter!(u, integrator, p, t).\nSSPRK53_2N1 and SSPRK53_2N2 - 5-stage, third order low-storage methods with large SSP coefficients. (SSP coefficient 2.18 and 2.15, free 3rd order Hermite interpolant). Fixed timestep only.\nCarpenterKennedy2N54 - The five-stage, fourth order low-storage method of Carpenter and Kennedy (free 3rd order Hermite interpolant). Fixed timestep only. Designed for hyperbolic PDEs (stability properties). Like SSPRK methods, CarpenterKennedy2N54 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK124 - 12-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only. Like SSPRK methods, NDBLSRK124 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK134 - 13-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only. Like SSPRK methods, NDBLSRK134 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK144 - 14-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only. Like SSPRK methods, NDBLSRK144 also takes optional arguments stage_limiter!, step_limiter!.\nCFRLDDRK64 - 6-stage, fourth order low-storage, low-dissipation, low-dispersion scheme. Fixed timestep only.\nTSLDDRK74 - 7-stage, fourth order low-storage low-dissipation, low-dispersion scheme with maximal accuracy and stability limit along the imaginary axes. Fixed timestep only.\nDGLDDRK73_C - 7-stage, third order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only. Like SSPRK methods, DGLDDRK73_C also takes optional arguments stage_limiter!, step_limiter!.\nDGLDDRK84_C - 8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only. Like SSPRK methods, DGLDDRK84_C also takes optional arguments stage_limiter!, step_limiter!.\nDGLDDRK84_F - 8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when the maximum spatial step size is not constrained. Fixed timestep only. Like SSPRK methods, DGLDDRK84_F also takes optional arguments stage_limiter!, step_limiter!.\nSHLDDRK64 - 6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only. Like SSPRK methods, SHLDDRK64 also takes optional arguments stage_limiter!, step_limiter!.\nRK46NL - 6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only.\nParsaniKetchesonDeconinck3S32 - 3-stage, second order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S82 - 8-stage, second order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S53 - 5-stage, third order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S173 - 17-stage, third order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S94 - 9-stage, fourth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S184 - 18-stage, fourth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S105 - 10-stage, fifth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S205 - 20-stage, fifth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\nCKLLSRK43_2 - 4-stage, third order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK54_3C - 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK95_4S - 9-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK95_4C - 9-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK95_4M - 9-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK54_3C_3R - 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK54_3M_3R - 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK54_3N_3R - 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK85_4C_3R - 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK85_4M_3R - 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK85_4P_3R - 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK54_3N_4R - 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK54_3M_4R - 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK65_4M_4R - 6-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK85_4FM_4R - 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nCKLLSRK75_4M_5R - 7-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\nRDPK3Sp35 - 5-stage, third order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL35 - 5-stage, third order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3Sp49 - 9-stage, fourth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL49 - 9-stage, fourth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3Sp510 - 10-stage, fifth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL510 - 10-stage, fifth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\n\nNOTE: All the 2N Methods (ORK256, CarpenterKennedy2N54, NDBLSRK124, NDBLSRK134, NDBLSRK144, DGLDDRK73_C, DGLDDRK84_C, DGLDDRK84_F and SHLDDRK64) work on the basic principle of being able to perform the step S1 = S1 + F(S2) in just 2 registers. Certain optimizations have been done to achieve this theoretical limit (when alias_u0 is set) but have a limitation that du should always be on the left-hand side (assignments only) in the implementation.\n\nExample - This is an invalid implementation for 2N methods:\n\nfunction f(du, u, p, t)\n    du[1] = u[1] * u[2]\n    du[2] = du[1] * u[2] # du appears on the RHS\nend\n\nIf you don't wish to have the optimization and have to use du on the RHS, please set the keyword argument williamson_condition to false in the algorithm (by default it is set to true). In this case, 3 registers worth of memory would be needed instead.\n\nExample :\n\nalg = CarpenterKennedy2N54(; williamson_condition = false)\n\nSo, the above implementation of f becomes valid.","category":"section"},{"location":"solvers/ode_solve/#Parallelized-Explicit-Extrapolation-Methods","page":"ODE Solvers","title":"Parallelized Explicit Extrapolation Methods","text":"The following are adaptive order, adaptive step size extrapolation methods:\n\nAitkenNeville - Euler extrapolation using Aitken-Neville with the Romberg Sequence.\nExtrapolationMidpointDeuflhard - Midpoint extrapolation using Barycentric coordinates\nExtrapolationMidpointHairerWanner - Midpoint extrapolation using Barycentric coordinates, following Hairer's ODEX in the adaptivity behavior.\n\nThese methods have arguments for max_order, min_order, and init_order on the adaptive order algorithm. The sequence_factor denotes which even multiple of sequence to take while evaluating internal discretizations. threading denotes whether to automatically multithread the f evaluations, allowing for a high degree of within-method parallelism. The defaults are:\n\nmax_order=10\nmin_order=1 except for ExtrapolationMidpointHairerWanner it's 2.\ninit_order=5\nthreading=true\nsequence_factor = 2\n\nAdditionally, the ExtrapolationMidpointDeuflhard and ExtrapolationMidpointHairerWanner methods have the additional argument:\n\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch. Default is :harmonic.\n\nTo override, utilize the keyword arguments. For example:\n\nalg = ExtrapolationMidpointDeuflhard(max_order = 7, min_order = 4, init_order = 4,\n    sequence = :bulirsch, threading = false)\nsolve(prob, alg)\n\nNote that the order that is referred to is the extrapolation order. For AitkenNeville this is the order of the method, for the others an extrapolation order of n gives an order 2(n+1) method.","category":"section"},{"location":"solvers/ode_solve/#Explicit-Multistep-Methods","page":"ODE Solvers","title":"Explicit Multistep Methods","text":"Methods using the approximation at more than one previous mesh point to determine the approximation at the next point are called multistep methods. These methods tend to be more efficient as the size of the system or the cost of f increases.","category":"section"},{"location":"solvers/ode_solve/#Adams-Bashforth-Explicit-Methods","page":"ODE Solvers","title":"Adams-Bashforth Explicit Methods","text":"These methods require a choice of dt.\n\nAB3 - The 3-step third order multistep method. Ralston's Second Order Method is used to calculate starting values.\nAB4 - The 4-step fourth order multistep method. Runge-Kutta method of order 4 is used to calculate starting values.\nAB5 - The 5-step fifth order multistep method. Runge-Kutta method of order 4 is used to calculate starting values.\nABM32 - It is third order method. In ABM32, AB3 works as predictor and Adams Moulton 2-steps method works as Corrector. Ralston's Second Order Method is used to calculate starting values.\nABM43 - It is fourth order method. In ABM43, AB4 works as predictor and Adams Moulton 3-steps method works as Corrector. Runge-Kutta method of order 4 is used to calculate starting values.\nABM54 - It is fifth order method. In ABM54, AB5 works as predictor and Adams Moulton 4-steps method works as Corrector. Runge-Kutta method of order 4 is used to calculate starting values.","category":"section"},{"location":"solvers/ode_solve/#Adaptive-step-size-Adams-explicit-Methods","page":"ODE Solvers","title":"Adaptive step size Adams explicit Methods","text":"VCAB3 - The 3rd order Adams method. Bogacki-Shampine 3/2 method is used to calculate starting values.\nVCAB4 - The 4th order Adams method. Runge-Kutta 4 is used to calculate starting values.\nVCAB5 - The 5th order Adams method. Runge-Kutta 4 is used to calculate starting values.\nVCABM3 - The 3rd order Adams-Moulton method. Bogacki-Shampine 3/2 method is used to calculate starting values.\nVCABM4 - The 4th order Adams-Moulton method. Runge-Kutta 4 is used to calculate starting values.\nVCABM5 - The 5th order Adams-Moulton method. Runge-Kutta 4 is used to calculate starting values.\nVCABM - An adaptive order adaptive time Adams Moulton method. It uses an order adaptivity algorithm is derived from Shampine's DDEABM.\nAN5 - An adaptive 5th order fixed-leading coefficient Adams method in Nordsieck form.\nJVODE_Adams - An adaptive time adaptive order fixed-leading coefficient Adams method in Nordsieck form. The order adaptivity algorithm is derived from Sundials' CVODE_Adams. In development.","category":"section"},{"location":"solvers/ode_solve/#OrdinaryDiffEq.jl-for-Stiff-Equations","page":"ODE Solvers","title":"OrdinaryDiffEq.jl for Stiff Equations","text":"","category":"section"},{"location":"solvers/ode_solve/#SDIRK-Methods","page":"ODE Solvers","title":"SDIRK Methods","text":"ImplicitEuler - A 1st order implicit solver. A-B-L-stable. Adaptive timestepping through a divided differences estimate via memory. Strong-stability preserving (SSP).\nImplicitMidpoint - A second order A-stable symplectic and symmetric implicit solver. Good for highly stiff equations which need symplectic integration.\nTrapezoid - A second order A-stable symmetric ESDIRK method. \"Almost symplectic\" without numerical dampening. Also known as Crank-Nicolson when applied to PDEs. Adaptive timestepping via divided differences on the memory. Good for highly stiff equations which are non-oscillatory.\nTRBDF2 - A second order A-B-L-S-stable one-step ESDIRK method. Includes stiffness-robust error estimates for accurate adaptive timestepping, smoothed derivatives for highly stiff and oscillatory problems.\nSDIRK2 - An A-B-L stable 2nd order SDIRK method\nKvaerno3 - An A-L stable stiffly-accurate 3rd order ESDIRK method\nKenCarp3 - An A-L stable stiffly-accurate 3rd order ESDIRK method with splitting\nCash4 - An A-L stable 4th order SDIRK method\nHairer4 - An A-L stable 4th order SDIRK method\nHairer42 - An A-L stable 4th order SDIRK method\nKvaerno4 - An A-L stable stiffly-accurate 4th order ESDIRK method\nKenCarp4 - An A-L stable stiffly-accurate 4th order ESDIRK method with splitting\nKenCarp47 - An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method with splitting\nKvaerno5 - An A-L stable stiffly-accurate 5th order ESDIRK method\nKenCarp5 - An A-L stable stiffly-accurate 5th order ESDIRK method with splitting\nKenCarp58 - An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method with splitting\nESDIRK54I8L2SA - An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method\nESDIRK436L2SA2 - An A-L stable stiffly-accurate 4th order six-stage ESDIRK method\nESDIRK437L2SA - An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method\nESDIRK547L2SA2 - An A-L stable stiffly-accurate 5th order seven-stage ESDIRK method","category":"section"},{"location":"solvers/ode_solve/#Fully-Implicit-Runge-Kutta-Methods-(FIRK)","page":"ODE Solvers","title":"Fully-Implicit Runge-Kutta Methods (FIRK)","text":"RadauIIA3 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.\nRadauIIA5 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.","category":"section"},{"location":"solvers/ode_solve/#Parallel-Diagonally-Implicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Parallel Diagonally Implicit Runge-Kutta Methods","text":"PDIRK44 - A 2 processor 4th order diagonally non-adaptive implicit method.\n\nThese methods also have option nlsolve same as SDIRK methods. These methods also require f to be thread safe. It parallelizes the nlsolve calls inside the method.","category":"section"},{"location":"solvers/ode_solve/#Rosenbrock-Methods","page":"ODE Solvers","title":"Rosenbrock Methods","text":"ROS3P - 3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\nRodas3 - 3rd order A-stable and stiffly stable Rosenbrock method.\nRodas3P - 3rd order A-stable and stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant and additional error test for interpolation. Keeps accuracy on discretizations of linear parabolic PDEs.\nRosShamp4- An A-stable 4th order Rosenbrock method.\nVeldd4 - A 4th order D-stable Rosenbrock method.\nVelds4 - A 4th order A-stable Rosenbrock method.\nGRK4T - An efficient 4th order Rosenbrock method.\nGRK4A - An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\nRos4LStab - A 4th order L-stable Rosenbrock method.\nRodas4 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas42 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas4P - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\nRodas4P2 - A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems. It is an improvement of Roadas4P and in case of inexact Jacobians a second order W method.\nRodas5 - A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant.\nRodas5P - A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant. Has improved stability in the adaptive time stepping embedding.\nROS2 - A 2nd order L-stable Rosenbrock-Wanner method with 2 internal stages.\nROS3 - A 3rd order L-stable Rosenbrock-Wanner method with 3 internal stages with an embedded strongly A-stable 2nd order method.\nROS2PR - A 2nd order stiffly accurate Rosenbrock-Wanner method with 3 internal stages with Rinf=0. For problems with medium stiffness the convergence behaviour is very poor and it is recommended to use ROS2S instead.\nROS3PR - A 3nd order stiffly accurate Rosenbrock-Wanner method with 3 internal stages and B_PR consistent of order 3, which is strongly A-stable with Rinf~=-0.73.\nScholz4_7 - A 3nd order stiffly accurate Rosenbrock-Wanner method with 3 internal stages and B_PR consistent of order 3, which is strongly A-stable with Rinf~=-0.73. Convergence with order 4 for the stiff case, but has a poor accuracy.\nROS3PRL - A 3nd order stiffly accurate Rosenbrock-Wanner method with 4 internal stages with B_PR consistent of order 2 with Rinf=0. The order of convergence decreases if medium stiff problems are considered, but it has good results for very stiff cases.\nROS3PRL2 - A 3nd order stiffly accurate Rosenbrock-Wanner method with 4 internal stages with B_PR consistent of order 3. The order of convergence does NOT decreases if medium stiff problems are considered as it does for ROS3PRL.","category":"section"},{"location":"solvers/ode_solve/#Rosenbrock-W-Methods","page":"ODE Solvers","title":"Rosenbrock-W Methods","text":"Rosenbrock23 - An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\nRosenbrock32 - An Order 3/2 A-Stable Rosenbrock-W method which is good for mildly stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\nRodas23W - An Order 2/3 L-Stable Rosenbrock-W method for stiff ODEs and DAEs in mass matrix form. 2nd order stiff-aware interpolation and additional error test for interpolation.\nRosenbrockW6S4OS - A 4th order L-stable Rosenbrock-W method (fixed step only).\nROS34PW1a - A 4th order L-stable Rosenbrock-W method.\nROS34PW1b - A 4th order L-stable Rosenbrock-W method.\nROS34PW2 - A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\nROS34PW3 - A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.\nROS34PRw - A 3nd order stiffly accurate Rosenbrock-Wanner W-method with 4 internal stages with B_PR consistent of order 2\nROS2S - A 2nd order stiffly accurate Rosenbrock-Wanner W-method with 3 internal stages with B_PR consistent of order 2 with Rinf=0.","category":"section"},{"location":"solvers/ode_solve/#Stabilized-Explicit-Methods","page":"ODE Solvers","title":"Stabilized Explicit Methods","text":"ROCK2 - Second order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nROCK4 - Fourth order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nRKC - Second order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nSERK2 - Second order stabilized extrapolated Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nESERK5 - Fifth order stabilized extrapolated Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\n\nROCK methods offer a min_stages and max_stages functionality. SERK methods derive higher orders by Aitken-Neville algorithm. SERK2 is defaulted to Predictive control but has option of PI control.","category":"section"},{"location":"solvers/ode_solve/#Parallelized-Implicit-Extrapolation-Methods","page":"ODE Solvers","title":"Parallelized Implicit Extrapolation Methods","text":"The following are adaptive order, adaptive step size extrapolation methods:\n\nImplicitEulerExtrapolation - Extrapolation of implicit Euler method with Romberg sequence. Similar to Hairer's SEULEX.\nImplicitEulerBarycentricExtrapolation - Extrapolation of the implicit Euler method, using Barycentric coordinates to improve the stability of the method.\nImplicitDeuflhardExtrapolation - Midpoint extrapolation using Barycentric coordinates\nImplicitHairerWannerExtrapolation - Midpoint extrapolation using Barycentric coordinates, following Hairer's SODEX in the adaptivity behavior.\n\nThese methods have arguments for max_order, min_order, and init_order on the adaptive order algorithm. threading denotes whether to automatically multithread the f evaluations and J/W instantiations+factorizations, allowing for a high degree of within-method parallelism. We recommend switching to multi-threading when the system consists of more than ~ 150 ODES. The defaults are:\n\nmax_order=10\nmin_order=1 except for ImplicitHairerWannerExtrapolation it's 2.\ninit_order=5\nthreading=false\n\nAdditionally, the ImplicitDeuflhardExtrapolation and ImplicitHairerWannerExtrapolation methods have the additional argument:\n\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch. Default is :harmonic.\n\nTo override, utilize the keyword arguments. For example:\n\nalg = ImplicitDeuflhardExtrapolation(max_order = 7, min_order = 4, init_order = 4,\n    sequence = :bulirsch)\nsolve(prob, alg)\n\nNote that the order that is referred to is the extrapolation order. For ImplicitEulerExtrapolation this is the order of the method, for the others an extrapolation order of n gives an order 2(n+1) method.","category":"section"},{"location":"solvers/ode_solve/#Parallelized-DIRK-Methods","page":"ODE Solvers","title":"Parallelized DIRK Methods","text":"These methods parallelize the J/W instantiation and factorization, making them efficient on small highly stiff ODEs. Has an option threading=true to turn on/off multithreading.\n\nPDIRK44: a 4th order 2-processor DIRK method.","category":"section"},{"location":"solvers/ode_solve/#exp_RK","page":"ODE Solvers","title":"Exponential Runge-Kutta Methods","text":"These methods are all fixed timestepping only.\n\nLawsonEuler - First order exponential Euler scheme.\nNorsettEuler - First order exponential-RK scheme. Alias: ETD1.\nETD2 - Second order Exponential Time Differencing method (in development).\nETDRK2 - 2nd order exponential-RK scheme.\nETDRK3 - 3rd order exponential-RK scheme.\nETDRK4 - 4th order exponential-RK scheme.\nHochOst4 - 4th order exponential-RK scheme with stiff order 4.\n\nThe methods are intended for semilinear problems constructed by SplitODEProblem or SplitODEFunction. They can also be used for a general nonlinear problem, in which case the Jacobian of the right-hand side is used as the linear operator in each time step.\n\nExcept for ETD2, all methods come with these options, which can be set in the methods' constructor:\n\nkrylov - boolean, default: false. Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems. krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm - integer, default: 30. Controls the size of Krylov subspace.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\nautodiff and chunksize: autodiff control if problem is not semilinear and explicit Jacobian is not given. See Extra Options for more details.","category":"section"},{"location":"solvers/ode_solve/#Adaptive-Exponential-Rosenbrock-Methods","page":"ODE Solvers","title":"Adaptive Exponential Rosenbrock Methods","text":"Exprb32 - 3rd order adaptive Exponential-Rosenbrock scheme.\nExprb43 - 4th order adaptive Exponential-Rosenbrock scheme.\n\nThe exponential Rosenbrock methods cannot be applied to semilinear problems. Options for the solvers are the same as Exponential Runge-Kutta Methods, except that Krylov approximation is always used.","category":"section"},{"location":"solvers/ode_solve/#Exponential-Propagation-Iterative-Runge-Kutta-Methods-(EPIRK)","page":"ODE Solvers","title":"Exponential Propagation Iterative Runge-Kutta Methods (EPIRK)","text":"These methods are all fixed timestepping only.\n\nExp4 - 4th order EPIRK scheme.\nEPIRK4s3A - 4th order EPIRK scheme with stiff order 4.\nEPIRK4s3B - 4th order EPIRK scheme with stiff order 4.\nEPIRK5P1 - 5th order EPIRK scheme.\nEPIRK5P2 - 5th order EPIRK scheme.\nEPIRK5s3 - 5th order “horizontal” EPIRK scheme with stiff order 5. Broken.\nEXPRB53s3- 5th order EPIRK scheme with stiff order 5.\n\nOptions:\n\nadaptive_krylov - boolean, default: true. Determines if the adaptive Krylov algorithm with timestepping of Neisen & Wright is used.\nm - integer, default: 30. Controls the size of Krylov subspace, or the size for the first step if adaptive_krylov=true.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\nautodiff and chunksize: autodiff control if problem is not semilinear and explicit Jacobian is not given. See Extra Options for more details.\n\nIt should be noted that many of the methods are still at an experimental stage of development, and thus should be used with caution.","category":"section"},{"location":"solvers/ode_solve/#Multistep-Methods","page":"ODE Solvers","title":"Multistep Methods","text":"Quasi-constant stepping is the time stepping strategy which matches the classic GEAR, LSODE,  and ode15s integrators. The variable-coefficient methods match the ideas of the classic EPISODE integrator and early VODE designs. The Fixed Leading Coefficient (FLC) methods match the behavior of the classic VODE and Sundials CVODE integrator.\n\nQNDF1 - An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function (NDF) method. Optional parameter kappa defaults to Shampine's accuracy-optimal -0.1850.\nQBDF1 - An adaptive order 1 L-stable BDF method. This is equivalent to implicit Euler but using the BDF error estimator.\nABDF2 - An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\nQNDF2 - An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\nQBDF2 - An adaptive order 2 L-stable BDF method using quasi-constant timesteps.\nQNDF - An adaptive order quasi-constant timestep NDF method. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients). Similar to ode15s.\nQBDF - An adaptive order quasi-constant timestep BDF method.\nMEBDF2 - The second order Modified Extended BDF method, which has improved stability properties over the standard BDF. Fixed timestep only.\nFBDF - A fixed-leading coefficient adaptive-order adaptive-time BDF method, similar to ode15i or CVODE_BDF in divided differences form.","category":"section"},{"location":"solvers/ode_solve/#Implicit-Strong-Stability-Preserving-Runge-Kutta-Methods-for-Hyperbolic-PDEs-(Conservation-Laws)","page":"ODE Solvers","title":"Implicit Strong-Stability Preserving Runge-Kutta Methods for Hyperbolic PDEs (Conservation Laws)","text":"SSPSDIRK2 - A second order A-L stable symplectic SDIRK method with the strong stability preserving (SSP) property (SSP coefficient 2). Fixed timestep only.","category":"section"},{"location":"solvers/ode_solve/#extra_options_ode","page":"ODE Solvers","title":"Extra Options","text":"All the Rosenbrock and SDIRK methods allow for specification of linsolve: the linear solver which is used. For more information on specifying the linear solver, see the manual page on solver specification.\n\nNote that performance overload information (Jacobians etc.) are not used in this mode. This can control automatic differentiation of the Jacobian as well. For more information on specifying the nonlinear solver, see the manual page on solver specification.\n\nAdditionally, the Rosenbrock and SDIRK methods have differentiation controls. In each of these, autodiff can be set to turn on/off autodifferentiation, and chunk_size can be used to set the chunksize of the Dual numbers (see the documentation for ForwardDiff.jl for details). In addition, the Rosenbrock and SDIRK methods can set diff_type, which is the type of numerical differentiation that is used (when autodifferentiation is disabled). The choices are Val{:central}, Val{:forward} or Val{:complex}.\n\nExamples:\n\nsol = solve(prob, Rosenbrock23()) # Standard, uses autodiff\nsol = solve(prob, Rosenbrock23(autodiff = AutoForwardDiff(chunksize = 10))) # Autodiff with chunksize of 10\nsol = solve(prob, Rosenbrock23(autodiff = AutoFiniteDiff())) # Numerical differentiation with central differencing\nsol = solve(prob, Rosenbrock23(autodiff = AutoFiniteDiff(fdtype = Val{:forward}))) # Numerical differentiation with forward differencing","category":"section"},{"location":"solvers/ode_solve/#Tableau-Method","page":"ODE Solvers","title":"Tableau Method","text":"Additionally, there is the tableau method:\n\nExplicitRK - A general Runge-Kutta solver which takes in a tableau. Can be adaptive. Tableaus are specified via the keyword argument tab=tableau. The default tableau is for Dormand-Prince 4/5. Other supplied tableaus can be found in the Supplied Tableaus section.\n\nExample usage:\n\nalg = ExplicitRK(tableau = constructDormandPrince())\nsolve(prob, alg)","category":"section"},{"location":"solvers/ode_solve/#CompositeAlgorithm","page":"ODE Solvers","title":"CompositeAlgorithm","text":"One unique feature of OrdinaryDiffEq.jl is the CompositeAlgorithm, which allows you to, with very minimal overhead, design a multimethod which switches between chosen algorithms as needed. The syntax is CompositeAlgorithm(algtup,choice_function) where algtup is a tuple of OrdinaryDiffEq.jl algorithms, and choice_function is a function which declares which method to use in the following step. For example, we can design a multimethod which uses Tsit5() but switches to Vern7() whenever dt is too small:\n\nchoice_function(integrator) = (Int(integrator.dt < 0.001) + 1)\nalg_switch = CompositeAlgorithm((Tsit5(), Vern7()), choice_function)\n\nThe choice_function takes in an integrator and thus all of the features available in the Integrator Interface can be used in the choice function.\n\nA helper algorithm was created for building 2-method automatic switching for stiffness detection algorithms. This is the AutoSwitch algorithm with the following options:\n\nAutoSwitch(nonstiffalg::nAlg, stiffalg::sAlg;\n    maxstiffstep = 10, maxnonstiffstep = 3,\n    nonstifftol::T = 9 // 10, stifftol::T = 9 // 10,\n    dtfac = 2.0, stiffalgfirst = false)\n\nThe nonstiffalg must have an appropriate stiffness estimate built into the method. The stiffalg can receive its estimate from the Jacobian calculation. maxstiffstep is the number of stiffness detects before switching to the stiff algorithm and maxnonstiffstep is vice versa. nonstifftol and stifftol are the tolerances associated with the stiffness comparison against the stability region. Decreasing stifftol makes switching to the non-stiff algorithm less likely. Decreasing nonstifftol makes switching to the stiff algorithm more likely. dtfac is the factor that dt is changed when switching: multiplied when going from non-stiff to stiff and divided when going stiff to non-stiff. stiffalgfirst denotes whether the first step should use the stiff algorithm.","category":"section"},{"location":"solvers/ode_solve/#Pre-Built-Stiffness-Detecting-and-Auto-Switching-Algorithms","page":"ODE Solvers","title":"Pre-Built Stiffness Detecting and Auto-Switching Algorithms","text":"These methods require a Autoalg(stiffalg) to be chosen as the method to switch to when the ODE is stiff. It can be any of the OrdinaryDiffEq.jl one-step stiff methods and has all the arguments of the AutoSwitch algorithm.\n\nAutoTsit5 - Tsit5 with automated switching.\nAutoDP5 - DP5 with automated switching.\nAutoVern6 - Vern6 with automated switching.\nAutoVern7 - Vern7 with automated switching.\nAutoVern8 - Vern8 with automated switching.\nAutoVern9 - Vern9 with automated switching.\n\nExample:\n\ntsidas_alg = AutoTsit5(Rodas5())\nsol = solve(prob, tsidas_alg)\n\ntsidas_alg = AutoTsit5(Rodas5(), nonstifftol = 11 / 10)\n\nIs the Tsit5 method with automatic switching to Rodas5.","category":"section"},{"location":"solvers/ode_solve/#ode_solve_sundials","page":"ODE Solvers","title":"Sundials.jl","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl:\n\nimport Pkg\nPkg.add(\"Sundials\")\nimport Sundials\n\nThe Sundials suite is built around multistep methods. These methods are more efficient than other methods when the cost of the function calculations is really high, but for less costly functions the cost of nurturing the timestep overweighs the benefits. However, the BDF method is a classic method for stiff equations and “generally works”.\n\nCVODE_BDF - CVode Backward Differentiation Formula (BDF) solver.\nCVODE_Adams - CVode Adams-Moulton solver.\nARKODE - Explicit and ESDIRK Runge-Kutta methods of orders 2-8 depending on choice of options.\n\nThe Sundials algorithms all come with a 3rd order Hermite polynomial interpolation.\n\nFor more details on controlling the Sundials.jl solvers, see the Sundials detailed solver API page","category":"section"},{"location":"solvers/ode_solve/#ODEInterface.jl","page":"ODE Solvers","title":"ODEInterface.jl","text":"The ODEInterface algorithms are the classic Fortran algorithms. While the non-stiff algorithms are superseded by the more featured and higher performance Julia implementations from OrdinaryDiffEq.jl, the stiff solvers such as radau are some of the most efficient methods available (but are restricted for use on arrays of Float64).\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ODEInterfaceDiffEq.jl:\n\nimport Pkg\nPkg.add(\"ODEInterfaceDiffEq\")\nimport ODEInterfaceDiffEq\n\ndopri5 - Hairer's classic implementation of the Dormand-Prince 4/5 method.\ndop853 - Explicit Runge-Kutta 8(5,3) by Dormand-Prince.\nodex - GBS extrapolation-algorithm based on the midpoint rule.\nseulex - Extrapolation-algorithm based on the linear implicit Euler method.\nradau - Implicit Runge-Kutta (Radau IIA) of variable order between 5 and 13.\nradau5 - Implicit Runge-Kutta method (Radau IIA) of order 5.\nrodas - Rosenbrock 4(3) method.\nddeabm - Adams-Bashforth-Moulton Predictor-Corrector method (order between 1 and 12)\nddebdf - Backward Differentiation Formula (orders between 1 and 5)\n\nNote that while the output only has a linear interpolation, a higher order interpolation is used for intermediate dense output for saveat and for event handling.","category":"section"},{"location":"solvers/ode_solve/#LSODA.jl","page":"ODE Solvers","title":"LSODA.jl","text":"This setup provides a wrapper to the algorithm LSODA, a well-known method which uses switching to solve both stiff and non-stiff equations.\n\nlsoda - The LSODA wrapper algorithm.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use LSODA.jl:\n\nimport Pkg\nPkg.add(\"LSODA\")\nimport LSODA","category":"section"},{"location":"solvers/ode_solve/#IRKGaussLegendre.jl","page":"ODE Solvers","title":"IRKGaussLegendre.jl","text":"This setup provides a specific solver, IRKGL16, which is a 16th order Symplectic Gauss-Legendre scheme. This scheme is highly efficient for precise integration of ODEs, specifically ODEs derived from Hamiltonian systems.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use IRKGaussLegendre.jl:\n\nimport Pkg\nPkg.add(\"IRKGaussLegendre\")\nimport IRKGaussLegendre\n\nIRKGL16(;kwargs...) has the following arguments:\n\nsecondorderode (boolean):\n=false (default): for a ODEProblem type\n=true: for a second-order differential equation\nsimd (boolean):\n=true: SIMD-vectorized implementation only available for Float32 or Float64 computations\n=false (default):  generic implementation that can use with arbitrary Julia-defined number systems\nmstep: output saved at every 'mstep' steps. Default 1.\ninitial_extrapolation: initialization method for stages.\n=false: simplest initialization\n=true (default): extrapolation from the stage values of previous step\nmaxtrials: maximum number of attempts to accept adaptive step size\nthreading\n=false (default): sequential execution of the numerical integration\n=true: computations using threads (shared memory multi-threading) for stage-wise parallelization","category":"section"},{"location":"solvers/ode_solve/#SimpleDiffEq.jl","page":"ODE Solvers","title":"SimpleDiffEq.jl","text":"This setup provides access to simplified versions of a few ODE solvers. They mostly exist for experimentation, but offer shorter compile times. They have limitations compared to OrdinaryDiffEq.jl and are not generally faster.\n\nSimpleTsit5 - A fixed timestep integrator form of Tsit5. Not compatible with events.\nSimpleATsit5 - An adaptive Tsit5 with an interpolation in its simplest form. Not compatible with events.\nGPUSimpleATsit5 - A version of SimpleATsit5 without the integrator interface. Only allows solve.\nSimpleEuler - A fixed timestep bare-bones Euler implementation with integrators.\nLoopEuler - A fixed timestep bare-bones Euler. Not compatible with events or the integrator interface.\nGPUEuler - A fully static Euler for specialized compilation to accelerators like GPUs and TPUs.\nSimpleRK4 - A fixed timestep bare-bones RK4 implementation with integrators.\nLoopRK4 - A fixed timestep bare-bones RK4. Not compatible with events or the integrator interface.\nGPURK4 - A fully static RK4 for specialized compilation to accelerators like GPUs and TPUs.\nGPUVern7 - A fully static Vern7 for specialized compilation to accelerators like GPUs and TPUs.\nGPUVern9 - A fully static Vern9 for specialized compilation to accelerators like GPUs and TPUs.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use SimpleDiffEq.jl:\n\nimport Pkg\nPkg.add(\"SimpleDiffEq\")\nimport SimpleDiffEq","category":"section"},{"location":"solvers/ode_solve/#ODE.jl","page":"ODE Solvers","title":"ODE.jl","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ODE.jl:\n\nimport Pkg\nPkg.add(\"ODE\")\nimport ODE\n\node23 - Bogacki-Shampine's order 2/3 Runge-Kutta  method\node45 - A Dormand-Prince order 4/5 Runge-Kutta method\node23s - A modified Rosenbrock order 2/3 method due to Shampine\node78 - A Fehlburg order 7/8 Runge-Kutta method\node4 - The classic Runge-Kutta order 4 method\node4ms - A fixed-step, fixed order Adams-Bashforth-Moulton method†\node4s - A 4th order Rosenbrock method due to Shampine\n\n†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.","category":"section"},{"location":"solvers/ode_solve/#MATLABDiffEq.jl","page":"ODE Solvers","title":"MATLABDiffEq.jl","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use MATLABDiffEq.jl:\n\nimport Pkg\nPkg.add(url = \"https://github.com/SciML/MATLABDiffEq.jl\")\nimport MATLABDiffEq\n\nThis requires a licensed MATLAB installation. The available methods are:\n\nMATLABDiffEq.ode23\nMATLABDiffEq.ode45\nMATLABDiffEq.ode113\nMATLABDiffEq.ode23s\nMATLABDiffEq.ode23t\nMATLABDiffEq.ode23tb\nMATLABDiffEq.ode15s\nMATLABDiffEq.ode15i\n\nFor more information on these algorithms, see the MATLAB documentation.","category":"section"},{"location":"solvers/ode_solve/#SciPyDiffEq.jl","page":"ODE Solvers","title":"SciPyDiffEq.jl","text":"SciPyDiffEq.jl is a wrapper over SciPy for easing the transition of new users (same exact results!) and benchmarking. This wrapper uses Julia's JIT acceleration to accelerate about 3x over SciPy+Numba, but it is still around 1000x slower than the pure-Julia methods and thus should probably be used sparingly.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use SciPyDiffEq.jl:\n\nimport Pkg\nPkg.add(url = \"https://github.com/SciML/SciPyDiffEq.jl\")\nimport SciPyDiffEq\n\nThe available methods are:\n\nSciPyDiffEq.RK45\nSciPyDiffEq.RK23\nSciPyDiffEq.Radau\nSciPyDiffEq.BDF\nSciPyDiffEq.LSODA","category":"section"},{"location":"solvers/ode_solve/#deSolveDiffEq.jl","page":"ODE Solvers","title":"deSolveDiffEq.jl","text":"deSolveDiffEq.jl is a wrapper over R's deSolve for easing the transition of new users (same exact results!) and benchmarking. This wrapper is around 1000x slower than the pure-Julia methods (~2x-3x overhead from directly using R) and thus should probably be used sparingly.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use deSolveDiffEq.jl:\n\nimport Pkg\nPkg.add(url = \"https://github.com/SciML/deSolveDiffEq.jl\")\nimport deSolveDiffEq\n\nThe available methods are:\n\ndeSolveDiffEq.lsoda\ndeSolveDiffEq.lsode\ndeSolveDiffEq.lsodes\ndeSolveDiffEq.lsodar\ndeSolveDiffEq.vode\ndeSolveDiffEq.daspk\ndeSolveDiffEq.euler\ndeSolveDiffEq.rk4\ndeSolveDiffEq.ode23\ndeSolveDiffEq.ode45\ndeSolveDiffEq.radau\ndeSolveDiffEq.bdf\ndeSolveDiffEq.bdf_d\ndeSolveDiffEq.adams\ndeSolveDiffEq.impAdams\ndeSolveDiffEq.impAdams_d","category":"section"},{"location":"solvers/ode_solve/#GeometricIntegrators.jl","page":"ODE Solvers","title":"GeometricIntegrators.jl","text":"GeometricIntegrators.jl is a set of fixed timestep algorithms written in Julia. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use GeometricIntegratorsDiffEq.jl:\n\nimport Pkg\nPkg.add(url = \"https://github.com/SciML/GeometricIntegratorsDiffEq.jl\")\nimport GeometricIntegratorsDiffEq\n\nGIEuler - 1st order Euler method\nGIMidpoint - 2nd order explicit midpoint method\nGIHeun2 - 2nd order Heun's method\nGIRalston2 - 2nd order Ralston's method\nGIHeun3 - 3rd order Heun's method\nGIRalston3 - 3rd order Ralston's method\nGIRunge - 3rd order Kutta's method\nGIKutta - 3rd order Kutta's method\nGIRK4 - standard 4th order Runge-Kutta\nGIRK416\nGIRK438 - 4th order Runge-Kutta, 3/8's rule\nGIImplicitEuler - 1st order implicit Euler method\nGIImplicitMidpoint - 2nd order implicit midpoint method\nGIRadauIA(s) - s-stage Radau-IA\nGIRadauIIA(s) - s-stage Radau-IA\nGILobattoIIIA(s)\nGILobattoIIIB(s)\nGILobattoIIIC(s)\nGILobattoIIIC̄(s)\nGILobattoIIID(s)\nGILobattoIIIE(s)\nGILobattoIIIF(s)\nGISRK3 - 3-stage order 4 symmetric Runge-Kutta method\nGISSPRK3 - 3rd order explicit SSP method\n`GICrankNicholson\nGIKraaijevangerSpijker\nGIQinZhang\nGICrouzeix\nGIGLRK(s) - Gauss-Legendre Runge-Kutta method of order 2s\n\nNote that all these methods require the user supplies dt.","category":"section"},{"location":"solvers/ode_solve/#BridgeDiffEq.jl","page":"ODE Solvers","title":"BridgeDiffEq.jl","text":"Bridge.jl is a set of fixed timestep algorithms written in Julia. These methods are made and optimized for out-of-place functions on immutable (static vector) types. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use BridgeDiffEq.jl:\n\nimport Pkg\nPkg.add(url = \"https://github.com/SciML/BridgeDiffEq.jl\")\nimport BridgeDiffEq\n\nBridgeR3 - 3rd order Ralston method\nBridgeBS3 - 3rd order Bogacki-Shampine method","category":"section"},{"location":"solvers/ode_solve/#TaylorIntegration.jl","page":"ODE Solvers","title":"TaylorIntegration.jl","text":"TaylorIntegration.jl is a pure-Julia implementation of an adaptive order Taylor series method for high accuracy integration of ODEs. These methods are optimized when the absolute tolerance is required to be very low. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use TaylorIntegration.jl:\n\nimport Pkg\nPkg.add(\"TaylorIntegration\")\nimport TaylorIntegration\n\nTaylorMethod(order) - Taylor integration method with maximal order (required)\n\nNote: this method is much faster if you put @taylorize on your derivative function!","category":"section"},{"location":"solvers/ode_solve/#QuDiffEq.jl","page":"ODE Solvers","title":"QuDiffEq.jl","text":"QuDiffEq.jl is a package for solving differential equations using quantum algorithm. It makes use of the Yao framework for simulating quantum circuits.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use QuDiffEq.jl:\n\nimport Pkg\nPkg.add(url = \"https://github.com/QuantumBFS/QuDiffEq.jl\")\nimport QuDiffEq\n\nQuLDE(k) - Algorithm based on truncated Taylor series. The method linearizes a system of non-linear differential equations and solves the resultant by means of a quantum circuit. k selects the order in the Taylor series approximation (for the quantum circuit).\nQuNLDE(k,ϵ)- Algorithm uses forward Euler to solve quadratic differential equations. k selects the order in the Taylor series approximation (for the quantum circuit). ϵ sets the precision for Hamiltonian evolution.","category":"section"},{"location":"solvers/ode_solve/#NeuralPDE.jl","page":"ODE Solvers","title":"NeuralPDE.jl","text":"This method trains a neural network using Flux.jl to approximate the solution of the ODE. Currently, this method isn't competitive, but it is a fun curiosity that will be improved with future integration with Zygote.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use NeuralPDE.jl:\n\nimport Pkg\nPkg.add(\"NeuralPDE\")\nimport NeuralPDE\n\nnnode(chain,opt=ADAM(0.1)) - Defines a neural network solver which utilizes a Flux.jl chain under the hood, which must be supplied by the user. Defaults to using the ADAM optimization method, but the user can pass any Flux.jl optimizer.","category":"section"},{"location":"solvers/ode_solve/#List-of-Supplied-Tableaus","page":"ODE Solvers","title":"List of Supplied Tableaus","text":"A large variety of tableaus have been supplied by default, via DiffEqDevTools.jl. The list of tableaus can be found in the developer docs. To use them, note you must install the library:\n\nimport Pkg\nPkg.add(\"DiffEqDevTools\")\nimport DiffEqDevTools\n\nFor the most useful and common algorithms, a hand-optimized version is supplied in OrdinaryDiffEq.jl, which is recommended for general uses (i.e. use DP5 instead of ExplicitRK with tableau=constructDormandPrince()). However, these serve as a good method for comparing between tableaus and understanding the pros/cons of the methods. Implemented are every published tableau (that I know exists). Note that user-defined tableaus also are accepted. To see how to define a tableau, checkout the premade tableau source code. Tableau docstrings should have appropriate citations (if not, file an issue).\n\nPlot recipes are provided which will plot the stability region for a given tableau.","category":"section"},{"location":"solvers/ode_solve/#ProbNumDiffEq.jl","page":"ODE Solvers","title":"ProbNumDiffEq.jl","text":"ProbNumDiffEq.jl provides probabilistic numerical solvers for ODEs. By casting the solution of ODEs as a problem of Bayesian inference, they return a posterior probability distribution over ODE solutions and thereby provide estimates of their own numerical approximation error. The solvers have adaptive timestepping, their order can be freely specified, and the returned posterior distribution naturally enables dense output and sampling. The full documentation is available at ProbNumDiffEq.jl.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ProbNumDiffEq.jl:\n\nimport Pkg\nPkg.add(\"ProbNumDiffEq\")\nimport ProbNumDiffEq\n\nEK1(order=3) - A semi-implicit ODE solver based on extended Kalman filtering and smoothing with first order linearization. Recommended, but requires that the Jacobian of the vector field is specified.\nEK0(order=3) - An explicit ODE solver based on extended Kalman filtering and smoothing with zeroth order linearization.\n\n[1]: Koskela, A. (2015). Approximating the matrix exponential of an advection-diffusion operator using the incomplete orthogonalization method. In Numerical Mathematics and Advanced Applications-ENUMATH 2013 (pp. 345-353). Springer, Cham.","category":"section"},{"location":"solvers/steady_state_solve/#Steady-State-Solvers","page":"Steady State Solvers","title":"Steady State Solvers","text":"solve(prob::SteadyStateProblem,alg;kwargs)\n\nSolves for the steady states in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.\n\nnote: Note\nFor a more complete documentation on nonlinear solvers for steady state problems, see NonlinearSolve.jl","category":"section"},{"location":"solvers/steady_state_solve/#Recommended-Methods","page":"Steady State Solvers","title":"Recommended Methods","text":"DynamicSS is a good choice if you think you may have multiple steady states or a bad initial guess. SSRootfind can be faster if you have a good initial guess. For DynamicSS, in many cases an adaptive stiff solver, like a Rosenbrock or BDF method (Rodas5 or QNDF), is a good way to allow for very large time steps as the steady state approaches.\n\nNote that if you use CVODE_BDF you may need to give a starting dt via dt=.....","category":"section"},{"location":"solvers/steady_state_solve/#Full-List-of-Methods","page":"Steady State Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/steady_state_solve/#SteadyStateDiffEq.jl","page":"Steady State Solvers","title":"SteadyStateDiffEq.jl","text":"SSRootfind : Uses a rootfinding algorithm to find a steady state. Defaults to using NLsolve.jl. A different algorithm can be specified via the nlsolve keyword argument. (This method is deprecated: use NonlinearSolve.jl instead).\nDynamicSS : Uses an ODE solver to find the steady state. Automatically terminates when close to the steady state. DynamicSS(alg;abstol=1e-8,reltol=1e-6,tspan=Inf) requires that an ODE algorithm is given as the first argument.  The absolute and relative tolerances specify the termination conditions on the derivative's closeness to zero.  This internally uses the TerminateSteadyState callback from the Callback Library.  The simulated time for which the given ODE is solved can be limited by tspan.  If tspan is a number, it is equivalent to passing (zero(tspan), tspan).\n\nExample usage:\n\nimport SteadyStateDiffEq as SS, DifferentialEquations as DE\nimport Sundials\nsol = SS.solve(prob, SS.SSRootfind())\nsol = SS.solve(prob, SS.DynamicSS(DE.Tsit5()))\nsol = SS.solve(prob, SS.DynamicSS(Sundials.CVODE_BDF()), dt = 1.0)","category":"section"},{"location":"features/performance_overloads/#performance_overloads","page":"Jacobians, Gradients, etc.","title":"Jacobians, Gradients, etc.","text":"The DiffEq ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data. In traditional libraries, there is usually only one option: the Jacobian. However, we allow for a large array of pre-computed functions to speed up the calculations. This is offered via the DiffEqFunction types which can be passed to the problems.","category":"section"},{"location":"features/performance_overloads/#Built-In-Jacobian-Options","page":"Jacobians, Gradients, etc.","title":"Built-In Jacobian Options","text":"warning: Warning\nThis subsection on Jacobian options only applies to the Julia-based solvers (OrdinaryDiffEq.jl, StochasticDiffEq.jl, etc.). Wrappers of traditional C/Fortran codes, such as Sundials.jl, always default to finite differencing using the in-solver code.\n\nAll applicable stiff differential equation solvers in the Julia ecosystem (OrdinaryDiffEq.jl, StochasticDiffEq.jl, DelayDiffEq.jl, etc.) take the following arguments for handling the automatic Jacobian construction with the following defaults:\n\nchunk_size: The chunk size used with ForwardDiff.jl. Defaults to Val{0}() and thus uses the internal ForwardDiff.jl algorithm for the choice.\nautodiff: Specifies whether to use automatic differentiation via ForwardDiff.jl or finite differencing via FiniteDiff.jl. Defaults to Val{true}() for automatic differentiation.\nstandardtag: Specifies whether to use package-specific tags instead of the ForwardDiff default function-specific tags. For more information, see this blog post. Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to nothing, which means it will be chosen true/false depending on circumstances of the solver, such as whether a Krylov subspace method is used for linsolve.\ndiff_type: The type of differentiation used in FiniteDiff.jl if autodiff=false. Defaults to Val{:forward}, with alternatives of Val{:central} and Val{:complex}.","category":"section"},{"location":"features/performance_overloads/#Passing-Jacobian-Function-Definitions","page":"Jacobians, Gradients, etc.","title":"Passing Jacobian Function Definitions","text":"If one wishes to define a Jacobian function directly for use in the solver, then the defined method is passed to the AbstractSciMLFunction type associated with the DEProblem. For example, ODEProblem definitions have a spot for jac in the ODEFunction specification. For more information on how to define Jacobians for the specific problems, see the appropriate problem type page, for example, the ODE problem page.","category":"section"},{"location":"types/steady_state_types/#Steady-State-Problems","page":"Steady State Problems","title":"Steady State Problems","text":"note: Note\nFor a more complete documentation on nonlinear solvers for steady state problems, see NonlinearSolve.jl","category":"section"},{"location":"types/steady_state_types/#Solution-Type","page":"Steady State Problems","title":"Solution Type","text":"","category":"section"},{"location":"types/steady_state_types/#Alias-Specifier","page":"Steady State Problems","title":"Alias Specifier","text":"","category":"section"},{"location":"types/steady_state_types/#SciMLBase.SteadyStateProblem","page":"Steady State Problems","title":"SciMLBase.SteadyStateProblem","text":"Defines a steady state ODE problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/steady_state_types/\n\nMathematical Specification of a Steady State Problem\n\nTo define a Steady State Problem, you simply need to give the function f which defines the ODE:\n\nfracdudt = f(u p t)\n\nand an initial guess u_0 of where f(u, p, t) = 0. f should be specified as f(u, p, t) (or in-place as f(du, u, p, t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nNote that for the steady-state to be defined, we must have that f is autonomous, that is f is independent of t. But the form which matches the standard ODE solver should still be used. The steady state solvers interpret the f by fixing t = .\n\nProblem Type\n\nConstructors\n\nSteadyStateProblem(f::ODEFunction, u0, p = NullParameters(); kwargs...)\nSteadyStateProblem{isinplace, specialize}(f, u0, p = NullParameters(); kwargs...)\n\nisinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nParameters are optional, and if not given, a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nAdditionally, the constructor from ODEProblems is provided:\n\nSteadyStateProblem(prob::ODEProblem)\n\nParameters are optional, and if not given, a NullParameters() singleton will be used, which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The function in the ODE.\nu0: The initial guess for the steady state.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\nSpecial Solution Fields\n\nThe SteadyStateSolution type is different from the other DiffEq solutions because it does not have temporal information.\n\n\n\n\n\n","category":"type"},{"location":"types/steady_state_types/#SciMLBase.NonlinearSolution","page":"Steady State Problems","title":"SciMLBase.NonlinearSolution","text":"struct NonlinearSolution{T, N, uType, R, P, A, O, uType2, S, Tr} <: SciMLBase.AbstractNonlinearSolution{T, N}\n\nRepresentation of the solution to a nonlinear equation defined by a NonlinearProblem, or the steady state solution to a differential equation defined by a SteadyStateProblem.\n\nFields\n\nu: the representation of the nonlinear equation's solution.\nresid: the residual of the solution.\nprob: the original NonlinearProblem/SteadyStateProblem that was solved.\nalg: the algorithm type used by the solver.\noriginal: if the solver is wrapped from an alternative solver ecosystem, such as NLsolve.jl, then this is the original return from said solver library.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully or whether it exited due to an error. For more details, see the return code documentation.\nleft: if the solver is bracketing method, this is the final left bracket value.\nright: if the solver is bracketing method, this is the final right bracket value.\nstats: statistics of the solver, such as the number of function evaluations required.\n\n\n\n\n\n","category":"type"},{"location":"types/steady_state_types/#SciMLBase.SteadyStateAliasSpecifier","page":"Steady State Problems","title":"SciMLBase.SteadyStateAliasSpecifier","text":"SteadyStateAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = nothing, alias_du0 = nothing, alias_tstops = nothing, alias = nothing)\n\nHolds information on what variables to alias when solving a SteadyStateProblem. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false.\nalias_du0::Union{Bool, Nothing}: alias the du0 array for DAEs. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias::Union{Bool, Nothing}: sets all fields of the SteadStateAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#Split-Step-Methods-for-Fully-Stiff-Problems","page":"Split-Step Methods for Fully Stiff Problems","title":"Split-Step Methods for Fully Stiff Problems","text":"When both drift and diffusion terms are stiff, split-step methods treat both parts implicitly. These methods can handle the most challenging stiff SDE problems.","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#Split-Step-Implicit-Methods","page":"Split-Step Methods for Fully Stiff Problems","title":"Split-Step Implicit Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#ISSEM-Implicit-Split-Step-Euler-Maruyama","page":"Split-Step Methods for Fully Stiff Problems","title":"ISSEM - Implicit Split-Step Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#ISSEulerHeun-Implicit-Split-Step-Euler-Heun","page":"Split-Step Methods for Fully Stiff Problems","title":"ISSEulerHeun - Implicit Split-Step Euler-Heun","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#When-to-Use-Split-Step-Methods","page":"Split-Step Methods for Fully Stiff Problems","title":"When to Use Split-Step Methods","text":"Use ISSEM when:\n\nBoth drift and diffusion are stiff\nExplicit methods fail even with small time steps\nStandard implicit methods (ImplicitEM) are insufficient\nWorking with Itô interpretation\n\nUse ISSEulerHeun when:\n\nBoth drift and diffusion are stiff\nWorking with Stratonovich interpretation\nNeed implicit treatment of diffusion term","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#Understanding-Full-Stiffness","page":"Split-Step Methods for Fully Stiff Problems","title":"Understanding Full Stiffness","text":"Drift Stiffness: Large negative eigenvalues in f(u,t) Diffusion Stiffness: Large coefficients in g(u,t) causing instability\n\nDetection Signs:\n\nImplicitEM still requires very small time steps\nSolutions become unstable despite implicit drift treatment\nLarge diffusion coefficients cause numerical artifacts","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#Algorithm-Structure","page":"Split-Step Methods for Fully Stiff Problems","title":"Algorithm Structure","text":"Split-step methods solve:\n\ndu = f(u,t)dt + g(u,t)dW\n\nBy treating both terms implicitly through operator splitting or fully implicit schemes.","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#Performance-Considerations","page":"Split-Step Methods for Fully Stiff Problems","title":"Performance Considerations","text":"More expensive per step than drift-only implicit methods\nMay require smaller time steps than expected\nJacobian computations for both drift and diffusion\nNonlinear solve complexity increases","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#Configuration","page":"Split-Step Methods for Fully Stiff Problems","title":"Configuration","text":"Same options as other implicit methods:\n\nISSEM(\n    linsolve = KrylovJL_GMRES(),\n    nlsolve = NLNewton(),\n    theta = 1.0,\n    autodiff = true\n)","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#Alternative-Approaches","page":"Split-Step Methods for Fully Stiff Problems","title":"Alternative Approaches","text":"If split-step methods are too expensive:\n\nTry stabilized explicit methods (SROCK family)\nConsider method of lines for PDE problems\nUse shorter time intervals with restarts\nReformulate problem to reduce stiffness","category":"section"},{"location":"api/stochasticdiffeq/stiff/split_step_methods/#References","page":"Split-Step Methods for Fully Stiff Problems","title":"References","text":"Implicit-explicit splitting schemes for SDEs\nStochastic operator splitting methods","category":"section"},{"location":"api/stochasticdiffeq/misc/#Miscellaneous-Methods","page":"Miscellaneous Methods","title":"Miscellaneous Methods","text":"This page covers specialized methods for particular types of problems or applications.","category":"section"},{"location":"api/stochasticdiffeq/misc/#Composite-Algorithms","page":"Miscellaneous Methods","title":"Composite Algorithms","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#StochasticCompositeAlgorithm-Multi-Method-Solving","page":"Miscellaneous Methods","title":"StochasticCompositeAlgorithm - Multi-Method Solving","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#RODE-Methods-(Random-ODEs)","page":"Miscellaneous Methods","title":"RODE Methods (Random ODEs)","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#RandomEM-Random-Euler-Method","page":"Miscellaneous Methods","title":"RandomEM - Random Euler Method","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#RandomHeun-Random-Heun-Method","page":"Miscellaneous Methods","title":"RandomHeun - Random Heun Method","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#RandomTamedEM-Tamed-Random-Euler","page":"Miscellaneous Methods","title":"RandomTamedEM - Tamed Random Euler","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#Langevin-Dynamics","page":"Miscellaneous Methods","title":"Langevin Dynamics","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#BAOAB-Langevin-Integrator","page":"Miscellaneous Methods","title":"BAOAB - Langevin Integrator","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#Predictor-Corrector-Methods","page":"Miscellaneous Methods","title":"Predictor-Corrector Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#PCEuler-Predictor-Corrector-Euler","page":"Miscellaneous Methods","title":"PCEuler - Predictor-Corrector Euler","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#Integro-Integral-Form-(IIF)-Methods","page":"Miscellaneous Methods","title":"Integro-Integral-Form (IIF) Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#IIF1M,-IIF2M,-IIF1Mil-IIF-Methods","page":"Miscellaneous Methods","title":"IIF1M, IIF2M, IIF1Mil - IIF Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#Simplified-Methods","page":"Miscellaneous Methods","title":"Simplified Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#SimplifiedEM-Simplified-Euler-Maruyama","page":"Miscellaneous Methods","title":"SimplifiedEM - Simplified Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#When-to-Use-Miscellaneous-Methods","page":"Miscellaneous Methods","title":"When to Use Miscellaneous Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/misc/#StochasticCompositeAlgorithm:","page":"Miscellaneous Methods","title":"StochasticCompositeAlgorithm:","text":"When problem characteristics change during integration\nCombining methods for different regimes\nAutomatic method switching based on conditions","category":"section"},{"location":"api/stochasticdiffeq/misc/#RODE-Methods:","page":"Miscellaneous Methods","title":"RODE Methods:","text":"Random ordinary differential equations\nProblems with random parameters but no Brownian motion\nUncertainty quantification applications","category":"section"},{"location":"api/stochasticdiffeq/misc/#BAOAB:","page":"Miscellaneous Methods","title":"BAOAB:","text":"Molecular dynamics simulations\nLangevin equations with specific structure\nWhen preserving equilibrium distributions is important","category":"section"},{"location":"api/stochasticdiffeq/misc/#IIF-Methods:","page":"Miscellaneous Methods","title":"IIF Methods:","text":"Semi-linear problems with stiff linear parts\nProblems amenable to integrating factor techniques\nWhen exponential integrators are appropriate","category":"section"},{"location":"api/stochasticdiffeq/misc/#PCEuler:","page":"Miscellaneous Methods","title":"PCEuler:","text":"Problems requiring specific drift-diffusion coupling\nWhen analytical ggprime function is available\nSpecialized predictor-corrector applications\n\nThese methods serve specific niches in stochastic computation and may be optimal for particular problem structures.","category":"section"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.StochasticCompositeAlgorithm","page":"Miscellaneous Methods","title":"StochasticDiffEq.StochasticCompositeAlgorithm","text":"StochasticCompositeAlgorithm(algs, choice_function)\n\nStochasticCompositeAlgorithm: Multi-Method Composite Algorithm\n\nComposite algorithm that automatically switches between multiple SDE solvers based on problem characteristics.\n\nMethod Properties\n\nApproach: Multi-method solving with automatic switching\nAdaptivity: Changes methods during integration\nFlexibility: Combines strengths of different algorithms\n\nParameters\n\nalgs::Tuple: Tuple of algorithms to switch between\nchoice_function::Function: Function determining which algorithm to use\n\nWhen to Use\n\nProblems with changing characteristics during integration\nWhen different regions require different solution approaches\nCombining methods for different regimes (e.g., stiff/nonstiff)\nWhen no single method is optimal for entire domain\n\nChoice Function\n\nThe choice_function(integrator) should return an integer indicating which algorithm from algs to use:\n\nfunction choice_function(integrator)\n    if stiff_region(integrator.u, integrator.t)\n        return 1  # Use first algorithm (e.g., implicit)\n    else\n        return 2  # Use second algorithm (e.g., explicit)\n    end\nend\n\nAlgorithm Features\n\nAutomatic method switching during integration\nMaintains continuity across method transitions\nCombines computational efficiency with robustness\nCan handle complex multi-scale problems\n\nReferences\n\nComposite algorithm methodology for SDEs\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.RandomEM","page":"Miscellaneous Methods","title":"StochasticDiffEq.RandomEM","text":"RandomEM()\n\nRandomEM: Random Euler Method (RODE)\n\nEuler method for Random Ordinary Differential Equations (RODEs) with random parameters.\n\nMethod Properties\n\nProblem type: Random ODEs (RODEs)\nStrong Order: 1.0 (for deterministic part)\nRandomness: Handles random parameters, not Brownian motion\nTime stepping: Fixed step size\n\nWhen to Use\n\nRandom ODEs with random parameters but no Brownian motion\nUncertainty quantification with parameter randomness\nProblems with random coefficients or initial conditions\nMonte Carlo simulation of deterministic systems with random inputs\n\nRODE vs SDE\n\nRODE: Random parameters, deterministic evolution\nSDE: Fixed parameters, stochastic (Brownian) evolution\n\nReferences\n\nRandom ordinary differential equation methods\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.RandomHeun","page":"Miscellaneous Methods","title":"StochasticDiffEq.RandomHeun","text":"RandomHeun()\n\nRandomHeun: Random Heun Method (RODE)\n\nHeun method for Random Ordinary Differential Equations with improved accuracy.\n\nMethod Properties\n\nProblem type: Random ODEs (RODEs)\nStrong Order: 2.0 (for deterministic part)\nRandomness: Handles random parameters\nTime stepping: Fixed step size\n\nWhen to Use\n\nRODEs requiring higher accuracy than RandomEM\nWhen computational cost per step is acceptable\nRandom parameter problems needing second-order accuracy\n\nReferences\n\nHigher-order methods for random ODEs\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.RandomTamedEM","page":"Miscellaneous Methods","title":"StochasticDiffEq.RandomTamedEM","text":"RandomTamedEM()\n\nRandomTamedEM: Tamed Random Euler Method (RODE)\n\nTamed Euler method for RODEs with potentially explosive behavior.\n\nMethod Properties\n\nProblem type: Random ODEs with potential blow-up\nApproach: Taming to prevent numerical explosion\nStability: Enhanced stability for unstable random systems\nTime stepping: Fixed step size with taming\n\nWhen to Use\n\nRODEs that may exhibit explosive growth\nWhen RandomEM gives unstable or explosive solutions\nRandom systems with strong nonlinearities\nProblems requiring enhanced numerical stability\n\nTaming Mechanism\n\nApplies taming technique to prevent numerical blow-up while maintaining accuracy for well-behaved solutions.\n\nReferences\n\nTamed methods for random differential equations\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.BAOAB","page":"Miscellaneous Methods","title":"StochasticDiffEq.BAOAB","text":"BAOAB(;gamma=1.0, scale_noise=true)\n\nBAOAB: Langevin Dynamics Integrator (Specialized)\n\nSpecialized integrator for Langevin dynamics in molecular dynamics simulations, particularly effective for configurational sampling.\n\nMethod Properties\n\nProblem type: Langevin dynamics (second-order SDEs)\nStructure: Position-velocity formulation\nSampling: Designed for equilibrium sampling\nTime stepping: Fixed step size\nConservation: Preserves equilibrium distributions\n\nParameters\n\ngamma::Real = 1.0: Friction coefficient\nscale_noise::Bool = true: Whether to scale noise appropriately\n\nSystem Structure\n\nDesigned for Langevin systems:\n\ndu = v dt\ndv = f(v,u) dt - γv dt + g(u)√(2γ) dW\n\nwhere:\n\nu: position coordinates\nv: velocity coordinates\nγ: friction coefficient\nf(v,u): force function\ng(u): noise scaling function\n\nWhen to Use\n\nMolecular dynamics simulations with Langevin thermostat\nConfigurational sampling of molecular systems\nEquilibrium sampling from canonical ensemble\nSecond-order SDEs with damping and noise\n\nAlgorithm Features\n\nBAOAB splitting: B(kick) - A(drift) - O(Ornstein-Uhlenbeck) - A(drift) - B(kick)\nPreserves correct equilibrium distribution\nRobust and efficient for molecular sampling\nWell-suited for long-time integration\n\nReferences\n\nLeimkuhler B., Matthews C., \"Robust and efficient configurational molecular sampling via Langevin dynamics\", J. Chem. Phys. 138, 174102 (2013)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.PCEuler","page":"Miscellaneous Methods","title":"StochasticDiffEq.PCEuler","text":"PCEuler(ggprime; theta=1/2, eta=1/2)\n\nPCEuler: Predictor-Corrector Euler Method (Nonstiff)\n\nA predictor-corrector variant of the Euler-Maruyama method requiring analytic derivatives of the diffusion term, with adjustable implicitness parameters for drift-diffusion coupling.\n\nMethod Properties\n\nStrong Order: 0.5 (in the Itô sense)\nWeak Order: 1.0\nTime stepping: Fixed time step only\nNoise types: General noise with available derivative information\nSDE interpretation: Itô only\n\nParameters\n\nggprime::Function: The required derivative of the diffusion term\nFor scalar problems: ggprime = g * ∂g/∂x\nFor multi-dimensional problems: ggprime_k = Σ_{j=1...M, i=1...D} g^(j)_i * ∂g^(j)_k/∂x_i\nwhere g^(j) corresponds to the noise vector due to the j-th noise channel\nMust match the in-place/out-of-place specification of the problem\ntheta::Real = 0.5: Degree of implicitness in the drift term (default: 0.5)\neta::Real = 0.5: Degree of implicitness in the diffusion term (default: 0.5)\n\nWhen to Use\n\nProblems requiring specific drift-diffusion coupling\nWhen analytical ggprime function is available\nSpecialized predictor-corrector applications\nWhen the derivative g*g' provides stability or accuracy benefits\n\nAlgorithm Description\n\nThe method uses a predictor-corrector approach with the specific requirement of computing the derivative of the diffusion coefficient. This additional information allows for improved handling of drift-diffusion interactions through the adjustable parameters θ and η.\n\nLimitations\n\nRequires analytical computation of ggprime (cannot be approximated)\nFixed time step only (no adaptive versions available)\nLimited to Itô interpretation\n\nReferences\n\nJentzen, A., Kloeden, P.E., \"The numerical approximation of stochastic partial differential equations\", Milan J. Math. 77, 205–244 (2009). https://doi.org/10.1007/s00032-009-0100-0\nOriginally introduced in PR #88 (commit 42e2510) by Tatsuhiro Onodera (2018)\n\nwarning: Warning\nThe derivative ggprime must be computed analytically for correctness.# Rossler The original paper contains a typo in the definition of ggprime - this implementation follows the corrected formulation.\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.IIF1M","page":"Miscellaneous Methods","title":"StochasticDiffEq.IIF1M","text":"IIF1M(;nlsolve=NLSOLVEJL_SETUP())\n\nIIF1M: Integrating Factor Method 1 (Semi-Linear)\n\nFirst-order integrating factor method for semi-linear SDEs with stiff linear parts.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 1.0\nTime stepping: Fixed or adaptive\nProblem type: Semi-linear SDEs with stiff linear components\nTreatment: Exponential integrator approach\n\nParameters\n\nnlsolve: Nonlinear solver configuration\n\nWhen to Use\n\nSemi-linear SDEs: du = (L*u + N(u))dt + g(u)dW where L is stiff linear operator\nProblems amenable to integrating factor techniques\nWhen exponential integrators are appropriate\nStiff linear parts with nonlinear perturbations\n\nAlgorithm Description\n\nApplies integrating factor exp(L*t) to handle stiff linear part exactly while treating nonlinear parts numerically.\n\nReferences\n\nIntegrating factor methods for stiff SDEs\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.IIF2M","page":"Miscellaneous Methods","title":"StochasticDiffEq.IIF2M","text":"IIF2M(;nlsolve=NLSOLVEJL_SETUP())\n\nIIF2M: Integrating Factor Method 2 (Semi-Linear)\n\nSecond-order integrating factor method for semi-linear SDEs.\n\nMethod Properties\n\nStrong Order: 2.0\nWeak Order: 2.0\nTime stepping: Fixed or adaptive\nProblem type: Semi-linear SDEs with stiff linear components\nTreatment: Higher-order exponential integrator\n\nParameters\n\nnlsolve: Nonlinear solver configuration\n\nWhen to Use\n\nWhen higher accuracy than IIF1M is needed\nSemi-linear problems requiring second-order convergence\nMore expensive but more accurate than IIF1M\n\nReferences\n\nHigher-order integrating factor methods for SDEs\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.IIF1Mil","page":"Miscellaneous Methods","title":"StochasticDiffEq.IIF1Mil","text":"IIF1Mil(;nlsolve=NLSOLVEJL_SETUP())\n\nIIF1Mil: Integrating Factor Milstein Method (Semi-Linear)\n\nIntegrating factor method combined with Milstein correction for semi-linear SDEs.\n\nMethod Properties\n\nStrong Order: 1.0 (with Milstein correction)\nWeak Order: 1.0\nTime stepping: Fixed or adaptive\nProblem type: Semi-linear SDEs with stiff linear components\nTreatment: Exponential integrator with Milstein correction\n\nParameters\n\nnlsolve: Nonlinear solver configuration\n\nWhen to Use\n\nSemi-linear SDEs requiring Milstein-type accuracy\nWhen both stiff linear treatment and higher-order stochastic accuracy are needed\nAlternative to IIF1M with enhanced stochastic treatment\n\nReferences\n\nIntegrating factor methods with Milstein correction\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/misc/#StochasticDiffEq.SimplifiedEM","page":"Miscellaneous Methods","title":"StochasticDiffEq.SimplifiedEM","text":"Kloeden, P.E., Platen, E., Numerical Solution of Stochastic Differential Equations. Springer. Berlin Heidelberg (2011)\n\nSimplifiedEM()\n\nSimplifiedEM: Simplified Euler-Maruyama Method (High Weak Order)\n\nSimplified version of the Euler-Maruyama method optimized for weak convergence with reduced computational cost.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 1.0\nTime stepping: Fixed step size\nNoise types: All forms (diagonal, non-diagonal, scalar, and colored noise)\nSDE interpretation: Itô\nComputational cost: Reduced compared to standard EM\n\nWhen to Use\n\nMonte Carlo simulations where weak convergence is sufficient\nProblems where computational efficiency is more important than strong accuracy\nLarge ensemble simulations\nWhen only statistical properties (expectations, moments) are needed\n\nAlgorithm Features\n\nSimplified implementation reducing computational overhead\nMaintains weak order 1.0 convergence\nMore efficient than standard EM for weak convergence applications\nHandles general noise structures\n\nWeak vs Strong Convergence\n\nOptimized for E[f(XT)] convergence, not pathwise |XT - X_h| convergence\nIdeal for computing expectations and statistical properties\nLess suitable when individual trajectory accuracy is important\n\nReferences\n\nKloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\", Springer. Berlin Heidelberg (2011)\n\n\n\n\n\n","category":"type"},{"location":"tutorials/jump_diffusion/#Jump-Diffusion-Equations","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"<meta http-equiv=\"Refresh\" content=\"0; url='https://docs.sciml.ai/JumpProcesses/stable/tutorials/jump_diffusion/'\" />","category":"section"},{"location":"types/bvp_types/#BVP-Problems","page":"BVP Problems","title":"BVP Problems","text":"","category":"section"},{"location":"types/bvp_types/#Solution-Type","page":"BVP Problems","title":"Solution Type","text":"BVProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"section"},{"location":"types/bvp_types/#Alias-Specifier","page":"BVP Problems","title":"Alias Specifier","text":"","category":"section"},{"location":"types/bvp_types/#Example-Problems","page":"BVP Problems","title":"Example Problems","text":"Example problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_bvp_linear_1, you can do something like:\n\n#] add DiffEqProblemLibrary\nimport DiffEqProblemLibrary.BVProblemLibrary\nimport BoundaryValueDiffEq as BVP\n# load problems\nprob = BVProblemLibrary.prob_bvp_linear_1\nsol = solve(prob, BVP.MIRK4(), dt = 0.05)","category":"section"},{"location":"types/bvp_types/#Linear-BVPs","page":"BVP Problems","title":"Linear BVPs","text":"","category":"section"},{"location":"types/bvp_types/#Nonlinear-BVPs","page":"BVP Problems","title":"Nonlinear BVPs","text":"","category":"section"},{"location":"types/bvp_types/#Regular-Nonlinear-BVPs","page":"BVP Problems","title":"Regular Nonlinear BVPs","text":"","category":"section"},{"location":"types/bvp_types/#SciMLBase.BVProblem","page":"BVP Problems","title":"SciMLBase.BVProblem","text":"Defines an BVP problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/bvp_types/\n\nMathematical Specification of a BVP Problem\n\nTo define a BVP Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nfracdudt = f(upt)\n\nalong with an implicit function bc which defines the residual equation, where\n\nbc(upt) = 0\n\nis the manifold on which the solution must live. A common form for this is the two-point BVProblem where the manifold defines the solution at two points:\n\nbeginalign*\nu(t_0) = a \nu(t_f) = b\nendalign*\n\nProblem Type\n\nConstructors\n\nTwoPointBVProblem{isinplace}(f, bc, u0, tspan, p=NullParameters(); kwargs...)\nBVProblem{isinplace}(f, bc, u0, tspan, p=NullParameters(); kwargs...)\n\nor if we have an initial guess function initialGuess(p, t) for the given BVP, we can pass the initial guess to the problem constructors:\n\nTwoPointBVProblem{isinplace}(f, bc, initialGuess, tspan, p=NullParameters(); kwargs...)\nBVProblem{isinplace}(f, bc, initialGuess, tspan, p=NullParameters(); kwargs...)\n\nFor any BVP problem type, bc must be inplace if f is inplace. Otherwise it must be out-of-place.\n\nIf the bvp is a StandardBVProblem (also known as a Multi-Point BV Problem) it must define either of the following functions\n\nbc!(residual, u, p, t)\nresidual = bc(u, p, t)\n\nwhere residual computed from the current u. u is an array of solution values where u[i] is at time t[i], while p are the parameters. For a TwoPointBVProblem, t = tspan. For the more general BVProblem, u can be all of the internal time points, and for shooting type methods u=sol the ODE solution. Note that all features of the ODESolution are present in this form. In both cases, the size of the residual matches the size of the initial condition.\n\nIf the bvp is a TwoPointBVProblem then bc must be a Tuple (bca, bcb) and each of them must define either of the following sets of functions:\n\nbca!(resid_a, u_a, p)\nbcb!(resid_b, u_b, p)\n\nor\n\nresid_a = bca(u_a, p)\nresid_b = bcb(u_b, p)\n\nwhere resid_a and resid_b are the residuals at the two endpoints, u_a and u_b are the solution values at the two endpoints, and p are the parameters.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFields\n\nf: The function for the ODE.\nbc: The boundary condition function.\nu0: The initial condition. Either the initial condition for the ODE as an initial value problem, or a Vector of values for u(t_i) for collocation methods.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters.\nlb: The lower bounds for the solution variables. Defaults to nothing.\nub: The upper bounds for the solution variables. Defaults to nothing.\nlcons: The lower bounds for the constraint residuals. Defaults to nothing.\nucons: The upper bounds for the constraint residuals. Defaults to nothing.\nproblem_type: The type of the problem, either StandardBVProblem or TwoPointBVProblem.\nkwargs: The keyword arguments passed onto the solves.\n\nSpecial Keyword Arguments\n\nnlls: Specify that the BVP is a nonlinear least squares problem. Use Val(true) or Val(false) for type stability. By default this is automatically inferred based on the size of the input and outputs, however this is type unstable for any array type that doesn't store array size as part of type information. If we can't reliably infer this, we set it to Nothing. Downstreams solvers must be setup to deal with this case.\n\n\n\n\n\n","category":"type"},{"location":"types/bvp_types/#SciMLBase.SecondOrderBVProblem","page":"BVP Problems","title":"SciMLBase.SecondOrderBVProblem","text":"Defines a second order BVP problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/bvp_types/\n\nMathematical Specification of a second order BVP Problem\n\nTo define a second order BVP Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nu = fracd^2 udt^2 = f(u u p t)\n\nalong with an implicit function bc which defines the residual equation, where\n\nbc(u u p t) = 0\n\nis the manifold on which the solution must live. A common form for this is the two-point SecondOrderBVProblem where the manifold defines the solution at two points:\n\nbeginalign*\ng(u(t_0) u(t_0)) = 0 \ng(u(t_f) u(t_f)) = 0\nendalign*\n\nProblem Type\n\nConstructors\n\nTwoPointSecondOrderBVProblem{isinplace}(f, bc, u0, tspan, p=NullParameters(); kwargs...)\nSecondOrderBVProblem{isinplace}(f, bc, u0, tspan, p=NullParameters(); kwargs...)\n\nor if we have an initial guess function initialGuess(p, t) for the given BVP, we can pass the initial guess to the problem constructors:\n\nTwoPointSecondOrderBVProblem{isinplace}(f, bc, initialGuess, tspan, p=NullParameters(); kwargs...)\nSecondOrderBVProblem{isinplace}(f, bc, initialGuess, tspan, p=NullParameters(); kwargs...)\n\nFor any BVP problem type, bc must be inplace if f is inplace. Otherwise it must be out-of-place.\n\nIf the bvp is a StandardSecondOrderBVProblem (also known as a Multi-Point BV Problem) it must define either of the following functions\n\nbc!(residual, du, u, p, t)\nresidual = bc(du, u, p, t)\n\nwhere residual computed from the current u. u is an array of solution values where u[i] is at time t[i], while p are the parameters. For a TwoPointBVProblem, t = tspan. For the more general BVProblem, u can be all of the internal time points, and for shooting type methods u=sol the ODE solution. Note that all features of the ODESolution are present in this form. In both cases, the size of the residual matches the size of the initial condition.\n\nIf the bvp is a TwoPointSecondOrderBVProblem then bc must be a Tuple (bca, bcb) and each of them must define either of the following sets of functions:\n\nbca!(resid_a, du_a, u_a, p)\nbcb!(resid_b, du_b, u_b, p)\n\nor\n\nresid_a = bca(du_a, u_a, p)\nresid_b = bcb(du_b, u_b, p)\n\nwhere resid_a and resid_b are the residuals at the two endpoints, u_a and u_b are the solution values at the two endpoints, du_a and du_b are the derivative of solution values at the two endpoints, and p are the parameters.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFields\n\nf: The function for the ODE.\nbc: The boundary condition function.\nu0: The initial condition. Either the initial condition for the ODE as an initial value problem, or a Vector of values for u(t_i) for collocation methods.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters.\nlb: The lower bounds for the solution variables. Defaults to nothing.\nub: The upper bounds for the solution variables. Defaults to nothing.\nlcons: The lower bounds for the constraint residuals. Defaults to nothing.\nucons: The upper bounds for the constraint residuals. Defaults to nothing.\nproblem_type: The type of the problem, either StandardSecondOrderBVProblem or TwoPointSecondOrderBVProblem.\nkwargs: The keyword arguments passed onto the solves.\n\n\n\n\n\n","category":"type"},{"location":"types/bvp_types/#SciMLBase.BVPFunction","page":"BVP Problems","title":"SciMLBase.BVPFunction","text":"struct BVPFunction{iip, specialize, twopoint, F, BF, C, EC, IC, FP, TMM, Ta, Tt, TJ, BCTJ, JVP, VJP, JP, BCJP, BCRP, SP, TW, TWt, TPJ, O, TCV, BCTCV, SYS, ID} <: SciMLBase.AbstractBVPFunction{iip, twopoint}\n\nA representation of a BVP function f, defined by:\n\nfracdudt = f(u p t)\n\nand the constraints:\n\ng(u p t) = 0\n\nIf the size of g(u, p, t) is different from the size of u, then the constraints are interpreted as a least squares problem, i.e. the objective function is:\n\nmin_u  g_i(u p t) ^2\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nBVPFunction{iip, specialize}(f, bc;\n    cost = __has_cost(f) ? f.cost : nothing,\n    equality = __has_equality(f) ? f.equality : nothing,\n    inequality = __has_inequality(f) ? f.inequality : nothing,\n    f_prototype = __has_f_prototype(f) ? f.f_prototype : nothing,\n    mass_matrix = __has_mass_matrix(f) ? f.mass_matrix : I,\n    analytic = __has_analytic(f) ? f.analytic : nothing,\n    tgrad= __has_tgrad(f) ? f.tgrad : nothing,\n    jac = __has_jac(f) ? f.jac : nothing,\n    bcjac = __has_jac(bc) ? bc.jac : nothing,\n    jvp = __has_jvp(f) ? f.jvp : nothing,\n    vjp = __has_vjp(f) ? f.vjp : nothing,\n    jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n    bcjac_prototype = __has_jac_prototype(bc) ? bc.jac_prototype : nothing,\n    sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n    paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n    syms = nothing,\n    indepsym= nothing,\n    paramsyms = nothing,\n    colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n    bccolorvec = __has_colorvec(f) ? bc.colorvec : nothing,\n    sys = __has_sys(f) ? f.sys : nothing,\n    twopoint::Union{Val, Bool} = Val(false))\n\nNote that both the function f and boundary condition bc are required. f should be given as f(du,u,p,t) or out = f(u,p,t). bc should be given as bc(res, u, p, t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f and bc. These include:\n\ncost(u, p): the target to be minimized, similar with the cost function in OptimizationFunction. This is used to define the objective function of the BVP, which can be minimized by optimization solvers.\nequality(res, u, t): equality constraints functions for the BVP.\ninequality(res, u, t): inequality constraints functions for the BVP.\nf_prototype: a prototype matrix matching the type of the ODE/DAE variables in an optimal control problem. For example, in the ODE/DAE that describe the dynamics of the optiml control problem, f_prototype should match the type and size of the ODE/DAE variables in f.\nmass_matrix: the mass matrix M represented in the BVP function. Can be used to determine that the equation is actually a BVP for differential algebraic equation (DAE) if M is singular.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the BVP. Generally only used for testing and development of the solvers.\ntgrad(dT,u,h,p,t) or dT=tgrad(u,p,t): returns fracf(upt)t\njac(J,du,u,p,gamma,t) or J=jac(du,u,p,gamma,t): returns fracdfdu\nbcjac(J,du,u,p,gamma,t) or J=jac(du,u,p,gamma,t): returns fracdbcdu\njvp(Jv,v,du,u,p,gamma,t) or Jv=jvp(v,du,u,p,gamma,t): returns the directional derivative fracdfdu v\nvjp(Jv,v,du,u,p,gamma,t) or Jv=vjp(v,du,u,p,gamma,t): returns the adjoint derivative fracdfdu^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nbcjac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\nbccolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the bcjac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\nAdditional Options:\n\ntwopoint: Specify that the BVP is a two-point boundary value problem. Use Val(true) or Val(false) for type stability.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the BVPFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"types/bvp_types/#SciMLBase.BVPAliasSpecifier","page":"BVP Problems","title":"SciMLBase.BVPAliasSpecifier","text":"BVPAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = nothing, alias_du0 = nothing, alias_tstops = nothing, alias = nothing)\n\nHolds information on what variables to alias when solving an BVP. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false .\nalias_du0::Union{Bool, Nothing}: alias the du0 array for DAEs. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias::Union{Bool, Nothing}: sets all fields of the BVPAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_1","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_1","text":"prob_bvp_linear_1\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 2pt\nfracdy_2dt = frac1λ y_1\nendalign*\n\nwith boundary condition\n\ny_1(0)=1  y_1(1)=0\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = fracexp(-tsqrtλ) - exp((t-2)sqrtλ)1-exp(-2sqrtλ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_2","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_2","text":"prob_bvp_linear_2\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ y_2\nendalign*\n\nwith boundary condition\n\ny_1(0)=1  y_1(1)=0\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = frac1-exp((t-1)λ)1-exp(-1λ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_3","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_3","text":"prob_bvp_linear_3\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(t y_1 y_2)\nendalign*\n\nwhere\n\nf(t y_1 y_2) = -2+cos(πt)y_2 + y_1 -(1+λπ^2)cos(πt) - 2+cos(πt)πsin(πt)\n\nwith boundary condition\n\ny_1(-1)=-1 y_1(1)=-1\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_4","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_4","text":"prob_bvp_linear_4\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(y_1 y_2)\nendalign*\n\nwhere\n\nf(y_1 y_2) = -y_2 + (1+λ) y_1\n\nwith boundary condition\n\nbeginalign*\ny_1(-1) = 1 + exp(-2) \ny_1(1)  = 1 + exp(-2(1+λ))\nendalign*\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = exp(t-1) + exp(-(1+λ)(1+t)λ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_5","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_5","text":"prob_bvp_linear_5\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(t y_1 y_2)\nendalign*\n\nwhere\n\nf(t y_1 y_2) = ty_2 + y_1 - (1+λπ^2)cos(πt) + πtsin(πt)\n\nwith boundary condition\n\ny_1(-1)=-1  y_1(1)=-1\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_6","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_6","text":"prob_bvp_linear_6\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(t y_2)\nendalign*\n\nwhere\n\nf(t y_2) = ty_2 - λπ^2cos(πt)-πtsin(πt)\n\nwith boundary condition\n\ny_1(-1)=-2  y_1(1)=0\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) + operatornameerf(tsqrt2λ)operatornameerf(1sqrt2λ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_7","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_7","text":"prob_bvp_linear_7\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(t y_1 y_2)\nendalign*\n\nwhere\n\nf(t y_1 y_2) = t y_2 + y_1 - (1+λπ^2)cos(πt) + π tsin(πt)\n\nwith boundary condition\n\ny_1(-1)=-1  y_1(1)=1\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) + t + fractoperatornameerf(tsqrt2λ) + sqrt2λπexp(-t^22λ)operatornameerf(tsqrt2λ) + sqrt2λπexp(-12λ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_8","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_8","text":"prob_bvp_linear_8\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ y_2\nendalign*\n\nwith boundary condition\n\ny_1(0)=1  y_1(1)=2\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = frac2 - exp(-1λ) - exp(-tλ)1 - exp(-1λ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_9","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_9","text":"prob_bvp_linear_9\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ+t^2 f(t y_1 y_2)\nendalign*\n\nwhere\n\nf(t y_1 y_2)=-4ty_2 - 2y_1\n\nwith boundary condition\n\ny_1(-1)=1(1+λ) y_1(1)=1(1+λ)\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = frac1λ + t^2 \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_10","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_10","text":"prob_bvp_linear_10\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = - frac1λ t y_2\nendalign*\n\nwith boundary condition\n\ny_1(-1)=0  y_1(1)=2\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = 1 + operatornameerf(tsqrt2λ)operatornameerf(1sqrt2λ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_11","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_11","text":"prob_bvp_linear_11\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λf(t y_1)\nendalign*\n\nwhere\n\nf(t y_1) = y_1 - (1+λπ^2)cos(πt)\n\nwith boundary condition\n\ny_1(-1)=0  y_1(1)=2\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_12","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_12","text":"prob_bvp_linear_12\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(t y_1)\nendalign*\n\nwhere\n\nf(t y_1) = y_1 - (1+λπ^2)cos(πt)\n\nwith boundary condition\n\ny_1(-1)=-1 y_1(1)=0\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) + fracexp((t+1)sqrtλ) - exp(-(t+1))sqrtλexp(2sqrtλ) - exp(-2sqrtλ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_13","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_13","text":"prob_bvp_linear_13\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(t y_1)\nendalign*\n\nwhere\n\nf(t y_1) = y_1 - (1+λπ^2)cos(πt)\n\nwith boundary condition\n\ny_1(-1)=0  y_1(1)=-1\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) + exp(-(t+1)sqrtλ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_14","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_14","text":"prob_bvp_linear_14\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2\nfracdy_2dt = frac1λ f(t y_1)\nendalign*\n\nwhere\n\nf(t y_1)=y_1-(1+λπ^2)cos(πt)\n\nwith boundary condition\n\ny_1(-1) = exp(-2sqrtλ) \ny_1(1)  = exp(-2sqrtλ)\n\nSolution\n\nThe analytical solution for t in -1 1 is\n\nbeginalign*\ny_1(t) = cos(πt) + exp((t-1)sqrtλ) + exp(-(t+1)sqrtλ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_15","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_15","text":"prob_bvp_linear_15\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ t y_1\nendalign*\n\nwith boundary condition\n\ny_1(-1)=1  y_1(1)=1\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_16","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_16","text":"prob_bvp_linear_16\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ^2 f(y_1)\nendalign*\n\nwhere\n\nf(t y_1) = -π^2 y_14\n\nwith boundary condition\n\ny_1(0)=0  y_1(1)=sin(π(2λ))\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = sin(πt2λ) text when  1λ text is odd  \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_17","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_17","text":"prob_bvp_linear_17\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = f(y_1)\nendalign*\n\nwhere\n\nf(t y_1) = -3λ y_1  (λ+t^2)^2\n\nwith boundary condition\n\ny_1(-01) = frac-01sqrtλ+001\ny_1(01)  = frac 01sqrtλ+001\n\nSolution\n\nThe analytical solution for t in -01 01 is\n\nbeginalign*\ny_1(t) = tsqrtλ+t^2 \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_linear_18","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_linear_18","text":"prob_bvp_linear_18\n\nLinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ y_2\nendalign*\n\nwith boundary condition\n\ny_1(0)=1  y_1(1)=frac01sqrtλ+001\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = exp(-tλ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_1","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_1","text":"prob_bvp_nonlinear_1\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ y_2\nendalign*\n\nwhere\n\nf(y_2)=-y_1\n\nwith boundary condition\n\ny_1(0)=1 y_1(1)=01sqrtλ+001\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = exp(-tλ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_2","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_2","text":"prob_bvp_nonlinear_2\n\nNonlinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ f(y_2)\nendalign*\n\nwhere\n\nf(y_2)=--y_2^2+1\n\nwith boundary condition\n\nbeginalign*\ny_1(0) = 1 + λ lncosh(-0745λ) \ny_1(1) = 1 + λ lncosh(0255λ)\nendalign*\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = 1 + λ lncosh((t-0745)λ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_3","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_3","text":"prob_bvp_nonlinear_3\n\nNonlinear boundary value problem with analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ f(y y_1)\nendalign*\n\nwhere\n\nf(y_1) = y_1 + y_1^2 - exp(-2tsqrtλ)\n\nwith boundary condition\n\ny_1(0)=1 y_1(1)=exp(-1sqrtλ)\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nbeginalign*\ny_1(t) = exp(-tsqrtλ) \ny_2(t) = y_1(t)\nendalign*\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_4","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_4","text":"prob_bvp_nonlinear_4\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ f(y_1 y_2)\nendalign*\n\nwhere\n\nf(y_1 y_2)=-y_2-y_1^2\n\nwith boundary condition\n\ny_1(0)=0 y_1(1)=12\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_5","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_5","text":"prob_bvp_nonlinear_5\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ f(y_1)\nendalign*\n\nwhere\n\nf(y_1) = λsinh(λz)\n\nwith boundary condition\n\ny_1(0)=0 y_1(1)=1\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_6","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_6","text":"prob_bvp_nonlinear_6\n\nThis problem describes a shock wave in a one dimension nozzle flow.\n\nThe steady state Navier-Stokes equations generate a second order differential equations which can be reduced to a first order system described by nonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = -frac1λ f(y_1)\nendalign*\n\nwhere\n\nf(t y_1 y_2) = left(frac1+γ2 - λ A(t)right) y_1 y_2 - fracy_2y_1 - fracA(t)A(t) left(1 - fracγ-12 y_1^2right)\n\nwith boundary condition\n\ny_1(0)=09129  y_1(1)=0375\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_7","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_7","text":"prob_bvp_nonlinear_7\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(y_1 y_2)\nendalign*\n\nwhere\n\nf(y_1 y_2) = -y_1 y_2 + y_1\n\nwith boundary condition\n\ny_1(0)=-13  y_1(1)=13\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_8","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_8","text":"prob_bvp_nonlinear_8\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(y_1 y_2)\nendalign*\n\nwhere\n\nf(y_1 y_2)=-y_1y_2+y_1\n\nwith boundary condition\n\ny_1(0)=1  y_1(1)=-13\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_9","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_9","text":"prob_bvp_nonlinear_9\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(y_1 y_2) \nendalign*\n\nwhere\n\nf(y_1 y_2)=-y_1y_2+y_1\n\nwith boundary condition\n\ny_1(0)=1 y_1(1)=13\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_10","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_10","text":"prob_bvp_nonlinear_10\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(y_1 y_2)\nendalign*\n\nwhere\n\nf(y_1 y_2)=-y_1y_2+y_1\n\nwith boundary condition\n\ny_1(0)=1  y_1(1)=32\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_11","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_11","text":"prob_bvp_nonlinear_11\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(y_1 y_2)\nendalign*\n\nwhere\n\nf(y_1 y_2)=-y_1y_2+y_1\n\nwith boundary condition\n\ny_1(0)=0  y_1(1)=32\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_12","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_12","text":"prob_bvp_nonlinear_12\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λ f(y_1 y_2)\nendalign*\n\nwhere\n\nf(y_1 y_2) = -y_1 y_2 + y_1\n\nwith boundary condition\n\ny_1(0)=-76  y_1(1)=32\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_13","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_13","text":"prob_bvp_nonlinear_13\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = sin(y_2) \nfracdy_2dt = y_3 \nfracdy_3dt = -y_4λ \nfracdy_4dt = f(y_1 y_2 y_3 y_4)\nendalign*\n\nwhere\n\nf(z θ M Q) = frac1λ (z-1) cos θ - M sec θ + λ Q tan θ\n\nwith boundary condition\n\ny_1(0)=0  y_3(0)=0 \ny_1(1)=0  y_3(1)=0\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_14","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_14","text":"prob_bvp_nonlinear_14\n\nThis problem arises from fluid injection through one side of a long vertical channel\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = y_3 \nfracdy_3dt = y_4 \nfracdy_4dt = λ(y_2 y_3 - y_1 y_4)\nendalign*\n\nwith boundary condition\n\ny_1(0)=0  y_2(0)=0 \ny_1(1)=1  y_2(1)=0\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.prob_bvp_nonlinear_15","page":"BVP Problems","title":"BVProblemLibrary.prob_bvp_nonlinear_15","text":"prob_bvp_nonlinear_15\n\nThis problem arises from fluid injection through one side of a long vertical channel\n\nNonlinear boundary value problem with no analytical solution, given by\n\nbeginalign*\nfracdy_1dt = y_2 \nfracdy_2dt = frac1λy_1y_4 - y_3y_2 \nfracdy_3dt = y_4 \nfracdy_4dt = y_5 \nfracdy_5dt = y_6 \nfracdy_6dt = frac1λ(-y_3y_6-y_1y_2)\nendalign*\n\nwith boundary condition\n\nbeginalign*\ny_1(0)=-1  y_3(0)=0  y_4(0)=0 \ny_1(1)= 1  y_3(1)=0  y_4(1)=0\nendalign*\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.flat_moon","page":"BVP Problems","title":"BVProblemLibrary.flat_moon","text":"flat_moon\n\nThis test problem is about the optimal-time launching of a satellite into orbit from flat moon without atmospheric drag.\n\nGiven by\n\nbeginalign*\nfracdz_1dt = z_3 t_f \nfracdz_2dt = z_4 t_f \nfracdz_3dt = Acos(z_5) t_f \nfracdz_4dt = (Asin(z_5)-g) t_f \nfracdz_5dt = -z_6cos(z_5) t_F \nfracdz_6dt = z_6^2sin(z_5) t_f \nfracdz_7dt = 0\nendalign*\n\nwith boundary condition\n\nbeginalign*\nz_1(0) = 0 \nz_2(0) = 0 \nz_3(0) = 0 \nz_4(0) = 0 \nz_5(1) = h \nz_6(1) = V_c \nz_7(1) = 0\nendalign*\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.flat_earth","page":"BVP Problems","title":"BVProblemLibrary.flat_earth","text":"flat_earth\n\nLaunch of a satellite into circular orbit from a flat Earth where we assume a uniform gravitational field g.\n\nGiven by\n\nbeginalign*\nfracdz_1dt = z_3 fracV_ch 2pt\nfracdz_2dt = z_4 fracV_ch 2pt\nfracdz_3dt = acc frac1V_csqrt1+z_6^2 2pt\nfracdz_4dt = acc frac1V_csqrt1+z_6^2-fracgV_c 2pt\nfracdz_5dt = 0 2pt\nfracdz_6dt = -z_5 fracV_ch 2pt\nfracdz_7dt = 0\nendalign*\n\nwith boundary conditions\n\nbeginalign*\nz_1(0) = 0 \nz_2(0) = 0 \nz_3(0) = 0 \nz_4(0) = 0 \nz_5(1) = h \nz_6(1) = V_c \nz_7(1) = 0\nendalign*\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.flat_earth_drag","page":"BVP Problems","title":"BVProblemLibrary.flat_earth_drag","text":"flat_earth_drag\n\nLaunch into circular orbit from a flat Earth including atmospheric drag.\n\nGiven by\n\nbeginalign*\nfracdz_1dt = z_3 fracV_ch \nfracdz_2dt = z_4 fracV_ch \nfracdz_3dt = fracfV_c left(-fracz_6z_6^2+z_7^2 - V_c ηexp(-z_2 β) z_3sqrtz_3^3+z_4^2right)m \nfracdz_4dt = fracfV_c left(-fracz_7z_6^2+z_7^2 - V_c ηexp(-z_2 β) z_4sqrtz_3^3+z_4^2right)m - g_accelV_c \nfracdz_5dt = -ηβ exp(-z_2 β) (z_6z_3+z_7z_4)sqrtz_3^3+z_4^2fracV_cm \nfracdz_6dt = η exp(-z_2 β) left(z_6(2z_3^2+z_4^2)+z_7z_3z_4right) V_csqrtz_3^2+z_4^2m \nfracdz_7dt = η exp(-z_2 β) left(z_7(z_3^2+2z_4^2)+z_6z_3z_4right) V_csqrtz_3^2+z_4^2m \nendalign*\n\nwith boundary conditions\n\nbeginalign*\nz_1(0) = 0 \nz_2(0) = 0 \nz_3(0) = 0 \nz_4(0) = 0 \nz_5(1) = h \nz_6(1) = V_c \nz_7(1) = 0\nendalign*\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"types/bvp_types/#BVProblemLibrary.measles","page":"BVP Problems","title":"BVProblemLibrary.measles","text":"measles\n\nThis is an epidemiology model, about the spread of diseases.\n\nGiven by\n\nbeginalign*\nfracdy_1dt = μ - β(t) y_1 y_3 \nfracdy_2dt = β(t) y_1 y_3 - fracy_2λ \nfracdy_3dt = fracy_2λ - fracy_3η\nendalign*\n\nwith boundary condition\n\ny(0) = y(1)\n\nSolution\n\nNo analytical solution\n\nReferences\n\nReference\n\n\n\n\n\n","category":"constant"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock","text":"Rosenbrock methods are semi-implicit Runge-Kutta methods designed for small to medium-sized stiff systems of differential equations. These methods combine the stability properties of implicit methods with computational efficiency by using approximate Jacobians. They work particularly well for strict tolerance requirements.","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#Key-Properties","page":"OrdinaryDiffEqRosenbrock","title":"Key Properties","text":"Rosenbrock methods provide:\n\nExcellent efficiency for small to medium systems (<1000 ODEs)\nL-stable and A-stable variants for stiff problems\nW-method structure making them robust to inaccurate Jacobians\nAutomatic differentiation compatibility for Jacobian computation\nHigh-order accuracy with embedded error estimation\nExcellent performance for strict tolerance requirements\nStiffly accurate variants for enhanced stability","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#When-to-Use-Rosenbrock-Methods","page":"OrdinaryDiffEqRosenbrock","title":"When to Use Rosenbrock Methods","text":"Rosenbrock methods are recommended for:\n\nSmall to medium stiff systems (10 to 1000 equations)\nProblems where Jacobians are available or can be computed efficiently\nMedium tolerance requirements (1e-8 to 1e-2)\nStiff ODEs arising from reaction kinetics and chemical systems\nModerately stiff PDEs after spatial discretization\nProblems requiring reliable error control for stiff systems","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#Solver-Selection-Guide","page":"OrdinaryDiffEqRosenbrock","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#Low-tolerance-(1e-2)","page":"OrdinaryDiffEqRosenbrock","title":"Low tolerance (>1e-2)","text":"Rosenbrock23: Second/third-order method, recommended for low tolerance requirements","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#Medium-tolerance-(1e-8-to-1e-2)","page":"OrdinaryDiffEqRosenbrock","title":"Medium tolerance (1e-8 to 1e-2)","text":"Rodas5P: Fifth-order method, most efficient for many problems\nRodas4P: Fourth-order method, more reliable than Rodas5P\nRodas5Pe: Enhanced fifth-order variant with stiffly accurate embedded estimate for better adaptivity on highly stiff equations\nRodas5Pr: Fifth-order variant with residual test for robust error estimation that guarantees accuracy on interpolation","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#Performance-Guidelines","page":"OrdinaryDiffEqRosenbrock","title":"Performance Guidelines","text":"Rodas5P: Best overall efficiency at medium tolerances\nRodas4P: Most reliable when Rodas5P fails\nRosenbrock23: Fastest at high tolerances (>1e-2)","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#When-to-Choose-Alternatives","page":"OrdinaryDiffEqRosenbrock","title":"When to Choose Alternatives","text":"Consider other methods when:\n\nSystem size > 1000: Use BDF methods (QNDF, FBDF)\nMatrix-free methods: If using Krylov solvers for the linear solver (matrix-free methods), SDIRK or BDF methods are preferred","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#Advantages-of-Rosenbrock-Methods","page":"OrdinaryDiffEqRosenbrock","title":"Advantages of Rosenbrock Methods","text":"Very low tolerances: Rosenbrock23 performs well even at very low tolerances\nVery high stiffness: Rosenbrock methods are often more stable than SDIRK or BDF methods because other methods can diverge due to bad initial guesses for the Newton method (leading to nonlinear solver divergence)\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqRosenbrock\", \"Rodas5P\")","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#Full-list-of-solvers","page":"OrdinaryDiffEqRosenbrock","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rosenbrock23","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rosenbrock23","text":"Rosenbrock23(; chunk_size = Val{0}(),\n               standardtag = Val{true}(),\n               autodiff = AutoForwardDiff(),\n               concrete_jac = nothing,\n               diff_type = Val{:forward}(),\n               linsolve = nothing,\n               precs = DEFAULT_PRECS,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rosenbrock23(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nShampine L.F. and Reichelt M., (1997) The MATLAB ODE Suite, SIAM Journal of Scientific Computing, 18 (1), pp. 1-22.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rosenbrock32","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rosenbrock32","text":"Rosenbrock32(; chunk_size = Val{0}(),\n               standardtag = Val{true}(),\n               autodiff = AutoForwardDiff(),\n               concrete_jac = nothing,\n               diff_type = Val{:forward}(),\n               linsolve = nothing,\n               precs = DEFAULT_PRECS,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  An Order 3/2 A-Stable Rosenbrock-W method which is good for mildly stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rosenbrock32(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nShampine L.F. and Reichelt M., (1997) The MATLAB ODE Suite, SIAM Journal of Scientific Computing, 18 (1), pp. 1-22.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3P","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3P","text":"ROS3P(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n            ForwardDiff default function-specific tags. For more information, see\n            [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n            Defaults to `Val{true}()`.\n        - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n            to specify whether to use automatic differentiation via\n            [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n            differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n            Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n            `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n            To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n            `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n        - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n            `nothing`, which means it will be chosen true/false depending on circumstances\n            of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n        - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n          For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n          `ROS3P(linsolve = KLUFactorization()`).\n           When `nothing` is passed, uses `DefaultLinearSolver`.\n        - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n          can be used as a left or right preconditioner.\n          Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n          function where the arguments are defined as:\n            - `W`: the current Jacobian of the nonlinear system. Specified as either\n                ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                representation of the operator. Users can construct the W-matrix on demand\n                by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                the `jac_prototype`.\n            - `du`: the current ODE derivative\n            - `u`: the current ODE state\n            - `p`: the ODE parameters\n            - `t`: the current ODE time\n            - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                the last call to `precs`. It is recommended that this is checked to only\n                update the preconditioner when `newW == true`.\n            - `Plprev`: the previous `Pl`.\n            - `Prprev`: the previous `Pr`.\n            - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                Solver-dependent and subject to change.\n          The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n          To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n          which is not used. Additionally, `precs` must supply the dispatch:\n          ```julia\n          Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n          ```\n          which is used in the solver setup phase to construct the integrator\n          type with the preconditioners `(Pl,Pr)`.\n          The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n          is defined as:\n          ```julia\n          DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n          ```\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nLang, J. & Verwer, ROS3P—An Accurate Third-Order Rosenbrock Solver Designed for Parabolic Problems J. BIT Numerical Mathematics (2001) 41: 731. doi:10.1023/A:1021900219772\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas3","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas3","text":"Rodas3(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Rodas3(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  3rd order A-stable and stiffly stable Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSandu, Verwer, Van Loon, Carmichael, Potra, Dabdub, Seinfeld, Benchmarking stiff ode solvers for atmospheric chemistry problems-I.  implicit vs explicit, Atmospheric Environment, 31(19), 3151-3166, 1997.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas23W","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas23W","text":"Rodas23W(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  An Order 2/3 L-Stable Rosenbrock-W method for stiff ODEs and DAEs in mass matrix form. 2nd order stiff-aware interpolation and additional error test for interpolation.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas23W(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G., Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas3P","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas3P","text":"Rodas3P(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `Rodas3P(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  3rd order A-stable and stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant and additional error test for interpolation. Keeps accuracy on discretizations of linear parabolic PDEs.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G., Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas4","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas4","text":"Rodas4(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Rodas4(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas42","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas42","text":"Rodas42(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `Rodas42(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas4P","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas4P","text":"Rodas4P(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `Rodas4P(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach, G., Rentrop, P., An adaptive method of lines approach for modelling flow and transport in rivers.  Adaptive method of lines , Wouver, A. Vande, Sauces, Ph., Schiesser, W.E. (ed.),S. 181-205,Chapman & Hall/CRC, 2001,\nSteinebach, G., Order-reduction of ROW-methods for DAEs and method of lines  applications.  Preprint-Nr. 1741, FB Mathematik, TH Darmstadt, 1995.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas4P2","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas4P2","text":"Rodas4P2(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems. It is an improvement of Rodas4P and in case of inexact Jacobians a second order W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas4P2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G., Improvement of Rosenbrock-Wanner Method RODASP, In: Reis T., Grundel S., Schöps S. (eds)  Progress in Differential-Algebraic Equations II. Differential-Algebraic Equations Forum. Springer, Cham., 165-184, 2020.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5","text":"Rodas5(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Rodas5(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nDi Marzo G. RODAS5(4) – Méthodes de Rosenbrock d'ordre 5(4) adaptées aux problèmes différentiels-algébriques. MSc mathematics thesis, Faculty of Science, University of Geneva, Switzerland.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5P","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5P","text":"Rodas5P(; chunk_size = Val{0}(),\n          standardtag = Val{true}(),\n          autodiff = AutoForwardDiff(),\n          concrete_jac = nothing,\n          diff_type = Val{:forward}(),\n          linsolve = nothing,\n          precs = DEFAULT_PRECS,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant. Has improved stability in the adaptive time stepping embedding.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas5P(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G. Construction of Rosenbrock–Wanner method Rodas5P and numerical benchmarks within the Julia Differential Equations package. In: BIT Numerical Mathematics, 63(2), 2023. doi:10.1007/s10543-023-00967-x\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5Pe","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5Pe","text":"Rodas5Pe(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  Variant of Rodas5P with modified embedded scheme.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas5Pe(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G. Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5Pr","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5Pr","text":"Rodas5Pr(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  Variant of Rodas5P with additional residual control.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas5Pr(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G. Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.RosenbrockW6S4OS","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.RosenbrockW6S4OS","text":"RosenbrockW6S4OS(; chunk_size = Val{0}(),\n                   standardtag = Val{true}(),\n                   autodiff = AutoForwardDiff(),\n                   concrete_jac = nothing,\n                   diff_type = Val{:forward}(),\n                   linsolve = nothing,\n                   precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable Rosenbrock-W method (fixed step only).\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify RosenbrockW6S4OS(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nhttps://doi.org/10.1016/j.cam.2009.09.017\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS2","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS2","text":"ROS2(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n           ForwardDiff default function-specific tags. For more information, see\n           [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n           Defaults to `Val{true}()`.\n       - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n           to specify whether to use automatic differentiation via\n           [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n           differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n           Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n           `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n           To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n           `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n       - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n           `nothing`, which means it will be chosen true/false depending on circumstances\n           of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n       - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n         For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n         `ROS2(linsolve = KLUFactorization()`).\n          When `nothing` is passed, uses `DefaultLinearSolver`.\n       - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n         can be used as a left or right preconditioner.\n         Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n         function where the arguments are defined as:\n           - `W`: the current Jacobian of the nonlinear system. Specified as either\n               ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n               commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n               representation of the operator. Users can construct the W-matrix on demand\n               by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n               the `jac_prototype`.\n           - `du`: the current ODE derivative\n           - `u`: the current ODE state\n           - `p`: the ODE parameters\n           - `t`: the current ODE time\n           - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n               the last call to `precs`. It is recommended that this is checked to only\n               update the preconditioner when `newW == true`.\n           - `Plprev`: the previous `Pl`.\n           - `Prprev`: the previous `Pr`.\n           - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n               Solver-dependent and subject to change.\n         The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n         To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n         which is not used. Additionally, `precs` must supply the dispatch:\n         ```julia\n         Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n         ```\n         which is used in the solver setup phase to construct the integrator\n         type with the preconditioners `(Pl,Pr)`.\n         The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n         is defined as:\n         ```julia\n         DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n         ```)\n\nRosenbrock-Wanner Method.  A 2nd order L-stable Rosenbrock method with 2 internal stages.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nJ. G. Verwer et al. (1999): A second-order Rosenbrock method applied to photochemical dispersion problems https://doi.org/10.1137/S1064827597326651\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS2PR","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS2PR","text":"ROS2PR(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `ROS2PR(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```)\n\nRosenbrock-Wanner Method.  2nd order stiffly accurate Rosenbrock method with 3 internal stages with (Rinf=0). For problems with medium stiffness the convergence behaviour is very poor and it is recommended to use ROS2S instead.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS2S","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS2S","text":"ROS2S(; chunk_size = Val{0}(),\n        standardtag = Val{true}(),\n        autodiff = AutoForwardDiff(),\n        concrete_jac = nothing,\n        diff_type = Val{:forward}(),\n        linsolve = nothing,\n        precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  2nd order stiffly accurate Rosenbrock-Wanner W-method with 3 internal stages with B_PR consistent of order 2 with (Rinf=0).\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS2S(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3","text":"ROS3(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n           ForwardDiff default function-specific tags. For more information, see\n           [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n           Defaults to `Val{true}()`.\n       - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n           to specify whether to use automatic differentiation via\n           [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n           differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n           Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n           `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n           To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n           `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n       - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n           `nothing`, which means it will be chosen true/false depending on circumstances\n           of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n       - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n         For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n         `ROS3(linsolve = KLUFactorization()`).\n          When `nothing` is passed, uses `DefaultLinearSolver`.\n       - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n         can be used as a left or right preconditioner.\n         Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n         function where the arguments are defined as:\n           - `W`: the current Jacobian of the nonlinear system. Specified as either\n               ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n               commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n               representation of the operator. Users can construct the W-matrix on demand\n               by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n               the `jac_prototype`.\n           - `du`: the current ODE derivative\n           - `u`: the current ODE state\n           - `p`: the ODE parameters\n           - `t`: the current ODE time\n           - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n               the last call to `precs`. It is recommended that this is checked to only\n               update the preconditioner when `newW == true`.\n           - `Plprev`: the previous `Pl`.\n           - `Prprev`: the previous `Pr`.\n           - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n               Solver-dependent and subject to change.\n         The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n         To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n         which is not used. Additionally, `precs` must supply the dispatch:\n         ```julia\n         Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n         ```\n         which is used in the solver setup phase to construct the integrator\n         type with the preconditioners `(Pl,Pr)`.\n         The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n         is defined as:\n         ```julia\n         DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n         ```)\n\nRosenbrock-Wanner Method.  3rd order L-stable Rosenbrock method with 3 internal stages with an embedded strongly A-stable 2nd order method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3PR","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3PR","text":"ROS3PR(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `ROS3PR(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```)\n\nRosenbrock-Wanner Method.  3nd order stiffly accurate Rosenbrock method with 3 internal stages with B_PR consistent of order 3, which is strongly A-stable with Rinf~=-0.73.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Scholz4_7","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Scholz4_7","text":"Scholz4_7(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n                ForwardDiff default function-specific tags. For more information, see\n                [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n                Defaults to `Val{true}()`.\n            - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n                to specify whether to use automatic differentiation via\n                [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n                differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n                Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n                `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n                To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n                `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n            - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n                `nothing`, which means it will be chosen true/false depending on circumstances\n                of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n            - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n              For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n              `Scholz4_7(linsolve = KLUFactorization()`).\n               When `nothing` is passed, uses `DefaultLinearSolver`.\n            - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n              can be used as a left or right preconditioner.\n              Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n              function where the arguments are defined as:\n                - `W`: the current Jacobian of the nonlinear system. Specified as either\n                    ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                    commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                    representation of the operator. Users can construct the W-matrix on demand\n                    by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                    the `jac_prototype`.\n                - `du`: the current ODE derivative\n                - `u`: the current ODE state\n                - `p`: the ODE parameters\n                - `t`: the current ODE time\n                - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                    the last call to `precs`. It is recommended that this is checked to only\n                    update the preconditioner when `newW == true`.\n                - `Plprev`: the previous `Pl`.\n                - `Prprev`: the previous `Pr`.\n                - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                    Solver-dependent and subject to change.\n              The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n              To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n              which is not used. Additionally, `precs` must supply the dispatch:\n              ```julia\n              Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n              ```\n              which is used in the solver setup phase to construct the integrator\n              type with the preconditioners `(Pl,Pr)`.\n              The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n              is defined as:\n              ```julia\n              DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n              ```)\n\nRosenbrock-Wanner Method.  3nd order stiffly accurate Rosenbrock method with 3 internal stages with B_PR consistent of order 3, which is strongly A-stable with Rinf~=-0.73. Convergence with order 4 for the stiff case, but has a poor accuracy.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW1a","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW1a","text":"ROS34PW1a(; chunk_size = Val{0}(),\n            standardtag = Val{true}(),\n            autodiff = AutoForwardDiff(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable Rosenbrock-W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW1a(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW1b","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW1b","text":"ROS34PW1b(; chunk_size = Val{0}(),\n            standardtag = Val{true}(),\n            autodiff = AutoForwardDiff(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable Rosenbrock-W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW1b(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW2","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW2","text":"ROS34PW2(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW3","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW3","text":"ROS34PW3(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW3(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PRw","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PRw","text":"ROS34PRw(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  3rd order stiffly accurate Rosenbrock-Wanner W-method with 4 internal stages, B_PR consistent of order 2. The order of convergence decreases if medium stiff problems are considered.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PRw(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nJoachim Rang, Improved traditional Rosenbrock–Wanner methods for stiff ODEs and DAEs, Journal of Computational and Applied Mathematics, https://doi.org/10.1016/j.cam.2015.03.010\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3PRL","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3PRL","text":"ROS3PRL(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `ROS3PRL(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```)\n\nRosenbrock-Wanner Method.  3rd order stiffly accurate Rosenbrock method with 4 internal stages, B_PR consistent of order 2 with Rinf=0. The order of convergence decreases if medium stiff problems are considered, but it has good results for very stiff cases.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3PRL2","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3PRL2","text":"ROS3PRL2(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n               ForwardDiff default function-specific tags. For more information, see\n               [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n               Defaults to `Val{true}()`.\n           - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n               to specify whether to use automatic differentiation via\n               [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n               differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n               Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n               `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n               To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n               `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n           - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n               `nothing`, which means it will be chosen true/false depending on circumstances\n               of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n           - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n             For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n             `ROS3PRL2(linsolve = KLUFactorization()`).\n              When `nothing` is passed, uses `DefaultLinearSolver`.\n           - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n             can be used as a left or right preconditioner.\n             Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n             function where the arguments are defined as:\n               - `W`: the current Jacobian of the nonlinear system. Specified as either\n                   ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                   commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                   representation of the operator. Users can construct the W-matrix on demand\n                   by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                   the `jac_prototype`.\n               - `du`: the current ODE derivative\n               - `u`: the current ODE state\n               - `p`: the ODE parameters\n               - `t`: the current ODE time\n               - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                   the last call to `precs`. It is recommended that this is checked to only\n                   update the preconditioner when `newW == true`.\n               - `Plprev`: the previous `Pl`.\n               - `Prprev`: the previous `Pr`.\n               - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                   Solver-dependent and subject to change.\n             The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n             To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n             which is not used. Additionally, `precs` must supply the dispatch:\n             ```julia\n             Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n             ```\n             which is used in the solver setup phase to construct the integrator\n             type with the preconditioners `(Pl,Pr)`.\n             The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n             is defined as:\n             ```julia\n             DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n             ```)\n\nRosenbrock-Wanner Method.  3rd order stiffly accurate Rosenbrock method with 4 internal stages, B_PR consistent of order 3. The order of convergence does NOT decreases if medium stiff problems are considered as it does for ROS3PRL.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROK4a","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROK4a","text":"ROK4a(; chunk_size = Val{0}(),\n        standardtag = Val{true}(),\n        autodiff = AutoForwardDiff(),\n        concrete_jac = nothing,\n        diff_type = Val{:forward}(),\n        linsolve = nothing,\n        precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  4rd order L-stable Rosenbrock-Krylov method with 4 internal stages, with a 3rd order embedded method which is strongly A-stable with Rinf~=0.55. (when using exact Jacobians)\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROK4a(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nTranquilli, Paul and Sandu, Adrian (2014): Rosenbrock–Krylov Methods for Large Systems of Differential Equations https://doi.org/10.1137/130923336\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.RosShamp4","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.RosShamp4","text":"RosShamp4(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n                ForwardDiff default function-specific tags. For more information, see\n                [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n                Defaults to `Val{true}()`.\n            - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n                to specify whether to use automatic differentiation via\n                [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n                differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n                Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n                `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n                To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n                `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n            - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n                `nothing`, which means it will be chosen true/false depending on circumstances\n                of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n            - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n              For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n              `RosShamp4(linsolve = KLUFactorization()`).\n               When `nothing` is passed, uses `DefaultLinearSolver`.\n            - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n              can be used as a left or right preconditioner.\n              Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n              function where the arguments are defined as:\n                - `W`: the current Jacobian of the nonlinear system. Specified as either\n                    ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                    commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                    representation of the operator. Users can construct the W-matrix on demand\n                    by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                    the `jac_prototype`.\n                - `du`: the current ODE derivative\n                - `u`: the current ODE state\n                - `p`: the ODE parameters\n                - `t`: the current ODE time\n                - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                    the last call to `precs`. It is recommended that this is checked to only\n                    update the preconditioner when `newW == true`.\n                - `Plprev`: the previous `Pl`.\n                - `Prprev`: the previous `Pr`.\n                - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                    Solver-dependent and subject to change.\n              The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n              To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n              which is not used. Additionally, `precs` must supply the dispatch:\n              ```julia\n              Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n              ```\n              which is used in the solver setup phase to construct the integrator\n              type with the preconditioners `(Pl,Pr)`.\n              The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n              is defined as:\n              ```julia\n              DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n              ```)\n\nRosenbrock-Wanner Method.  An A-stable 4th order Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nL. F. Shampine, Implementation of Rosenbrock Methods, ACM Transactions on Mathematical Software (TOMS), 8: 2, 93-113. doi:10.1145/355993.355994\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Veldd4","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Veldd4","text":"Veldd4(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Veldd4(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```)\n\nRosenbrock-Wanner Method.  A 4th order D-stable Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nvan Veldhuizen, D-stability and Kaps-Rentrop-methods, M. Computing (1984) 32: 229. doi:10.1007/BF02243574\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Velds4","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Velds4","text":"Velds4(; chunk_size = Val{0}(),\n         standardtag = Val{true}(),\n         autodiff = AutoForwardDiff(),\n         concrete_jac = nothing,\n         diff_type = Val{:forward}(),\n         linsolve = nothing,\n         precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order A-stable Rosenbrock method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Velds4(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nvan Veldhuizen, D-stability and Kaps-Rentrop-methods, M. Computing (1984) 32: 229. doi:10.1007/BF02243574\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.GRK4T","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.GRK4T","text":"GRK4T(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n            ForwardDiff default function-specific tags. For more information, see\n            [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n            Defaults to `Val{true}()`.\n        - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n            to specify whether to use automatic differentiation via\n            [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n            differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n            Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n            `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n            To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n            `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n        - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n            `nothing`, which means it will be chosen true/false depending on circumstances\n            of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n        - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n          For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n          `GRK4T(linsolve = KLUFactorization()`).\n           When `nothing` is passed, uses `DefaultLinearSolver`.\n        - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n          can be used as a left or right preconditioner.\n          Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n          function where the arguments are defined as:\n            - `W`: the current Jacobian of the nonlinear system. Specified as either\n                ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                representation of the operator. Users can construct the W-matrix on demand\n                by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                the `jac_prototype`.\n            - `du`: the current ODE derivative\n            - `u`: the current ODE state\n            - `p`: the ODE parameters\n            - `t`: the current ODE time\n            - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                the last call to `precs`. It is recommended that this is checked to only\n                update the preconditioner when `newW == true`.\n            - `Plprev`: the previous `Pl`.\n            - `Prprev`: the previous `Pr`.\n            - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                Solver-dependent and subject to change.\n          The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n          To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n          which is not used. Additionally, `precs` must supply the dispatch:\n          ```julia\n          Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n          ```\n          which is used in the solver setup phase to construct the integrator\n          type with the preconditioners `(Pl,Pr)`.\n          The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n          is defined as:\n          ```julia\n          DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n          ```)\n\nRosenbrock-Wanner Method.  An efficient 4th order Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nKaps, P. & Rentrop, Generalized Runge-Kutta methods of order four with stepsize control for stiff ordinary differential equations. P. Numer. Math. (1979) 33: 55. doi:10.1007/BF01396495\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.GRK4A","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.GRK4A","text":"GRK4A(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n            ForwardDiff default function-specific tags. For more information, see\n            [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n            Defaults to `Val{true}()`.\n        - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n            to specify whether to use automatic differentiation via\n            [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n            differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n            Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n            `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n            To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n            `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n        - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n            `nothing`, which means it will be chosen true/false depending on circumstances\n            of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n        - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n          For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n          `GRK4A(linsolve = KLUFactorization()`).\n           When `nothing` is passed, uses `DefaultLinearSolver`.\n        - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n          can be used as a left or right preconditioner.\n          Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n          function where the arguments are defined as:\n            - `W`: the current Jacobian of the nonlinear system. Specified as either\n                ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                representation of the operator. Users can construct the W-matrix on demand\n                by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                the `jac_prototype`.\n            - `du`: the current ODE derivative\n            - `u`: the current ODE state\n            - `p`: the ODE parameters\n            - `t`: the current ODE time\n            - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                the last call to `precs`. It is recommended that this is checked to only\n                update the preconditioner when `newW == true`.\n            - `Plprev`: the previous `Pl`.\n            - `Prprev`: the previous `Pr`.\n            - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                Solver-dependent and subject to change.\n          The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n          To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n          which is not used. Additionally, `precs` must supply the dispatch:\n          ```julia\n          Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n          ```\n          which is used in the solver setup phase to construct the integrator\n          type with the preconditioners `(Pl,Pr)`.\n          The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n          is defined as:\n          ```julia\n          DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n          ```)\n\nRosenbrock-Wanner Method.  An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nKaps, P. & Rentrop, Generalized Runge-Kutta methods of order four with stepsize control for stiff ordinary differential equations. P. Numer. Math. (1979) 33: 55. doi:10.1007/BF01396495\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/Rosenbrock/#OrdinaryDiffEqRosenbrock.Ros4LStab","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Ros4LStab","text":"Ros4LStab(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n                ForwardDiff default function-specific tags. For more information, see\n                [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n                Defaults to `Val{true}()`.\n            - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n                to specify whether to use automatic differentiation via\n                [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n                differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n                Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n                `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n                To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n                `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n            - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n                `nothing`, which means it will be chosen true/false depending on circumstances\n                of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n            - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n              For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n              `Ros4LStab(linsolve = KLUFactorization()`).\n               When `nothing` is passed, uses `DefaultLinearSolver`.\n            - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n              can be used as a left or right preconditioner.\n              Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n              function where the arguments are defined as:\n                - `W`: the current Jacobian of the nonlinear system. Specified as either\n                    ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                    commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                    representation of the operator. Users can construct the W-matrix on demand\n                    by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                    the `jac_prototype`.\n                - `du`: the current ODE derivative\n                - `u`: the current ODE state\n                - `p`: the ODE parameters\n                - `t`: the current ODE time\n                - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                    the last call to `precs`. It is recommended that this is checked to only\n                    update the preconditioner when `newW == true`.\n                - `Plprev`: the previous `Pl`.\n                - `Prprev`: the previous `Pr`.\n                - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                    Solver-dependent and subject to change.\n              The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n              To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n              which is not used. Additionally, `precs` must supply the dispatch:\n              ```julia\n              Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n              ```\n              which is used in the solver setup phase to construct the integrator\n              type with the preconditioners `(Pl,Pr)`.\n              The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n              is defined as:\n              ```julia\n              DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n              ```)\n\nRosenbrock-Wanner Method.  A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"examples/conditional_dosing/#Conditional-Dosing-in-Pharmacometrics","page":"Conditional Dosing in Pharmacometrics","title":"Conditional Dosing in Pharmacometrics","text":"In this example, we will show how to model a conditional dosing using the DiscreteCallbacks. The problem is as follows. The patient has a drug A(t) in their system. The concentration of the drug is given as C(t)=A(t)/V for some volume constant V. At t=4, the patient goes to the clinic and is checked. If the concentration of the drug in their body is below 4, then they will receive a new dose.\n\nFor our model, we will use the simple decay equation. We will write this in the in-place form to make it easy to extend to more complicated examples:\n\nimport DifferentialEquations as DE\nfunction f(du, u, p, t)\n    du[1] = -u[1]\nend\nu0 = [10.0]\nconst V = 1\nprob = DE.ODEProblem(f, u0, (0.0, 10.0))\n\nLet's see what the solution looks like without any events.\n\nsol = DE.solve(prob, DE.Tsit5())\nimport Plots;\nPlots.gr();\nPlots.plot(sol)\n\nWe see that at time t=4, the patient should receive a dose. Let's code up that event. We need to check at t=4 if the concentration u[1]/4 is <4, and if so, add 10 to u[1]. We do this with the following:\n\ncondition(u, t, integrator) = t == 4 && u[1] / V < 4\naffect!(integrator) = integrator.u[1] += 10\ncb = DE.DiscreteCallback(condition, affect!)\n\nNow we will give this callback to the solver, and tell it to stop at t=4 so that way the condition can be checked:\n\nsol = DE.solve(prob, DE.Tsit5(), tstops = [4.0], callback = cb)\nimport Plots;\nPlots.gr();\nPlots.plot(sol)\n\nLet's show that it actually added 10 instead of setting the value to 10. We could have set the value using affect!(integrator) = integrator.u[1] = 10\n\nprintln(sol(4.00000))\nprintln(sol(4.000000000001))\n\nNow let's model a patient whose decay rate for the drug is lower:\n\nfunction f(du, u, p, t)\n    du[1] = -u[1] / 6\nend\nu0 = [10.0]\nconst V = 1\nprob = DE.ODEProblem(f, u0, (0.0, 10.0))\n\nsol = DE.solve(prob, DE.Tsit5())\nimport Plots;\nPlots.gr();\nPlots.plot(sol)\n\nUnder the same criteria, with the same event, this patient will not receive a second dose:\n\nsol = DE.solve(prob, DE.Tsit5(), tstops = [4.0], callback = cb)\nimport Plots;\nPlots.gr();\nPlots.plot(sol)","category":"section"},{"location":"extras/timestepping/#timestepping","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"","category":"section"},{"location":"extras/timestepping/#Common-Setup","page":"Timestepping Method Descriptions","title":"Common Setup","text":"All methods start by calculating a scaled error estimate on each scalar component of u:\n\nerr^scaled_i = norm(err_i(abstol_i + max(uprev_iu_i)reltol_i))\n\nOn this scaled error estimate, we calculate the norm. This norm is usually the Hairer norm:\n\nnorm(x) = sqrt(sum(x^2)length(x))\n\nThis norm works well because it does not change if we add new pieces to the differential equation: it scales our error by the number of equations so that independent equations will not step differently than a single solve.\n\nIn all cases, the step is rejected if err^scaled1 since that means the error is larger than the tolerances, and the step is accepted if err^scaled1.","category":"section"},{"location":"extras/timestepping/#Integral-Controller-(Standard-Controller)","page":"Timestepping Method Descriptions","title":"Integral Controller (Standard Controller)","text":"The integral control algorithm is the “standard algorithm” for adaptive timestepping. Note that it is not the default in DifferentialEquations.jl because it is usually awful for performance, but it is explained first because it is the most widely taught algorithm and others build on its techniques.\n\nThe control simply changes dt proportional to the error. There is an exponentiation based on the order of the algorithm which goes back to a result by Cechino for the optimal stepsize to reduce the error. The algorithm is:\n\nqtmp = integrator.EEst^(1 / (alg_adaptive_order(integrator.alg) + 1)) /\n       integrator.opts.gamma\n@fastmath q = max(inv(integrator.opts.qmax), min(inv(integrator.opts.qmin), qtmp))\nintegrator.dtnew = integrator.dt / q\n\nThus, q is the scaling factor for dt, and it must be between qmin and qmax. gamma is the safety factor, 0.9, for how much dt is decreased below the theoretical “optimal” value.\n\nSince proportional control is “jagged”, i.e. can cause large changes between one step to the next, it can effect the stability of explicit methods. Thus, it's only applied by default to low order implicit solvers.","category":"section"},{"location":"extras/timestepping/#Proportional-Integral-Controller-(PI-Controller)","page":"Timestepping Method Descriptions","title":"Proportional-Integral Controller (PI Controller)","text":"The proportional-integral control algorithm is a standard control algorithm from control theory. It mixes proportional control with memory in order to make the timesteps more stable, which actually increases the adaptive stability region of the algorithm. This stability property means that it's well-suited for explicit solvers, and it's applied by default to the Rosenbrock methods as well. The form for the updates is:\n\nEEst, beta1, q11, qold, beta2 = integrator.EEst, integrator.opts.beta1, integrator.q11,\nintegrator.qold, integrator.opts.beta2\n@fastmath q11 = EEst^beta1\n@fastmath q = q11 / (qold^beta2)\nintegrator.q11 = q11\n@fastmath q = max(inv(integrator.opts.qmax),\n    min(inv(integrator.opts.qmin), q / integrator.opts.gamma))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n    q = one(q)\nend\nq\n\nbeta1 is the gain on the proportional part, and beta2 is the gain for the history portion. qoldinit is the initialized value for the gain history.","category":"section"},{"location":"extras/timestepping/#Proportional-Integral-Derivative-Controller-(PID-Controller)","page":"Timestepping Method Descriptions","title":"Proportional-Integral-Derivative Controller (PID Controller)","text":"","category":"section"},{"location":"extras/timestepping/#Gustafsson-Acceleration","page":"Timestepping Method Descriptions","title":"Gustafsson Acceleration","text":"The Gustafsson acceleration algorithm accelerates changes so that way algorithms can more swiftly change to handle quick transients. This algorithm is thus well-suited for stiff solvers where this can be expected, and is the default for algorithms like the (E)SDIRK methods.\n\ngamma = integrator.opts.gamma\nniters = integrator.cache.newton_iters\nfac = min(gamma,\n    (1 + 2 * integrator.alg.max_newton_iter) * gamma /\n    (niters + 2 * integrator.alg.max_newton_iter))\nexpo = 1 / (alg_order(integrator.alg) + 1)\nqtmp = (integrator.EEst^expo) / fac\n@fastmath q = max(inv(integrator.opts.qmax), min(inv(integrator.opts.qmin), qtmp))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n    q = one(q)\nend\nintegrator.qold = q\nq\n\nIn this case, niters is the number of Newton iterations which was required in the most recent step of the algorithm. Note that these values are used differently depending on acceptance and rejection. When the step is accepted, the following logic is applied:\n\nif integrator.success_iter > 0\n    expo = 1 / (alg_adaptive_order(integrator.alg) + 1)\n    qgus = (integrator.dtacc / integrator.dt) *\n           (((integrator.EEst^2) / integrator.erracc)^expo)\n    qgus = max(inv(integrator.opts.qmax),\n        min(inv(integrator.opts.qmin), qgus / integrator.opts.gamma))\n    qacc = max(q, qgus)\nelse\n    qacc = q\nend\nintegrator.dtacc = integrator.dt\nintegrator.erracc = max(1e-2, integrator.EEst)\nintegrator.dt / qacc\n\nWhen it rejects, it is the same as the proportional control:\n\nif integrator.success_iter == 0\n    integrator.dt *= 0.1\nelse\n    integrator.dt = integrator.dt / integrator.qold\nend","category":"section"},{"location":"extras/timestepping/#Abstract-Controller","page":"Timestepping Method Descriptions","title":"Abstract Controller","text":"The AbstractController type allows one to implement custom adaptive timestepping schemes easily. This can be useful if one wants to use a priori error estimates, for instance.\n\nTo implement a custom controller, subtype AbstractController for the specific DE system.\n\nstruct CustomController <: AbstractController\nend\n\nand overload\n\nfunction stepsize_controller!(integrator, controller::CustomController, alg)\n    ...\n    nothing\nend\nfunction step_accept_controller!(integrator, controller::CustomController, alg)\n    ...\n    nothing\nend\nfunction step_reject_controller!(integrator, controller::CustomController, alg)\n    ...\n    nothing\nend\n\nFor instance, the PI controller for SDEs can be reproduced by\n\nstruct CustomController <: StochasticDiffEq.AbstractController\nend\n\nfunction StochasticDiffEq.stepsize_controller!(integrator::StochasticDiffEq.SDEIntegrator,\n        controller::CustomController, alg)\n    integrator.q11 = DiffEqBase.value(FastPower.fastpower(\n        integrator.EEst, controller.beta1))\n    integrator.q = DiffEqBase.value(integrator.q11 /\n                                    FastPower.fastpower(integrator.qold, controller.beta2))\n    integrator.q = DiffEqBase.value(max(inv(integrator.opts.qmax),\n        min(inv(integrator.opts.qmin),\n            integrator.q / integrator.opts.gamma)))\n    nothing\nend\n\nfunction StochasticDiffEq.step_accept_controller!(\n        integrator::StochasticDiffEq.SDEIntegrator,\n        controller::CustomController, alg)\n    integrator.dtnew = DiffEqBase.value(integrator.dt / integrator.q) *\n                       oneunit(integrator.dt)\n    nothing\nend\n\nfunction step_reject_controller!(integrator::StochasticDiffEq.SDEIntegrator,\n        controller::CustomController, alg)\n    integrator.dtnew = integrator.dt / min(inv(integrator.opts.qmin),\n        integrator.q11 / integrator.opts.gamma)\nend\n\nand used via\n\n# Assuming: import DifferentialEquations as DE\nsol = DE.solve(prob, DE.EM(), dt = dt, adaptive = true, controller = CustomController())","category":"section"},{"location":"features/ensemble/#ensemble","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Performing Monte Carlo simulations, solving with a predetermined set of initial conditions, and GPU-parallelizing a parameter search all fall under the ensemble simulation interface. This interface allows one to declare a template DEProblem to parallelize, tweak the template in trajectories for many trajectories, solve each in parallel batches, reduce the solutions down to specific answers, and compute summary statistics on the results.","category":"section"},{"location":"features/ensemble/#Performing-an-Ensemble-Simulation","page":"Parallel Ensemble Simulations","title":"Performing an Ensemble Simulation","text":"","category":"section"},{"location":"features/ensemble/#Building-a-Problem","page":"Parallel Ensemble Simulations","title":"Building a Problem","text":"To perform a simulation on an ensemble of trajectories, define a EnsembleProblem. The constructor is:\n\nEnsembleProblem(prob::DEProblem;\n    output_func = (sol, i) -> (sol, false),\n    prob_func = (prob, i, repeat) -> (prob),\n    reduction = (u, data, I) -> (append!(u, data), false),\n    u_init = [], safetycopy = prob_func !== DEFAULT_PROB_FUNC)\n\noutput_func: The function determines what is saved from the solution to the output array. Defaults to saving the solution itself. The output is (out,rerun) where out is the output and rerun is a boolean which designates whether to rerun.\nprob_func: The function by which the problem is to be modified. prob is the problem, i is the unique id 1:trajectories for the problem, and repeat is the iteration of the repeat. At first, it is 1, but if rerun was true this will be 2, 3, etc. counting the number of times problem i has been repeated.\nreduction: This function determines how to reduce the data in each batch. Defaults to appending the data into u, initialised via u_init, from the batches. I is a range of indices giving the trajectories corresponding to the batches. The second part of the output determines whether the simulation has converged. If true, the simulation will exit early. By default, this is always false.\nu_init: The initial form of the object that gets updated in-place inside the reduction function.\nsafetycopy: Determines whether a safety deepcopy is called on the prob before the prob_func. By default, this is true for any user-given prob_func, as without this, modifying the arguments of something in the prob_func, such as parameters or caches stored within the user function, are not necessarily thread-safe. If you know that your function is thread-safe, then setting this to false can improve performance when used with threads. For nested problems, e.g., SDE problems with custom noise processes, deepcopy might be insufficient. In such cases, use a custom prob_func.\n\nOne can specify a function prob_func which changes the problem. For example:\n\nfunction prob_func(prob, i, repeat)\n    @. prob.u0 = randn() * prob.u0\n    prob\nend\n\nmodifies the initial condition for all of the problems by a standard normal random number (a different random number per simulation). Notice that since problem types are immutable, it uses .=. Otherwise, one can just create a new problem type:\n\nfunction prob_func(prob, i, repeat)\n    @. prob.u0 = u0_arr[i]\n    prob\nend\n\nIf your function is a ParameterizedFunction, you can do similar modifications to prob.f to perform a parameter search. The output_func is a reduction function. Its arguments are the generated solution and the unique index for the run. For example, if we wish to only save the 2nd coordinate at the end of each solution, we can do:\n\noutput_func(sol, i) = (sol[end, 2], false)\n\nThus, the ensemble simulation would return as its data an array which is the end value of the 2nd dependent variable for each of the runs.","category":"section"},{"location":"features/ensemble/#Solving-the-Problem","page":"Parallel Ensemble Simulations","title":"Solving the Problem","text":"sim = solve(prob, alg, ensemblealg, kwargs...)\n\nThe keyword arguments take in the arguments for the common solver interface and will pass them to the differential equation solver. The ensemblealg is optional, and will default to EnsembleThreads(). The special keyword arguments to note are:\n\ntrajectories: The number of simulations to run. This argument is required.\nbatch_size : The size of the batches on which the reductions are applies. Defaults to trajectories.\npmap_batch_size: The size of the pmap batches. Default is batch_size÷100 > 0 ? batch_size÷100 : 1","category":"section"},{"location":"features/ensemble/#EnsembleAlgorithms","page":"Parallel Ensemble Simulations","title":"EnsembleAlgorithms","text":"The choice of ensemble algorithm allows for control over how the multiple trajectories are handled. Currently, the ensemble algorithm types are:\n\nEnsembleSerial() - No parallelism\nEnsembleThreads() - The default. This uses multithreading. It's local (single computer, shared memory) parallelism only. Fastest when the trajectories are quick.\nEnsembleDistributed() - Uses pmap internally. It will use as many processors as you have Julia processes. To add more processes, use addprocs(n). See Julia's documentation for more details. Recommended for the case when each trajectory calculation isn't “too quick” (at least about a millisecond each?).\nEnsembleSplitThreads() - This uses threading on each process, splitting the problem into nprocs() even parts. This is for solving many quick trajectories on a multi-node machine. It's recommended you have one process on each node.\nEnsembleGPUArray() - Requires installing and using DiffEqGPU. This uses a GPU for computing the ensemble with hyperparallelism. It will automatically recompile your Julia functions to the GPU. A standard GPU sees a 5x performance increase over a 16 core Xeon CPU. However, there are limitations on what functions can auto-compile in this fashion, please see the DiffEqGPU README for more details\n\nFor example, EnsembleThreads() is invoked by:\n\nDE.solve(ensembleprob, alg, DE.EnsembleThreads(); trajectories = 1000)","category":"section"},{"location":"features/ensemble/#Solution-Type","page":"Parallel Ensemble Simulations","title":"Solution Type","text":"The resulting type is a EnsembleSimulation, which includes the array of solutions.","category":"section"},{"location":"features/ensemble/#Plot-Recipe","page":"Parallel Ensemble Simulations","title":"Plot Recipe","text":"There is a plot recipe for a AbstractEnsembleSimulation which composes all of the plot recipes for the component solutions. The keyword arguments are passed along. A useful argument to use is linealpha which will change the transparency of the plots. An additional argument is idxs which allows you to choose which components of the solution to plot. For example, if the differential equation is a vector of 9 values, idxs=1:2:9 will plot only the solutions of the odd components. Another additional argument is zcolors (an alias of marker_z) which allows you to pass a zcolor for each series. For details about zcolor see the Series documentation for Plots.jl.","category":"section"},{"location":"features/ensemble/#Analyzing-an-Ensemble-Experiment","page":"Parallel Ensemble Simulations","title":"Analyzing an Ensemble Experiment","text":"Analysis tools are included for generating summary statistics and summary plots for a EnsembleSimulation.\n\nTo use this functionality, import the analysis module via:\n\nimport DifferentialEquations as DE\n# Access the analysis module via DE.EnsembleAnalysis\n\n(or more directly import SciMLBase; SciMLBase.EnsembleAnalysis).","category":"section"},{"location":"features/ensemble/#Time-steps-vs-time-points","page":"Parallel Ensemble Simulations","title":"Time steps vs time points","text":"For the summary statistics, there are two types. You can either summarize by time steps or by time points. Summarizing by time steps assumes that the time steps are all the same time point, i.e. the integrator used a fixed dt or the values were saved using saveat. Summarizing by time points requires interpolating the solution.","category":"section"},{"location":"features/ensemble/#Analysis-at-a-time-step-or-time-point","page":"Parallel Ensemble Simulations","title":"Analysis at a time step or time point","text":"get_timestep(sim, i) # Returns an iterator of each simulation at time step i\nget_timepoint(sim, t) # Returns an iterator of each simulation at time point t\ncomponentwise_vectors_timestep(sim, i) # Returns a vector of each simulation at time step i\ncomponentwise_vectors_timepoint(sim, t) # Returns a vector of each simulation at time point t","category":"section"},{"location":"features/ensemble/#Summary-Statistics","page":"Parallel Ensemble Simulations","title":"Summary Statistics","text":"","category":"section"},{"location":"features/ensemble/#Single-Time-Statistics","page":"Parallel Ensemble Simulations","title":"Single Time Statistics","text":"The available functions for time steps are:\n\ntimestep_mean(sim, i) # Computes the mean of each component at time step i\ntimestep_median(sim, i) # Computes the median of each component at time step i\ntimestep_quantile(sim, q, i) # Computes the quantile q of each component at time step i\ntimestep_meanvar(sim, i)  # Computes the mean and variance of each component at time step i\ntimestep_meancov(sim, i, j) # Computes the mean at i and j, and the covariance, for each component\ntimestep_meancor(sim, i, j) # Computes the mean at i and j, and the correlation, for each component\ntimestep_weighted_meancov(sim, W, i, j) # Computes the mean at i and j, and the weighted covariance W, for each component\n\nThe available functions for time points are:\n\ntimepoint_mean(sim, t) # Computes the mean of each component at time t\ntimepoint_median(sim, t) # Computes the median of each component at time t\ntimepoint_quantile(sim, q, t) # Computes the quantile q of each component at time t\ntimepoint_meanvar(sim, t) # Computes the mean and variance of each component at time t\ntimepoint_meancov(sim, t1, t2) # Computes the mean at t1 and t2, the covariance, for each component\ntimepoint_meancor(sim, t1, t2) # Computes the mean at t1 and t2, the correlation, for each component\ntimepoint_weighted_meancov(sim, W, t1, t2) # Computes the mean at t1 and t2, the weighted covariance W, for each component","category":"section"},{"location":"features/ensemble/#Full-Timeseries-Statistics","page":"Parallel Ensemble Simulations","title":"Full Timeseries Statistics","text":"Additionally, the following functions are provided for analyzing the full timeseries. The mean and meanvar versions return a DiffEqArray which can be directly plotted. The meancov and meancor return a matrix of tuples, where the tuples are the (mean_t1,mean_t2,cov or cor).\n\nThe available functions for the time steps are:\n\ntimeseries_steps_mean(sim) # Computes the mean at each time step\ntimeseries_steps_median(sim) # Computes the median at each time step\ntimeseries_steps_quantile(sim, q) # Computes the quantile q at each time step\ntimeseries_steps_meanvar(sim) # Computes the mean and variance at each time step\ntimeseries_steps_meancov(sim) # Computes the covariance matrix and means at each time step\ntimeseries_steps_meancor(sim) # Computes the correlation matrix and means at each time step\ntimeseries_steps_weighted_meancov(sim) # Computes the weighted covariance matrix and means at each time step\n\nThe available functions for the time points are:\n\ntimeseries_point_mean(sim, ts) # Computes the mean at each time point in ts\ntimeseries_point_median(sim, ts) # Computes the median at each time point in ts\ntimeseries_point_quantile(sim, q, ts) # Computes the quantile q at each time point in ts\ntimeseries_point_meanvar(sim, ts) # Computes the mean and variance at each time point in ts\ntimeseries_point_meancov(sim, ts) # Computes the covariance matrix and means at each time point in ts\ntimeseries_point_meancor(sim, ts) # Computes the correlation matrix and means at each time point in ts\ntimeseries_point_weighted_meancov(sim, ts) # Computes the weighted covariance matrix and means at each time point in ts","category":"section"},{"location":"features/ensemble/#EnsembleSummary","page":"Parallel Ensemble Simulations","title":"EnsembleSummary","text":"The EnsembleSummary type is included to help with analyzing the general summary statistics. Two constructors are provided:\n\nEnsembleSummary(sim; quantiles = [0.05, 0.95])\nEnsembleSummary(sim, ts; quantiles = [0.05, 0.95])\n\nThe first produces a (mean,var) summary at each time step. As with the summary statistics, this assumes that the time steps are all the same. The second produces a (mean,var) summary at each time point t in ts. This requires the ability to interpolate the solution. Quantile is used to determine the qlow and qhigh quantiles at each timepoint. It defaults to the 5% and 95% quantiles.","category":"section"},{"location":"features/ensemble/#Plot-Recipe-2","page":"Parallel Ensemble Simulations","title":"Plot Recipe","text":"The EnsembleSummary comes with a plot recipe for visualizing the summary statistics. The extra keyword arguments are:\n\nidxs: the solution components to plot. Defaults to plotting all components.\nerror_style: The style for plotting the error. Defaults to ribbon. Other choices are :bars for error bars and :none for no error bars.\nci_type : Defaults to :quantile which has (qlow,qhigh) quantiles whose limits were determined when constructing the EnsembleSummary. Gaussian CI 1.96*(standard error of the mean) can be set using ci_type=:SEM.\n\nOne useful argument is fillalpha which controls the transparency of the ribbon around the mean.","category":"section"},{"location":"features/ensemble/#Example-1:-Solving-an-ODE-With-Different-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Example 1: Solving an ODE With Different Initial Conditions","text":"","category":"section"},{"location":"features/ensemble/#Random-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Random Initial Conditions","text":"Let's test the sensitivity of the linear ODE to its initial condition. To do this, we would like to solve the linear ODE 100 times and plot what the trajectories look like. Let's start by opening up some extra processes so that way the computation will be parallelized. Here we will choose to use distributed parallelism, which means that the required functions must be made available to all processes. This can be achieved with @everywhere macro:\n\nimport Distributed\nimport DifferentialEquations as DE\nimport Plots\n\nDistributed.addprocs()\n@everywhere import DifferentialEquations as DE\n\nNow let's define the linear ODE, which is our base problem:\n\n# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = DE.ODEProblem((u, p, t) -> 1.01u, 0.5, (0.0, 1.0))\n\nFor our ensemble simulation, we would like to change the initial condition around. This is done through the prob_func. This function takes in the base problem and modifies it to create the new problem that the trajectory actually solves. Here, we will take the base problem, multiply the initial condition by a rand(), and use that for calculating the trajectory:\n\n@everywhere function prob_func(prob, i, repeat)\n    DE.remake(prob, u0 = rand() * prob.u0)\nend\n\nNow we build and solve the EnsembleProblem with this base problem and prob_func:\n\nensemble_prob = DE.EnsembleProblem(prob, prob_func = prob_func)\nsim = DE.solve(ensemble_prob, DE.Tsit5(), DE.EnsembleDistributed(), trajectories = 10)\n\nWe can use the plot recipe to plot what the 10 ODEs look like:\n\nPlots.plot(sim, linealpha = 0.4)\n\nWe note that if we wanted to find out what the initial condition was for a given trajectory, we can retrieve it from the solution. sim[i] returns the ith solution object. sim[i].prob is the problem that specific trajectory solved, and sim[i].prob.u0 would then be the initial condition used in the ith trajectory.\n\nNote: If the problem has callbacks, the functions for the condition and affect! must be named functions (not anonymous functions).","category":"section"},{"location":"features/ensemble/#Using-multithreading","page":"Parallel Ensemble Simulations","title":"Using multithreading","text":"The previous ensemble simulation can also be parallelized using a multithreading approach, which will make use of the different cores within a single computer. Because the memory is shared across the different threads, it is not necessary to use the @everywhere macro. Instead, the same problem can be implemented simply as:\n\nimport DifferentialEquations as DE\nprob = DE.ODEProblem((u, p, t) -> 1.01u, 0.5, (0.0, 1.0))\nfunction prob_func(prob, i, repeat)\n    DE.remake(prob, u0 = rand() * prob.u0)\nend\nensemble_prob = DE.EnsembleProblem(prob, prob_func = prob_func)\nsim = DE.solve(ensemble_prob, DE.Tsit5(), DE.EnsembleThreads(), trajectories = 10)\nimport Plots;\nPlots.plot(sim);\n\nThe number of threads to be used has to be defined outside of Julia, in the environmental variable JULIA_NUM_THREADS (see Julia's documentation for details).","category":"section"},{"location":"features/ensemble/#Pre-Determined-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Pre-Determined Initial Conditions","text":"Often, you may already know what initial conditions you want to use. This can be specified by the i argument of the prob_func. This i is the unique index of each trajectory. So, if we have trajectories=100, then we have i as some index in 1:100, and it's different for each trajectory.\n\nSo, if we wanted to use a grid of evenly spaced initial conditions from 0 to 1, we could simply index the linspace type:\n\ninitial_conditions = range(0, stop = 1, length = 100)\nfunction prob_func(prob, i, repeat)\n    DE.remake(prob, u0 = initial_conditions[i])\nend\n\nIt's worth noting that if you run this code successfully, there will be no visible output.","category":"section"},{"location":"features/ensemble/#Example-2:-Solving-an-SDE-with-Different-Parameters","page":"Parallel Ensemble Simulations","title":"Example 2: Solving an SDE with Different Parameters","text":"Let's solve the same SDE, but with varying parameters. Let's create a Lotka-Volterra system with multiplicative noise. Our Lotka-Volterra system will have as its drift component:\n\nfunction f(du, u, p, t)\n    du[1] = p[1] * u[1] - p[2] * u[1] * u[2]\n    du[2] = -3 * u[2] + u[1] * u[2]\nend\n\nFor our noise function, we will use multiplicative noise:\n\nfunction g(du, u, p, t)\n    du[1] = p[3] * u[1]\n    du[2] = p[4] * u[2]\nend\n\nNow we build the SDE with these functions:\n\nimport DifferentialEquations as DE\np = [1.5, 1.0, 0.1, 0.1]\nprob = DE.SDEProblem(f, g, [1.0, 1.0], (0.0, 10.0), p)\n\nThis is the base problem for our study. What would like to do with this experiment is keep the same parameters in the deterministic component each time, but vary the parameters for the amount of noise using 0.3rand(2) as our parameters. Once again, we do this with a prob_func, and here we modify the parameters in prob.p:\n\n# `p` is a global variable, referencing it would be type unstable.\n# Using a let block defines a small local scope in which we can\n# capture that local `p` which isn't redefined anywhere in that local scope.\n# This allows it to be type stable.\nprob_func = let p = p\n    (prob, i, repeat) -> begin\n        x = 0.3rand(2)\n        DE.remake(prob, p = [p[1], p[2], x[1], x[2]])\n    end\nend\n\nNow we solve the problem 10 times and plot all of the trajectories in phase space:\n\nensemble_prob = DE.EnsembleProblem(prob, prob_func = prob_func)\nsim = DE.solve(ensemble_prob, DE.SRIW1(), trajectories = 10);\nnothing # hide\n\nimport Plots;\nPlots.plot(sim, linealpha = 0.6, color = :blue, idxs = (0, 1), title = \"Phase Space Plot\");\nPlots.plot!(sim, linealpha = 0.6, color = :red, idxs = (0, 2), title = \"Phase Space Plot\")\n\nWe can then summarize this information with the mean/variance bounds using a EnsembleSummary plot. We will take the mean/quantile at every 0.1 time units and directly plot the summary:\n\nsumm = DE.EnsembleSummary(sim, 0:0.1:10)\nPlots.plot(summ, fillalpha = 0.5)\n\nNote that here we used the quantile bounds, which default to [0.05,0.95] in the EnsembleSummary constructor. We can change to standard error of the mean bounds using ci_type=:SEM in the plot recipe.","category":"section"},{"location":"features/ensemble/#Example-3:-Using-the-Reduction-to-Halt-When-Estimator-is-Within-Tolerance","page":"Parallel Ensemble Simulations","title":"Example 3: Using the Reduction to Halt When Estimator is Within Tolerance","text":"In this problem, we will solve the equation just as many times as needed to get the standard error of the mean for the final time point below our tolerance 0.5. Since we only care about the endpoint, we can tell the output_func to discard the rest of the data.\n\nfunction output_func(sol, i)\n    last(sol), false\nend\n\nOur prob_func will simply randomize the initial condition:\n\nimport DifferentialEquations as DE\n# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = DE.ODEProblem((u, p, t) -> 1.01u, 0.5, (0.0, 1.0))\n\nfunction prob_func(prob, i, repeat)\n    DE.remake(prob, u0 = rand() * prob.u0)\nend\n\nOur reduction function will append the data from the current batch to the previous batch, and declare convergence if the standard error of the mean is calculated as sufficiently small:\n\nimport Statistics\nfunction reduction(u, batch, I)\n    u = append!(u, batch)\n    finished = (Statistics.var(u) / sqrt(last(I))) / Statistics.mean(u) < 0.5\n    u, finished\nend\n\nThen we can define and solve the problem:\n\nprob2 = DE.EnsembleProblem(prob, prob_func = prob_func, output_func = output_func,\n    reduction = reduction, u_init = Vector{Float64}())\nsim = DE.solve(prob2, DE.Tsit5(), trajectories = 10000, batch_size = 20)\n\nSince batch_size=20, this means that every 20 simulations, it will take this batch, append the results to the previous batch, calculate (var(u)/sqrt(last(I)))/mean(u), and if that's small enough, exit the simulation. In this case, the simulation exits only after 20 simulations (i.e. after calculating the first batch). This can save a lot of time!\n\nIn addition to saving time by checking convergence, we can save memory by reducing between batches. For example, say we only care about the mean at the end once again. Instead of saving the solution at the end for each trajectory, we can instead save the running summation of the endpoints:\n\nfunction reduction(u, batch, I)\n    u + sum(batch), false\nend\nprob2 = DE.EnsembleProblem(prob, prob_func = prob_func, output_func = output_func,\n    reduction = reduction, u_init = 0.0)\nsim2 = DE.solve(prob2, DE.Tsit5(), trajectories = 100, batch_size = 20)\n\nthis will sum up the endpoints after every 20 solutions, and save the running sum. The final result will have sim2.u as simply a number, and thus sim2.u/100 would be the mean.","category":"section"},{"location":"features/ensemble/#Example-4:-Using-the-Analysis-Tools","page":"Parallel Ensemble Simulations","title":"Example 4: Using the Analysis Tools","text":"In this example, we will show how to analyze a EnsembleSolution. First, let's generate a 10 solution Monte Carlo experiment. For our problem, we will use a 4x2 system of linear stochastic differential equations:\n\nfunction f(du, u, p, t)\n    for i in 1:length(u)\n        du[i] = 1.01 * u[i]\n    end\nend\nfunction σ(du, u, p, t)\n    for i in 1:length(u)\n        du[i] = 0.87 * u[i]\n    end\nend\nimport DifferentialEquations as DE\nprob = DE.SDEProblem(f, σ, ones(4, 2) / 2, (0.0, 1.0)) #prob_sde_2Dlinear\n\nTo solve this 10 times, we use the EnsembleProblem constructor and solve with trajectories=10. Since we wish to compare values at the timesteps, we need to make sure the steps all hit the same times. We thus set adaptive=false and explicitly give a dt.\n\nprob2 = DE.EnsembleProblem(prob)\nsim = DE.solve(prob2, DE.SRIW1(), dt = 1 // 2^(3), trajectories = 10, adaptive = false);\n@info \"Ensemble solution computed with $(length(sim)) trajectories\" # hide\nnothing # hide\n\nNote that if you don't do the timeseries_steps calculations, this code is compatible with adaptive timestepping. Using adaptivity is usually more efficient!\n\nWe can compute the mean and the variance at the 3rd timestep using:\n\nimport DifferentialEquations as DE\nm, v = DE.EnsembleAnalysis.timestep_meanvar(sim, 3)\n\nor we can compute the mean and the variance at the t=0.5 using:\n\nm, v = DE.EnsembleAnalysis.timepoint_meanvar(sim, 0.5)\n\nWe can get a series for the mean and the variance at each time step using:\n\nm_series, v_series = DE.EnsembleAnalysis.timeseries_steps_meanvar(sim)\n\nor at chosen values of t:\n\nts = 0:0.1:1\nm_series = DE.EnsembleAnalysis.timeseries_point_mean(sim, ts)\n\nNote that these mean and variance series can be directly plotted. We can compute covariance matrices similarly:\n\nDE.EnsembleAnalysis.timeseries_steps_meancov(sim) # Use the time steps, assume fixed dt\nDE.EnsembleAnalysis.timeseries_point_meancov(sim, 0:(1 // 2 ^ (3)):1, 0:(1 // 2 ^ (3)):1) # Use time points, interpolate\n\nFor general analysis, we can build a EnsembleSummary type.\n\nsumm = DE.EnsembleSummary(sim)\n\nwill summarize at each time step, while\n\nsumm = DE.EnsembleSummary(sim, 0.0:0.1:1.0)\n\nwill summarize at the 0.1 time points using the interpolations. To visualize the results, we can plot it. Since there are 8 components to the differential equation, this can get messy, so let's only plot the 3rd component:\n\nimport Plots;\nPlots.plot(summ; idxs = 3);\n\nWe can change to errorbars instead of ribbons and plot two different indices:\n\nPlots.plot(summ; idxs = (3, 5), error_style = :bars)\n\nOr we can simply plot the mean of every component over time:\n\nPlots.plot(summ; error_style = :none)","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#OrdinaryDiffEqTaylorSeries","page":"OrdinaryDiffEqTaylorSeries","title":"OrdinaryDiffEqTaylorSeries","text":"Taylor series methods for ordinary differential equations using automatic differentiation. These methods achieve very high-order accuracy by computing Taylor expansions of the solution using automatic differentiation techniques through TaylorDiff.jl.\n\nwarn: Development Status\nThese methods are still in development and may not be fully optimized or reliable for production use.","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Key-Properties","page":"OrdinaryDiffEqTaylorSeries","title":"Key Properties","text":"Taylor series methods provide:\n\nVery high-order accuracy with arbitrary order capability\nAutomatic differentiation for derivative computation\nStep size control through Taylor series truncation\nNatural error estimation from higher-order terms\nExcellent accuracy for smooth problems\nSingle-step methods without requiring history","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#When-to-Use-Taylor-Series-Methods","page":"OrdinaryDiffEqTaylorSeries","title":"When to Use Taylor Series Methods","text":"These methods are recommended for:\n\nUltra-high precision problems where maximum accuracy is needed\nSmooth problems with well-behaved derivatives\nScientific computing requiring very low error tolerances\nProblems with expensive function evaluations where high-order methods reduce total steps","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Mathematical-Background","page":"OrdinaryDiffEqTaylorSeries","title":"Mathematical Background","text":"Taylor series methods compute the solution using Taylor expansions: u(t + h) = u(t) + h*u'(t) + h²/2!*u''(t) + h³/3!*u'''(t) + ...\n\nThe derivatives are computed automatically using automatic differentiation, allowing arbitrary-order methods without manual derivative computation.","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Solver-Selection-Guide","page":"OrdinaryDiffEqTaylorSeries","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Available-Methods","page":"OrdinaryDiffEqTaylorSeries","title":"Available Methods","text":"ExplicitTaylor2: Second-order Taylor series method for moderate accuracy\nExplicitTaylor: Arbitrary-order Taylor series method (specify order with order = Val{p}())","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Usage-considerations","page":"OrdinaryDiffEqTaylorSeries","title":"Usage considerations","text":"Smooth problems only: Methods assume the function has many continuous derivatives\nComputational cost: Higher orders require more automatic differentiation computations\nMemory requirements: Higher orders store more derivative information","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Performance-Guidelines","page":"OrdinaryDiffEqTaylorSeries","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#When-Taylor-series-methods-excel","page":"OrdinaryDiffEqTaylorSeries","title":"When Taylor series methods excel","text":"Very smooth problems where high-order derivatives exist and are well-behaved\nHigh precision requirements beyond standard double precision\nLong-time integration where accumulated error matters\nProblems where function evaluations dominate computational cost","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Problem-characteristics","page":"OrdinaryDiffEqTaylorSeries","title":"Problem characteristics","text":"Polynomial and analytic functions work extremely well\nSmooth ODEs from physics simulations\nProblems requiring very low tolerances (< 1e-12)","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Limitations-and-Considerations","page":"OrdinaryDiffEqTaylorSeries","title":"Limitations and Considerations","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Method-limitations","page":"OrdinaryDiffEqTaylorSeries","title":"Method limitations","text":"Requires smooth functions - non-smooth problems may cause issues\nMemory overhead for storing multiple derivatives\nLimited to problems where high-order derivatives are meaningful\nAutomatic differentiation compatibility - requires functions compatible with TaylorDiff.jl and Symbolics.jl tracing\nLong compile times due to automatic differentiation and symbolic processing overhead","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#When-to-consider-alternatives","page":"OrdinaryDiffEqTaylorSeries","title":"When to consider alternatives","text":"Non-smooth problems: Use adaptive Runge-Kutta methods instead\nStiff problems: Taylor methods are explicit and may be inefficient\nLarge systems: Automatic differentiation cost may become prohibitive\nStandard accuracy needs: Lower-order methods are often sufficient","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Alternative-Approaches","page":"OrdinaryDiffEqTaylorSeries","title":"Alternative Approaches","text":"Consider these alternatives:\n\nHigh-order Runge-Kutta methods (Feagin, Verner) for good accuracy with less overhead\nExtrapolation methods for high accuracy with standard function evaluations\nAdaptive methods for problems with varying smoothness\nImplicit methods for stiff problems requiring high accuracy","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Installation-and-Usage","page":"OrdinaryDiffEqTaylorSeries","title":"Installation and Usage","text":"Taylor series methods require explicit installation of the specialized library:\n\nusing Pkg\nPkg.add(\"OrdinaryDiffEqTaylorSeries\")\n\nThen use the methods with:\n\nusing OrdinaryDiffEqTaylorSeries\n\n# Example: Second-order Taylor method\nfunction f(u, p, t)\n    σ, ρ, β = p\n    du1 = σ * (u[2] - u[1])\n    du2 = u[1] * (ρ - u[3]) - u[2]\n    du3 = u[1] * u[2] - β * u[3]\n    [du1, du2, du3]\nend\n\nu0 = [1.0, 0.0, 0.0]\ntspan = (0.0, 10.0)\np = [10.0, 28.0, 8/3]\nprob = ODEProblem(f, u0, tspan, p)\n\n# Second-order Taylor method\nsol = solve(prob, ExplicitTaylor2())\n\n# Arbitrary-order Taylor method (e.g., 8th order)\nsol = solve(prob, ExplicitTaylor(order = Val{8}()))","category":"section"},{"location":"api/ordinarydiffeq/explicit/TaylorSeries/#Full-list-of-solvers","page":"OrdinaryDiffEqTaylorSeries","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#OrdinaryDiffEqTsit5","page":"OrdinaryDiffEqTsit5","title":"OrdinaryDiffEqTsit5","text":"The Tsitouras 5/4 method is the recommended default solver for most non-stiff differential equation problems. This method provides an excellent balance of efficiency, reliability, and accuracy.","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#Key-Properties","page":"OrdinaryDiffEqTsit5","title":"Key Properties","text":"Tsit5 offers:\n\nFifth-order accuracy with embedded fourth-order error estimation\nExcellent efficiency at default tolerances (1e-6 to 1e-3)\nFSAL (First Same As Last) property for computational efficiency\nHigh-quality interpolation for dense output\nRobust performance across a wide range of problem types\nOptimized coefficients for minimal error in practical applications","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#When-to-Use-Tsit5","page":"OrdinaryDiffEqTsit5","title":"When to Use Tsit5","text":"Tsit5 is recommended for:\n\nMost non-stiff problems as the first choice solver\nDefault and higher tolerances (1e-3 to 1e-6)\nGeneral-purpose integration when problem characteristics are unknown\nEducational and research applications as a reliable baseline\nReal-time applications requiring predictable performance\nProblems where simplicity and reliability are preferred over maximum efficiency","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#Solver-Selection-Guide","page":"OrdinaryDiffEqTsit5","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#Primary-recommendation","page":"OrdinaryDiffEqTsit5","title":"Primary recommendation","text":"Tsit5: Use as the default choice for non-stiff problems at standard tolerances","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#Automatic-switching","page":"OrdinaryDiffEqTsit5","title":"Automatic switching","text":"AutoTsit5: Automatically switches to a stiff solver when stiffness is detected, making it robust for problems of unknown character","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#When-to-Consider-Alternatives","page":"OrdinaryDiffEqTsit5","title":"When to Consider Alternatives","text":"Consider other solvers when:\n\nHigher accuracy needed: Use Verner methods (Vern6, Vern7, Vern8, Vern9) for tolerances below 1e-6\nHigher tolerances: Use BS3 or OwrenZen3 for tolerances above 1e-3\nRobust error control needed: Use BS5 when Tsit5 struggles with error estimation\nEquation is stiff: Use implicit methods (SDIRK, BDF) or semi-implicit methods (Rosenbrock) for stiff problems\nSpecial properties required: Use specialized methods (SSP, symplectic, etc.) for specific problem types\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqTsit5\", \"Tsit5\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#Full-list-of-solvers","page":"OrdinaryDiffEqTsit5","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#OrdinaryDiffEqTsit5.Tsit5","page":"OrdinaryDiffEqTsit5","title":"OrdinaryDiffEqTsit5.Tsit5","text":"Tsit5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Tsitouras 5/4 Runge-Kutta method. (free 4th order interpolant). Recommended for most non-stiff problems. Good default choice for unknown stiffness. Highly efficient and generic. Very good performance for most non-stiff ODEs. Recommended as default method for unknown stiffness problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{tsitouras2011runge,     title={Runge–Kutta pairs of order 5 (4) satisfying only the first column simplifying assumption},     author={Tsitouras, Ch},     journal={Computers \\& Mathematics with Applications},     volume={62},     number={2},     pages={770–775},     year={2011},     publisher={Elsevier},     doi={10.1016/j.camwa.2011.06.002}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Tsit5/#OrdinaryDiffEqTsit5.AutoTsit5","page":"OrdinaryDiffEqTsit5","title":"OrdinaryDiffEqTsit5.AutoTsit5","text":"Automatic switching algorithm that can switch between the (non-stiff) Tsit5() and stiff_alg.\n\nAutoTsit5(stiff_alg; kwargs...)\n\nThis method is equivalent to AutoAlgSwitch(Tsit5(), stiff_alg; kwargs...). To gain access to stiff algorithms you might have to install additional libraries, such as OrdinaryDiffEqRosenbrock.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/implicit/PDIRK/#OrdinaryDiffEqPDIRK","page":"OrdinaryDiffEqPDIRK","title":"OrdinaryDiffEqPDIRK","text":"PDIRK methods are parallel DIRK methods. SDIRK methods, or singly-diagonally implicit methods, have to build and solve a factorize a Jacobian of the form W = I-gammaJ where gamma is dependent on the chosen method. PDIRK methods use multiple different choices of gamma, i.e. W_i = I-gamma_iJ, which are all used in the update process. There are some advantages to this, as no SDIRK method can be a higher order than 5, while DIRK methods generally can have arbitrarily high order and lower error coefficients, leading to lower errors at larger dt sizes. With the right construction of the tableau, these matrices can be factorized and the underlying steps can be computed in parallel, which is why these are the parallel DIRK methods.\n\nwarning: Experimental\nOrdinaryDiffEqPDIRK is experimental, as there are no parallel DIRK tableaus that achieve good performance in the literature.\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqPDIRK\", \"PDIRK44\")","category":"section"},{"location":"api/ordinarydiffeq/implicit/PDIRK/#Full-list-of-solvers","page":"OrdinaryDiffEqPDIRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/PDIRK/#OrdinaryDiffEqPDIRK.PDIRK44","page":"OrdinaryDiffEqPDIRK","title":"OrdinaryDiffEqPDIRK.PDIRK44","text":"PDIRK44(; chunk_size = Val{0}(),\n          autodiff = AutoForwardDiff(),\n          standardtag = Val{true}(),\n          concrete_jac = nothing,\n          diff_type = Val{:forward},\n          linsolve = nothing,\n          precs = DEFAULT_PRECS,\n          nlsolve = NLNewton(),\n          extrapolant = :constant,\n          thread = OrdinaryDiffEq.True())\n\nParallel Diagonally Implicit Runge-Kutta Method. A 2 processor 4th order diagonally non-adaptive implicit method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify PDIRK44(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD,\nextrapolant: TBD,\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n\"@article{iserles1990theory, title={On the theory of parallel Runge—Kutta methods}, author={Iserles, Arieh and Norrsett, SP}, journal={IMA Journal of numerical Analysis}, volume={10}, number={4}, pages={463–488}, year={1990}, publisher={Oxford University Press}}\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/#StochasticDiffEq.jl","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"StochasticDiffEq.jl","text":"StochasticDiffEq.jl is a component package in the DifferentialEquations ecosystem for solving stochastic differential equations (SDEs) and random ordinary differential equations (RODEs). It provides a comprehensive suite of high-performance numerical methods for stochastic problems.","category":"section"},{"location":"api/stochasticdiffeq/#Installation","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Installation","text":"To install StochasticDiffEq.jl, use the Julia package manager:\n\nusing Pkg\nPkg.add(\"StochasticDiffEq\")","category":"section"},{"location":"api/stochasticdiffeq/#Quick-Start","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Quick Start","text":"Here's a simple example of solving an SDE:\n\nusing StochasticDiffEq\n\n# Define the drift and diffusion functions\nfunction drift!(du, u, p, t)\n    du[1] = 1.01 * u[1]\nend\n\nfunction diffusion!(du, u, p, t)\n    du[1] = 0.87 * u[1]\nend\n\n# Initial condition and time span\nu0 = [0.5]\ntspan = (0.0, 1.0)\n\n# Define the SDE problem\nprob = SDEProblem(drift!, diffusion!, u0, tspan)\n\n# Solve using the default algorithm\nsol = solve(prob)","category":"section"},{"location":"api/stochasticdiffeq/#Solver-Categories","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Solver Categories","text":"StochasticDiffEq.jl provides several categories of solvers optimized for different types of problems:","category":"section"},{"location":"api/stochasticdiffeq/#Nonstiff-Solvers","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Nonstiff Solvers","text":"Basic Methods: Euler-Maruyama, Heun methods\nSRA/SRI Methods: High-order adaptive methods (SOSRI, SOSRA)\nHigh Weak Order: Methods optimized for weak convergence (DRI1)\nCommutative Noise: Specialized methods for commuting noise terms","category":"section"},{"location":"api/stochasticdiffeq/#Stiff-Solvers","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Stiff Solvers","text":"Implicit Methods: Drift-implicit methods for stiff problems\nSplit-Step Methods: Methods handling stiffness in diffusion\nStabilized Methods: SROCK-type methods for parabolic PDEs","category":"section"},{"location":"api/stochasticdiffeq/#Jump-Diffusion","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Jump-Diffusion","text":"Tau-Leaping: Methods for jump-diffusion processes","category":"section"},{"location":"api/stochasticdiffeq/#Recommended-Methods","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Recommended Methods","text":"For most users, we recommend starting with these methods:\n\nGeneral Purpose: SOSRI() - Excellent for diagonal/scalar Itô SDEs\nAdditive Noise: SOSRA() - Optimal for problems with additive noise\nStiff Problems: SKenCarp() - Best for stiff problems with additive noise\nCommutative Noise: RKMilCommute() - For multi-dimensional commutative noise\nHigh Efficiency: EM() - When computational speed is most important","category":"section"},{"location":"api/stochasticdiffeq/#Advanced-Features","page":"StochasticDiffEq.jl: SDE solvers and utilities","title":"Advanced Features","text":"Adaptive time stepping with sophisticated error control\nSupport for all noise types (diagonal, non-diagonal, additive, scalar)\nBoth Itô and Stratonovich interpretations\nIntegration with the broader DifferentialEquations.jl ecosystem\nGPU compatibility for high-performance computing\nExtensive callback and event handling capabilities\n\nSee the individual solver pages for detailed information about each method's properties, when to use them, and their theoretical foundations.","category":"section"},{"location":"solvers/benchmarks/#Solver-Benchmarks","page":"Solver Benchmarks","title":"Solver Benchmarks","text":"Benchmarks for the solvers can be found at benchmarks.sciml.ai. The source code can be found at SciMLBenchmarks.jl. A variety of different problems are already tested. However, if you would like additional problems to be benchmarked, please open an issue or PR at the SciMLBenchmarks.jl repository with the code that defines the DEProblem.","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#OrdinaryDiffEqBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF","text":"BDF (Backward Differentiation Formula) methods for fully implicit differential-algebraic equations (DAEs) in the form F(du/dt, u, t) = 0. These methods provide robust integration for index-1 DAE systems with fully implicit formulations.\n\nwarn: Performance Consideration\nDFBDF and family have not been made fully efficient yet, and thus Sundials.jl IDA is recommended for production use.","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Key-Properties","page":"OrdinaryDiffEqBDF","title":"Key Properties","text":"Fully implicit DAE BDF methods provide:\n\nGeneral DAE capability for F(du/dt, u, t) = 0 formulations\nIndex-1 DAE support for properly formulated DAE systems\nRobust nonlinear solver integration for implicit equation systems\nHigh-order accuracy with excellent stability properties\nLarge stiff system capability with efficient linear algebra","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#When-to-Use-Fully-Implicit-DAE-BDF-Methods","page":"OrdinaryDiffEqBDF","title":"When to Use Fully Implicit DAE BDF Methods","text":"These methods are recommended for:\n\nFully implicit DAE systems where F(du/dt, u, t) = 0 cannot be easily rearranged\nIndex-1 DAE problems that cannot be easily rearranged to semi-explicit form\nMultibody dynamics with complex kinematic constraints\nElectrical circuits with ideal components and algebraic loops\nChemical engineering with equilibrium and conservation constraints\nLarge-scale DAE systems requiring robust implicit integration","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Mathematical-Background","page":"OrdinaryDiffEqBDF","title":"Mathematical Background","text":"Fully implicit DAEs have the general form: F(du/dt, u, t) = 0\n\nUnlike semi-explicit forms, these cannot be written as du/dt = f(u,t) even after constraint elimination. BDF methods discretize the time derivative using backward differences and solve the resulting nonlinear system at each timestep.","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Problem-Formulation","page":"OrdinaryDiffEqBDF","title":"Problem Formulation","text":"Use DAEProblem with implicit function specification:\n\nfunction f2(out, du, u, p, t)\n    out[1] = -0.04u[1] + 1e4 * u[2] * u[3] - du[1]\n    out[2] = +0.04u[1] - 3e7 * u[2]^2 - 1e4 * u[2] * u[3] - du[2]\n    out[3] = u[1] + u[2] + u[3] - 1.0\nend\nu₀ = [1.0, 0, 0]\ndu₀ = [-0.04, 0.04, 0.0]\ntspan = (0.0, 100000.0)\ndifferential_vars = [true, true, false]\nprob = DAEProblem(f2, du₀, u₀, tspan, differential_vars = differential_vars)\nsol = solve(prob, DFBDF())","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Solver-Selection-Guide","page":"OrdinaryDiffEqBDF","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Recommended-DAE-Methods","page":"OrdinaryDiffEqBDF","title":"Recommended DAE Methods","text":"DFBDF: Recommended - Variable-order BDF for general DAE systems\nDImplicitEuler: For non-smooth problems with discontinuities","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Method-characteristics","page":"OrdinaryDiffEqBDF","title":"Method characteristics","text":"DFBDF: Most robust and efficient for general smooth DAE problems\nDImplicitEuler: Best choice for problems with discontinuities, events, or non-smooth behavior","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Performance-Guidelines","page":"OrdinaryDiffEqBDF","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#When-fully-implicit-DAE-BDF-methods-excel","page":"OrdinaryDiffEqBDF","title":"When fully implicit DAE BDF methods excel","text":"Index-1 DAE systems with complex implicit structure\nComplex constraint structures with multiple algebraic relationships\nLarge-scale problems where specialized DAE methods are essential\nMultiphysics simulations with mixed differential-algebraic structure\nProblems where semi-explicit formulation is impractical","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Index-considerations","page":"OrdinaryDiffEqBDF","title":"Index considerations","text":"Index-1 formulation required: Problems should be written in index-1 form\nCompare with mass matrix methods: For some index-1 problems, mass matrix formulation may be more efficient\nHigher-index problems: Should be reduced to index-1 form before using these methods","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Important-DAE-Requirements","page":"OrdinaryDiffEqBDF","title":"Important DAE Requirements","text":"","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Initial-conditions","page":"OrdinaryDiffEqBDF","title":"Initial conditions","text":"Both u₀ and du₀ must be provided and consistent with constraints\ndifferential_vars specification helps identify algebraic variables\nConsistent initialization is crucial for index-1 DAE problems","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Function-specification","page":"OrdinaryDiffEqBDF","title":"Function specification","text":"Residual form: F(du/dt, u, t) = 0 with F returning zero for satisfied equations\nProper scaling: Ensure equations are well-conditioned numerically\nJacobian availability: Analytical Jacobians improve performance when available","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Alternative-Approaches","page":"OrdinaryDiffEqBDF","title":"Alternative Approaches","text":"Consider these alternatives:\n\nMass matrix DAE methods for index-1 problems with M du/dt = f(u,t) structure\nIndex reduction techniques using ModelingToolkit.jl to convert problems to index-1 form if needed\nConstraint stabilization methods for drift control\nProjection methods for manifold preservation\n\nFor more details on DAE formulations and alternative approaches, see this blog post on Neural DAEs.\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqBDF\", \"DFBDF\")","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#Full-list-of-solvers","page":"OrdinaryDiffEqBDF","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#DAE","page":"OrdinaryDiffEqBDF","title":"DAE","text":"","category":"section"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#OrdinaryDiffEqBDF.DImplicitEuler","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.DImplicitEuler","text":"DImplicitEuler(; chunk_size = Val{0}(),\n                 autodiff = AutoForwardDiff(),\n                 standardtag = Val{true}(),\n                 concrete_jac = nothing,\n                 linsolve = nothing,\n                 precs = DEFAULT_PRECS,\n                 nlsolve = NLNewton(),\n                 extrapolant = :constant,\n                 controller = :Standard)\n\nMultistep Method. 1st order A-L and stiffly stable adaptive implicit Euler. Implicit Euler for implicit DAE form. It uses an apriori error estimator for adaptivity based on a finite differencing approximation from SPICE.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify DImplicitEuler(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#OrdinaryDiffEqBDF.DABDF2","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.DABDF2","text":"DABDF2(; chunk_size = Val{0}(),\n         autodiff = AutoForwardDiff(),\n         standardtag = Val{true}(),\n         concrete_jac = nothing,\n         linsolve = nothing,\n         precs = DEFAULT_PRECS,\n         nlsolve = NLNewton(),\n         extrapolant = :constant,\n         controller = :Standard)\n\nMultistep Method. 2nd order A-L stable adaptive BDF method. Fully implicit implementation of BDF2.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify DABDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{celaya2014implementation, title={Implementation of an Adaptive BDF2 Formula and Comparison with the MATLAB Ode15s}, author={Celaya, E Alberdi and Aguirrezabala, JJ Anza and Chatzipantelidis, Panagiotis}, journal={Procedia Computer Science}, volume={29}, pages={1014–1026}, year={2014}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/fullyimplicitdae/BDF/#OrdinaryDiffEqBDF.DFBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.DFBDF","text":"DFBDF(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        κ = nothing,\n        tol = nothing,\n        nlsolve = NLNewton(),\n        extrapolant = :linear,\n        controller = :Standard,\n        max_order::Val{MO} = Val{5}())\n\nMultistep Method. Fixed-leading coefficient adaptive-order adaptive-time BDF method. Fully implicit implementation of FBDF based on Shampine's\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify DFBDF(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\nmax_order: TBD\n\nReferences\n\n@article{shampine2002solving, title={Solving 0= F (t, y (t), y′(t)) in Matlab}, author={Shampine, Lawrence F}, year={2002}, publisher={Walter de Gruyter GmbH and Co. KG} }\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/usage/#Usage-Guide","page":"Usage","title":"Usage Guide","text":"This page provides guidance on using StochasticDiffEq.jl effectively.","category":"section"},{"location":"api/stochasticdiffeq/usage/#Basic-Usage","page":"Usage","title":"Basic Usage","text":"","category":"section"},{"location":"api/stochasticdiffeq/usage/#Problem-Definition","page":"Usage","title":"Problem Definition","text":"StochasticDiffEq.jl uses the standard DifferentialEquations.jl problem interface:\n\nusing StochasticDiffEq\n\n# For scalar problems\nfunction f(u, p, t)  # drift\n    return μ * u\nend\n\nfunction g(u, p, t)  # diffusion  \n    return σ * u\nend\n\n# For in-place systems\nfunction f!(du, u, p, t)\n    du[1] = μ * u[1]\n    du[2] = -ν * u[2]\nend\n\nfunction g!(du, u, p, t)\n    du[1] = σ₁ * u[1]\n    du[2] = σ₂ * u[2]\nend\n\n# Create problem\nprob = SDEProblem(f, g, u0, tspan)","category":"section"},{"location":"api/stochasticdiffeq/usage/#Solver-Selection","page":"Usage","title":"Solver Selection","text":"Choose solvers based on your problem characteristics:\n\n# Default - good for most problems\nsol = solve(prob)\n\n# Specify solver explicitly\nsol = solve(prob, SOSRI())          # Recommended for diagonal noise\nsol = solve(prob, SOSRA())          # Optimal for additive noise  \nsol = solve(prob, SKenCarp())       # For stiff problems\nsol = solve(prob, EM())             # For maximum efficiency","category":"section"},{"location":"api/stochasticdiffeq/usage/#Algorithm-Parameters","page":"Usage","title":"Algorithm Parameters","text":"Most solvers accept parameters for customization:\n\n# Euler-Maruyama with step splitting\nsol = solve(prob, EM(split = true))\n\n# RKMilCommute with Stratonovich interpretation\nsol = solve(prob, RKMilCommute(interpretation = :Stratonovich))\n\n# Implicit methods with solver options\nsol = solve(prob, SKenCarp(linsolve = KrylovJL_GMRES()))","category":"section"},{"location":"api/stochasticdiffeq/usage/#Tolerances-and-Adaptive-Stepping","page":"Usage","title":"Tolerances and Adaptive Stepping","text":"Set absolute and relative tolerances:\n\nsol = solve(prob, SOSRI(), abstol = 1e-6, reltol = 1e-3)\n\nFor fixed time stepping:\n\nsol = solve(prob, EM(), dt = 0.01, adaptive = false)","category":"section"},{"location":"api/stochasticdiffeq/usage/#Noise-Types","page":"Usage","title":"Noise Types","text":"","category":"section"},{"location":"api/stochasticdiffeq/usage/#Diagonal-Noise","page":"Usage","title":"Diagonal Noise","text":"Most common case - each component has independent noise:\n\nfunction g!(du, u, p, t)\n    du[1] = σ₁ * u[1]\n    du[2] = σ₂ * u[2]\nend","category":"section"},{"location":"api/stochasticdiffeq/usage/#Scalar-Noise","page":"Usage","title":"Scalar Noise","text":"Single noise source affects all components:\n\nfunction g!(du, u, p, t)\n    du[1] = σ * u[1]\n    du[2] = σ * u[2]\nend","category":"section"},{"location":"api/stochasticdiffeq/usage/#Non-diagonal-Noise","page":"Usage","title":"Non-diagonal Noise","text":"Multiple noise sources with cross-terms:\n\nfunction g!(du, u, p, t)\n    du[1] = σ₁₁ * u[1] + σ₁₂ * u[2]\n    du[2] = σ₂₁ * u[1] + σ₂₂ * u[2]\nend","category":"section"},{"location":"api/stochasticdiffeq/usage/#Additive-Noise","page":"Usage","title":"Additive Noise","text":"Noise independent of solution:\n\nfunction g!(du, u, p, t)\n    du[1] = σ₁\n    du[2] = σ₂\nend","category":"section"},{"location":"api/stochasticdiffeq/usage/#Itô-vs-Stratonovich","page":"Usage","title":"Itô vs Stratonovich","text":"Specify interpretation when creating problems or choosing solvers:\n\n# Itô interpretation (default)\nprob = SDEProblem(f!, g!, u0, tspan, interpretation = :Ito)\n\n# Stratonovich interpretation  \nprob = SDEProblem(f!, g!, u0, tspan, interpretation = :Stratonovich)\n\n# Or at solver level\nsol = solve(prob, RKMil(interpretation = :Stratonovich))","category":"section"},{"location":"api/stochasticdiffeq/usage/#Performance-Tips","page":"Usage","title":"Performance Tips","text":"Use appropriate solvers: Match solver to problem type\nIn-place functions: Use f!(du,u,p,t) for better performance\nTolerances: Don't make tolerances unnecessarily strict\nStatic arrays: Use StaticArrays.jl for small systems\nGPU: Use CuArrays.jl for large problems","category":"section"},{"location":"api/stochasticdiffeq/usage/#Common-Pitfalls","page":"Usage","title":"Common Pitfalls","text":"Wrong noise type: Ensure solver supports your noise structure\nStiffness: Use appropriate stiff solvers for stiff problems\nCommuting noise: Use specialized solvers for better efficiency\nHigh dimensions: Consider weak convergence methods for Monte Carlo","category":"section"},{"location":"api/stochasticdiffeq/usage/#Integration-with-DifferentialEquations.jl","page":"Usage","title":"Integration with DifferentialEquations.jl","text":"StochasticDiffEq.jl integrates with the broader ecosystem:\n\nusing DifferentialEquations\n\n# Callbacks\ncondition(u, t, integrator) = u[1] - 0.5\naffect!(integrator) = terminate!(integrator)\ncb = ContinuousCallback(condition, affect!)\n\nsol = solve(prob, SOSRI(), callback = cb)\n\n# Ensemble simulations\nmonte_prob = EnsembleProblem(prob)\nsim = solve(monte_prob, SOSRI(), EnsembleThreads(), trajectories = 1000)","category":"section"},{"location":"tutorials/dae_example/#Differential-Algebraic-Equations","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"Differential Algebraic Equations (DAEs) are differential equations which have constraint equations on their evolution. This tutorial will introduce you to the functionality for solving differential algebraic equations (DAEs). Other introductions can be found by checking out SciMLTutorials.jl.\n\nnote: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"section"},{"location":"tutorials/dae_example/#Mass-Matrix-Differential-Algebraic-Equations-(DAEs)","page":"Differential Algebraic Equations","title":"Mass-Matrix Differential-Algebraic Equations (DAEs)","text":"Instead of just defining an ODE as u = f(upt), it can be common to express the differential equation in the form with a mass matrix:\n\nMu = f(upt)\n\nwhere M is known as the mass matrix. Let's solve the Robertson equation. In previous tutorials, we wrote this equation as:\n\nbeginaligned\ndy_1 = -004 y_1 + 10^4 y_2 y_3 \ndy_2 =  004 y_1 - 10^4 y_2 y_3 - 310^7 y_2^2 \ndy_3 = 310^7 y_2^2 \nendaligned\n\nBut we can instead write this with a conservation relation:\n\nbeginaligned\nfracdy_1dt = -004 y_1 + 10^4 y_2 y_3 \nfracdy_2dt =  004 y_1 - 10^4 y_2 y_3 - 310^7 y_2^2 \n1 =  y_1 + y_2 + y_3 \nendaligned\n\nIn this form, we can write this as a mass matrix ODE where M is singular (this is another form of a differential-algebraic equation (DAE)). Here, the last row of M is just zero. We can implement this form as:\n\nimport DifferentialEquations as DE\nimport Plots\nfunction rober(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁ * y₁ + k₃ * y₂ * y₃\n    du[2] = k₁ * y₁ - k₃ * y₂ * y₃ - k₂ * y₂^2\n    du[3] = y₁ + y₂ + y₃ - 1\n    nothing\nend\nM = [1.0 0 0\n     0 1.0 0\n     0 0 0]\nf = DE.ODEFunction(rober, mass_matrix = M)\nprob_mm = DE.ODEProblem(f, [1.0, 0.0, 0.0], (0.0, 1e5), (0.04, 3e7, 1e4))\nsol = DE.solve(prob_mm, DE.Rodas5(), reltol = 1e-8, abstol = 1e-8)\n\nPlots.plot(sol, xscale = :log10, tspan = (1e-6, 1e5), layout = (3, 1))\n\nnote: Note\nIf your mass matrix is singular, i.e. your system is a DAE, then you need to make sure you choose a solver that is compatible with DAEs","category":"section"},{"location":"tutorials/dae_example/#Implicitly-Defined-Differential-Algebraic-Equations-(DAEs)","page":"Differential Algebraic Equations","title":"Implicitly-Defined Differential-Algebraic Equations (DAEs)","text":"In this example, we will solve the Robertson equation in its implicit form:\n\nf(duupt) = 0\n\nThis equation is a DAE of the form:\n\nbeginaligned\nfracdudt = f(upt) \n 0 = g(upt) \n endaligned\n\nwhich is also known as a constrained differential equation, where g is the constraint equation. The Robertson model can be written in the form:\n\nbeginaligned\nfracdy_1dt = -004y₁ + 10^4 y_2 y_3 \nfracdy_2dt = 004 y_1 - 10^4 y_2 y_3 - 310^7 y_2^2 \n1 =  y_1 + y_2 + y_3 \nendaligned\n\nwith initial conditions y_1(0) = 1, y_2(0) = 0, y_3(0) = 0, dy_1 = - 004, dy_2 = 004, and dy_3 = 00.\n\nThe workflow for DAEs is the same as for the other types of equations, where all you need to know is how to define the problem. A DAEProblem is specified by defining an in-place update f(out,du,u,p,t) which uses the values to mutate out as the output. To makes this into a DAE, we move all the variables to one side. Thus, we can define the function:\n\nfunction f2(out, du, u, p, t)\n    out[1] = -0.04u[1] + 1e4 * u[2] * u[3] - du[1]\n    out[2] = +0.04u[1] - 3e7 * u[2]^2 - 1e4 * u[2] * u[3] - du[2]\n    out[3] = u[1] + u[2] + u[3] - 1.0\nend\n\nwith initial conditions\n\nu₀ = [1.0, 0, 0]\ndu₀ = [-0.04, 0.04, 0.0]\ntspan = (0.0, 100000.0)\n\nand make the DAEProblem:\n\ndifferential_vars = [true, true, false]\nprob = DE.DAEProblem(f2, du₀, u₀, tspan, differential_vars = differential_vars)\n\ndifferential_vars is an option which states which of the variables are differential, i.e. not purely algebraic (which means that their derivative shows up in the residual equations). This is required for the initialization algorithm to properly compute consistent initial conditions. Notice that the first two variables are determined by their changes, but the last is simply determined by the conservation equation. Thus, we use differential_vars = [true,true,false].","category":"section"},{"location":"tutorials/dae_example/#DAE-Initialization","page":"Differential Algebraic Equations","title":"DAE Initialization","text":"DAEs require that the initial conditions satisfy the constraint equations. In this example, our initial conditions are already consistent (they satisfy f(du₀, u₀, p, t₀) = 0). However, in many cases, you may not have consistent initial conditions, and the solver needs to compute them.\n\nThe IDA solver from Sundials can handle initialization through the initializealg parameter. The most commonly used initialization algorithm is BrownFullBasicInit(), which modifies the algebraic variables and derivatives to satisfy the constraints:\n\nimport Sundials\nimport DiffEqBase\n# Explicitly use Brown's initialization algorithm\nsol = DE.solve(prob, Sundials.IDA(), initializealg = DiffEqBase.BrownFullBasicInit())\n\nIf you're confident your initial conditions are already consistent, you can verify this using CheckInit():\n\n# This will verify initial conditions and error if they're inconsistent\nsol_check = DE.solve(prob, Sundials.IDA(), initializealg = DiffEqBase.CheckInit())\n\nFor more details on DAE initialization options, see the DAE Initialization documentation.\n\nIn order to clearly see all the features of this solution, it should be plotted on a logarithmic scale. We'll also plot each on a different subplot, to allow scaling the y-axis appropriately.\n\nPlots.plot(sol, xscale = :log10, tspan = (1e-6, 1e5), layout = (3, 1))","category":"section"},{"location":"tutorials/dae_example/#Handling-Inconsistent-Initial-Conditions","page":"Differential Algebraic Equations","title":"Handling Inconsistent Initial Conditions","text":"Let's see what happens when we provide inconsistent initial conditions and how the initialization algorithms handle them:\n\n# Inconsistent initial conditions - y₃ should be 0 to satisfy y₁ + y₂ + y₃ = 1\nu₀_inconsistent = [1.0, 0.0, 0.5]  # Sum is 1.5, not 1!\ndu₀_inconsistent = [-0.04, 0.04, 0.0]\n\nprob_inconsistent = DE.DAEProblem(f2, du₀_inconsistent, u₀_inconsistent, tspan,\n                                  differential_vars = differential_vars)\n\n# This would error with CheckInit() because conditions are inconsistent:\n# sol_error = DE.solve(prob_inconsistent, Sundials.IDA(),\n#                      initializealg = DiffEqBase.CheckInit())\n\n# But BrownFullBasicInit() will fix the inconsistency automatically:\nsol_fixed = DE.solve(prob_inconsistent, Sundials.IDA(),\n                    initializealg = DiffEqBase.BrownFullBasicInit())\n\nprintln(\"Original (inconsistent) y₃ = \", u₀_inconsistent[3])\nprintln(\"Corrected y₃ after initialization = \", sol_fixed.u[1][3])\nprintln(\"Sum after correction = \", sum(sol_fixed.u[1]))\n\nAs you can see, BrownFullBasicInit() automatically adjusted the algebraic variable y₃ to satisfy the constraint equation y₁ + y₂ + y₃ = 1.","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK","text":"Singly Diagonally Implicit Runge-Kutta (SDIRK) methods are a family of implicit Runge-Kutta methods designed for solving stiff ordinary differential equations. These methods are particularly effective for problems where explicit methods become unstable due to stiffness.","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#Key-Properties","page":"OrdinaryDiffEqSDIRK","title":"Key Properties","text":"SDIRK methods have several important characteristics:\n\nA-stable and L-stable: Can handle highly stiff problems without numerical instability\nStiffly accurate: Many SDIRK methods provide additional numerical stability for stiff problems\nDiagonally implicit structure: The implicit system only requires solving a sequence of nonlinear equations rather than a large coupled system\nGood for moderate to large systems: More efficient than fully implicit RK methods for many problems","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#When-to-Use-SDIRK-Methods","page":"OrdinaryDiffEqSDIRK","title":"When to Use SDIRK Methods","text":"SDIRK methods are recommended for:\n\nStiff differential equations where explicit methods fail or require very small timesteps\nProblems requiring good stability properties at moderate to high tolerances\nSystems where Rosenbrock methods (which require Jacobians) are not suitable or available\nIMEX problems using the KenCarp family, which can split stiff and non-stiff terms","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqSDIRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#High-tolerance-(1e-2)","page":"OrdinaryDiffEqSDIRK","title":"High tolerance (>1e-2)","text":"TRBDF2: Second-order A-B-L-S-stable method, good for oscillatory problems","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#Medium-tolerance-(1e-8-to-1e-2)","page":"OrdinaryDiffEqSDIRK","title":"Medium tolerance (1e-8 to 1e-2)","text":"KenCarp4: Fourth-order method with excellent stability, good all-around choice\nKenCarp47: Seventh-stage fourth-order method, enhanced stability\nKvaerno4 or Kvaerno5: High-order stiffly accurate methods","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#Low-tolerance-(1e-8)","page":"OrdinaryDiffEqSDIRK","title":"Low tolerance (<1e-8)","text":"Kvaerno5: Fifth-order stiffly accurate method for high accuracy\nKenCarp5: Fifth-order method with splitting capabilities","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#Special-Cases","page":"OrdinaryDiffEqSDIRK","title":"Special Cases","text":"ImplicitEuler: First-order method, only recommended for problems with discontinuities or when f is not differentiable\nTrapezoid: Second-order symmetric method, reversible but not symplectic. Good for eliminating damping often seen with L-stable methods\nImplicitMidpoint: Second-order A-stable symplectic method for energy-preserving systems\nSSPSDIRK2: Strong stability preserving variant for problems requiring monotonicity preservation\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqSDIRK\", \"KenCarp4\")","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#Full-list-of-solvers","page":"OrdinaryDiffEqSDIRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#IMEX-SDIRK","page":"OrdinaryDiffEqSDIRK","title":"IMEX SDIRK","text":"These methods support SplitODEProblem for implicit-explicit (IMEX) integration, where the stiff part is treated implicitly and the non-stiff part is treated explicitly.","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#Higher-Order-ESDIRK","page":"OrdinaryDiffEqSDIRK","title":"Higher-Order ESDIRK","text":"Higher-order ESDIRK (Explicit first stage Singly Diagonally Implicit Runge-Kutta) methods. These are high-order L-stable implicit methods where the first stage is explicit.\n\nnote: Note\nThese methods do not support SplitODEProblem. For IMEX integration with split problems, use the KenCarp methods above instead.","category":"section"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.ImplicitEuler","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.ImplicitEuler","text":"ImplicitEuler(; chunk_size = Val{0}(),\n                autodiff = AutoForwardDiff(),\n                standardtag = Val{true}(),\n                concrete_jac = nothing,\n                diff_type = Val{:forward}(),\n                linsolve = nothing,\n                precs = DEFAULT_PRECS,\n                nlsolve = NLNewton(),\n                extrapolant = :constant,\n                controller = :PI,\n                step_limiter! = trivial_limiter!)\n\nSDIRK Method. A 1st order implicit solver. A-B-L-stable. Adaptive timestepping through a divided differences estimate. Strong-stability preserving (SSP). Good for highly stiff equations.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ImplicitEuler(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@book{wanner1996solving,     title={Solving ordinary differential equations II},     author={Wanner, Gerhard and Hairer, Ernst},     volume={375},     year={1996},     publisher={Springer Berlin Heidelberg New York}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.ImplicitMidpoint","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.ImplicitMidpoint","text":"ImplicitMidpoint(; chunk_size = Val{0}(),\n                   autodiff = AutoForwardDiff(),\n                   standardtag = Val{true}(),\n                   concrete_jac = nothing,\n                   diff_type = Val{:forward}(),\n                   linsolve = nothing,\n                   precs = DEFAULT_PRECS,\n                   nlsolve = NLNewton(),\n                   extrapolant = :linear,\n                   step_limiter! = trivial_limiter!)\n\nSDIRK Method. A second order A-stable symplectic and symmetric implicit solver. Excellent for Hamiltonian systems and highly stiff equations.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ImplicitMidpoint(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@book{wanner1996solving,     title={Solving ordinary differential equations II},     author={Wanner, Gerhard and Hairer, Ernst},     volume={375},     year={1996},     publisher={Springer Berlin Heidelberg New York}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.Trapezoid","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.Trapezoid","text":"Trapezoid(; chunk_size = Val{0}(),\n            autodiff = AutoForwardDiff(),\n            standardtag = Val{true}(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS,\n            nlsolve = NLNewton(),\n            extrapolant = :linear,\n            controller = :PI,\n            step_limiter! = trivial_limiter!)\n\nSDIRK Method. A second order A-stable symmetric ESDIRK method. 'Almost symplectic' without numerical dampening.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Trapezoid(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nAndre Vladimirescu. 1994. The Spice Book. John Wiley & Sons, Inc., New York, NY, USA.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.TRBDF2","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.TRBDF2","text":"TRBDF2(; chunk_size = Val{0}(),\n         autodiff = AutoForwardDiff(),\n         standardtag = Val{true}(),\n         concrete_jac = nothing,\n         diff_type = Val{:forward}(),\n         linsolve = nothing,\n         precs = DEFAULT_PRECS,\n         nlsolve = NLNewton(),\n         smooth_est = true,\n         extrapolant = :linear,\n         controller = :PI,\n         step_limiter! = trivial_limiter!)\n\nSDIRK Method. A second order A-B-L-S-stable one-step ESDIRK method. Includes stiffness-robust error estimates for accurate adaptive timestepping, smoothed derivatives for highly stiff and oscillatory problems. Good for high tolerances (>1e-2) on stiff problems.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify TRBDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{hosea1996analysis,     title={Analysis and implementation of TR-BDF2},     author={Hosea, ME and Shampine, LF},     journal={Applied Numerical Mathematics},     volume={20},     number={1-2},     pages={21–37},     year={1996},     publisher={Elsevier}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SDIRK2","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SDIRK2","text":"SDIRK2(; chunk_size = Val{0}(),\n         autodiff = AutoForwardDiff(),\n         standardtag = Val{true}(),\n         concrete_jac = nothing,\n         diff_type = Val{:forward}(),\n         linsolve = nothing,\n         precs = DEFAULT_PRECS,\n         nlsolve = NLNewton(),\n         smooth_est = true,\n         extrapolant = :linear,\n         controller = :PI,\n         step_limiter! = trivial_limiter!)\n\nSDIRK Method. SDIRK2: SDIRK Method An A-B-L stable 2nd order SDIRK method\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SDIRK2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{hindmarsh2005sundials,     title={{SUNDIALS}: Suite of nonlinear and differential/algebraic equation solvers},     author={Hindmarsh, Alan C and Brown, Peter N and Grant, Keith E and Lee, Steven L and Serban, Radu and Shumaker, Dan E and Woodward, Carol S},     journal={ACM Transactions on Mathematical Software (TOMS)},     volume={31},     number={3},     pages={363–396},     year={2005},     publisher={ACM}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SDIRK22","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SDIRK22","text":"SDIRK22(; chunk_size = Val{0}(),\n          autodiff = AutoForwardDiff(),\n          standardtag = Val{true}(),\n          concrete_jac = nothing,\n          diff_type = Val{:forward}(),\n          linsolve = nothing,\n          precs = DEFAULT_PRECS,\n          nlsolve = NLNewton(),\n          smooth_est = true,\n          extrapolant = :linear,\n          controller = :PI,\n          step_limiter! = trivial_limiter!)\n\nSDIRK Method. Description TBD\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SDIRK22(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@techreport{kennedy2016diagonally,     title={Diagonally implicit Runge-Kutta methods for ordinary differential equations. A review},     author={Kennedy, Christopher A and Carpenter, Mark H},     year={2016}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SSPSDIRK2","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SSPSDIRK2","text":"SSPSDIRK2(; chunk_size = Val{0}(),\n            autodiff = AutoForwardDiff(),\n            standardtag = Val{true}(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS,\n            nlsolve = NLNewton(),\n            smooth_est = true,\n            extrapolant = :constant,\n            controller = :PI)\n\nSDIRK Method. SSPSDIRK is an SSP-optimized SDIRK method, so it's an implicit SDIRK method for handling stiffness but if the dt is below the SSP coefficient * dt, then the SSP property of the SSP integrators (the other page) is satisfied. As such this is a method which is expected to be good on advection-dominated cases where an explicit SSP integrator would be used, but where reaction equations are sufficient stiff to justify implicit integration.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SSPSDIRK2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{ketcheson2009optimal,     title={Optimal implicit strong stability preserving Runge–Kutta methods},     author={Ketcheson, David I and Macdonald, Colin B and Gottlieb, Sigal},     journal={Applied Numerical Mathematics},     volume={59},     number={2},     pages={373–392},     year={2009},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.Kvaerno3","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.Kvaerno3","text":"Kvaerno3(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           smooth_est = true,\n           extrapolant = :linear,\n           controller = :PI,\n           step_limiter! = trivial_limiter!)\n\nSDIRK Method. An A-L stable stiffly-accurate 3rd order ESDIRK method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Kvaerno3(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{kvaerno2004singly,     title={Singly diagonally implicit Runge–Kutta methods with an explicit first stage},     author={Kv{\\ae}rn{\\o}, Anne},     journal={BIT Numerical Mathematics},     volume={44},     number={3},     pages={489–502},     year={2004},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.KenCarp3","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.KenCarp3","text":"KenCarp3(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           smooth_est = true,\n           extrapolant = :linear,\n           controller = :PI,\n           step_limiter! = trivial_limiter!)\n\nSDIRK Method. An A-L stable stiffly-accurate 3rd order ESDIRK method with splitting.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify KenCarp3(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@book{kennedy2001additive,     title={Additive Runge-Kutta schemes for convection-diffusion-reaction equations},     author={Kennedy, Christopher Alan},     year={2001},     publisher={National Aeronautics and Space Administration, Langley Research Center}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.CFNLIRK3","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.CFNLIRK3","text":"CFNLIRK3(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           extrapolant = :linear)\n\nSDIRK Method. Third order method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify CFNLIRK3(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{calvo2001linearly,     title={Linearly implicit Runge–Kutta methods for advection–reaction–diffusion equations},     author={Calvo, MP and De Frutos, J and Novo, J},     journal={Applied Numerical Mathematics},     volume={37},     number={4},     pages={535–549},     year={2001},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.Cash4","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.Cash4","text":"Cash4(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        diff_type = Val{:forward}(),\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        nlsolve = NLNewton(),\n        smooth_est = true,\n        extrapolant = :linear,\n        controller = :PI,\n        embedding = 3)\n\nSDIRK Method. An A-L stable 4th order SDIRK method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Cash4(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nembedding: TBD\n\nReferences\n\n@article{hindmarsh2005sundials,     title={{SUNDIALS}: Suite of nonlinear and differential/algebraic equation solvers},     author={Hindmarsh, Alan C and Brown, Peter N and Grant, Keith E and Lee, Steven L and Serban, Radu and Shumaker, Dan E and Woodward, Carol S},     journal={ACM Transactions on Mathematical Software (TOMS)},     volume={31},     number={3},     pages={363–396},     year={2005},     publisher={ACM}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SFSDIRK4","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SFSDIRK4","text":"SFSDIRK4(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           extrapolant = :linear)\n\nSDIRK Method. Method of order 4.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SFSDIRK4(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{ferracina2008strong,     title={Strong stability of singly-diagonally-implicit Runge–Kutta methods},     author={Ferracina, Luca and Spijker, MN},     journal={Applied Numerical Mathematics},     volume={58},     number={11},     pages={1675–1686},     year={2008},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SFSDIRK5","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SFSDIRK5","text":"SFSDIRK5(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           extrapolant = :linear)\n\nSDIRK Method. Method of order 5.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SFSDIRK5(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{ferracina2008strong,     title={Strong stability of singly-diagonally-implicit Runge–Kutta methods},     author={Ferracina, Luca and Spijker, MN},     journal={Applied Numerical Mathematics},     volume={58},     number={11},     pages={1675–1686},     year={2008},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SFSDIRK6","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SFSDIRK6","text":"SFSDIRK6(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           extrapolant = :linear)\n\nSDIRK Method. Method of order 6.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SFSDIRK6(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{ferracina2008strong,     title={Strong stability of singly-diagonally-implicit Runge–Kutta methods},     author={Ferracina, Luca and Spijker, MN},     journal={Applied Numerical Mathematics},     volume={58},     number={11},     pages={1675–1686},     year={2008},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SFSDIRK7","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SFSDIRK7","text":"SFSDIRK7(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           extrapolant = :linear)\n\nSDIRK Method. Method of order 7.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SFSDIRK7(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{ferracina2008strong,     title={Strong stability of singly-diagonally-implicit Runge–Kutta methods},     author={Ferracina, Luca and Spijker, MN},     journal={Applied Numerical Mathematics},     volume={58},     number={11},     pages={1675–1686},     year={2008},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.SFSDIRK8","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.SFSDIRK8","text":"SFSDIRK8(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           extrapolant = :linear)\n\nSDIRK Method. Method of order 8.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SFSDIRK8(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{ferracina2008strong,     title={Strong stability of singly-diagonally-implicit Runge–Kutta methods},     author={Ferracina, Luca and Spijker, MN},     journal={Applied Numerical Mathematics},     volume={58},     number={11},     pages={1675–1686},     year={2008},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.Hairer4","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.Hairer4","text":"Hairer4(; chunk_size = Val{0}(),\n          autodiff = AutoForwardDiff(),\n          standardtag = Val{true}(),\n          concrete_jac = nothing,\n          diff_type = Val{:forward}(),\n          linsolve = nothing,\n          precs = DEFAULT_PRECS,\n          nlsolve = NLNewton(),\n          smooth_est = true,\n          extrapolant = :linear,\n          controller = :PI)\n\nSDIRK Method. An A-L stable 4th order SDIRK method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Hairer4(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and     differential-algebraic problems. Computational mathematics (2nd revised ed.),     Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.Hairer42","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.Hairer42","text":"Hairer42(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           smooth_est = true,\n           extrapolant = :linear,\n           controller = :PI)\n\nSDIRK Method. An A-L stable 4th order SDIRK method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Hairer42(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and     differential-algebraic problems. Computational mathematics (2nd revised ed.),     Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.Kvaerno4","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.Kvaerno4","text":"Kvaerno4(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           smooth_est = true,\n           extrapolant = :linear,\n           controller = :PI,\n           step_limiter! = trivial_limiter!)\n\nSDIRK Method. An A-L stable stiffly-accurate 4th order ESDIRK method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Kvaerno4(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter: TBD\n\nReferences\n\n@article{kvaerno2004singly,     title={Singly diagonally implicit Runge–Kutta methods with an explicit first stage},     author={Kv{\\ae}rn{\\o}, Anne},     journal={BIT Numerical Mathematics},     volume={44},     number={3},     pages={489–502},     year={2004},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.Kvaerno5","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.Kvaerno5","text":"Kvaerno5(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           smooth_est = true,\n           extrapolant = :linear,\n           controller = :PI,\n           step_limiter! = trivial_limiter!)\n\nSDIRK Method. An A-L stable stiffly-accurate 5th order ESDIRK method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Kvaerno5(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter: TBD\n\nReferences\n\n@article{kvaerno2004singly,     title={Singly diagonally implicit Runge–Kutta methods with an explicit first stage},     author={Kv{\\ae}rn{\\o}, Anne},     journal={BIT Numerical Mathematics},     volume={44},     number={3},     pages={489–502},     year={2004},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.KenCarp4","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.KenCarp4","text":"KenCarp4(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           smooth_est = true,\n           extrapolant = :linear,\n           controller = :PI,\n           step_limiter! = trivial_limiter!)\n\nSDIRK Method. An A-L stable stiffly-accurate 4th order ESDIRK method with splitting. Includes splitting capabilities. Recommended for medium tolerance stiff problems (>1e-8).\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify KenCarp4(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter: TBD\n\nReferences\n\n@book{kennedy2001additive,     title={Additive Runge-Kutta schemes for convection-diffusion-reaction equations},     author={Kennedy, Christopher Alan},     year={2001},     publisher={National Aeronautics and Space Administration, Langley Research Center}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.KenCarp47","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.KenCarp47","text":"KenCarp47(; chunk_size = Val{0}(),\n            autodiff = AutoForwardDiff(),\n            standardtag = Val{true}(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS,\n            nlsolve = NLNewton(),\n            smooth_est = true,\n            extrapolant = :linear,\n            controller = :PI)\n\nSDIRK Method. An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method with splitting.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify KenCarp47(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{kennedy2019higher,     title={Higher-order additive Runge–Kutta schemes for ordinary differential equations},     author={Kennedy, Christopher A and Carpenter, Mark H},     journal={Applied Numerical Mathematics},     volume={136},     pages={183–205},     year={2019},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.KenCarp5","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.KenCarp5","text":"KenCarp5(; chunk_size = Val{0}(),\n           autodiff = AutoForwardDiff(),\n           standardtag = Val{true}(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           nlsolve = NLNewton(),\n           smooth_est = true,\n           extrapolant = :linear,\n           controller = :PI,\n           step_limiter! = trivial_limiter!)\n\nSDIRK Method. An A-L stable stiffly-accurate 5th order ESDIRK method with splitting.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify KenCarp5(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter: TBD\n\nReferences\n\n@book{kennedy2001additive,     title={Additive Runge-Kutta schemes for convection-diffusion-reaction equations},     author={Kennedy, Christopher Alan},     year={2001},     publisher={National Aeronautics and Space Administration, Langley Research Center}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.KenCarp58","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.KenCarp58","text":"KenCarp58(; chunk_size = Val{0}(),\n            autodiff = AutoForwardDiff(),\n            standardtag = Val{true}(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS,\n            nlsolve = NLNewton(),\n            smooth_est = true,\n            extrapolant = :linear,\n            controller = :PI)\n\nSDIRK Method. An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method with splitting.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify KenCarp58(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{kennedy2019higher,     title={Higher-order additive Runge–Kutta schemes for ordinary differential equations},     author={Kennedy, Christopher A and Carpenter, Mark H},     journal={Applied Numerical Mathematics},     volume={136},     pages={183–205},     year={2019},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.ESDIRK54I8L2SA","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.ESDIRK54I8L2SA","text":"ESDIRK54I8L2SA(; chunk_size = Val{0}(),\n                 autodiff = AutoForwardDiff(),\n                 standardtag = Val{true}(),\n                 concrete_jac = nothing,\n                 diff_type = Val{:forward}(),\n                 linsolve = nothing,\n                 precs = DEFAULT_PRECS,\n                 nlsolve = NLNewton(),\n                 extrapolant = :linear,\n                 controller = :PI)\n\nSDIRK Method. Optimized ESDIRK tableaus. Updates of the original KenCarp tableau expected to achieve lower error for the same steps in theory, but are still being fully evaluated in context.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ESDIRK54I8L2SA(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{Kennedy2019DiagonallyIR, title={Diagonally implicit Runge–Kutta methods for stiff ODEs}, author={Christopher A. Kennedy and Mark H. Carpenter}, journal={Applied Numerical Mathematics}, year={2019}, volume={146}, pages={221-244} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.ESDIRK436L2SA2","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.ESDIRK436L2SA2","text":"ESDIRK436L2SA2(; chunk_size = Val{0}(),\n                 autodiff = AutoForwardDiff(),\n                 standardtag = Val{true}(),\n                 concrete_jac = nothing,\n                 diff_type = Val{:forward}(),\n                 linsolve = nothing,\n                 precs = DEFAULT_PRECS,\n                 nlsolve = NLNewton(),\n                 extrapolant = :linear,\n                 controller = :PI)\n\nSDIRK Method. Optimized ESDIRK tableaus. Updates of the original KenCarp tableau expected to achieve lower error for the same steps in theory, but are still being fully evaluated in context.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ESDIRK436L2SA2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{Kennedy2019DiagonallyIR, title={Diagonally implicit Runge–Kutta methods for stiff ODEs}, author={Christopher A. Kennedy and Mark H. Carpenter}, journal={Applied Numerical Mathematics}, year={2019}, volume={146}, pages={221-244} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.ESDIRK437L2SA","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.ESDIRK437L2SA","text":"ESDIRK437L2SA(; chunk_size = Val{0}(),\n                autodiff = AutoForwardDiff(),\n                standardtag = Val{true}(),\n                concrete_jac = nothing,\n                diff_type = Val{:forward}(),\n                linsolve = nothing,\n                precs = DEFAULT_PRECS,\n                nlsolve = NLNewton(),\n                extrapolant = :linear,\n                controller = :PI)\n\nSDIRK Method. Optimized ESDIRK tableaus. Updates of the original KenCarp tableau expected to achieve lower error for the same steps in theory, but are still being fully evaluated in context.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ESDIRK437L2SA(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{Kennedy2019DiagonallyIR, title={Diagonally implicit Runge–Kutta methods for stiff ODEs}, author={Christopher A. Kennedy and Mark H. Carpenter}, journal={Applied Numerical Mathematics}, year={2019}, volume={146}, pages={221-244} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.ESDIRK547L2SA2","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.ESDIRK547L2SA2","text":"ESDIRK547L2SA2(; chunk_size = Val{0}(),\n                 autodiff = AutoForwardDiff(),\n                 standardtag = Val{true}(),\n                 concrete_jac = nothing,\n                 diff_type = Val{:forward}(),\n                 linsolve = nothing,\n                 precs = DEFAULT_PRECS,\n                 nlsolve = NLNewton(),\n                 extrapolant = :linear,\n                 controller = :PI)\n\nSDIRK Method. Optimized ESDIRK tableaus. Updates of the original KenCarp tableau expected to achieve lower error for the same steps in theory, but are still being fully evaluated in context.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ESDIRK547L2SA2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{Kennedy2019DiagonallyIR, title={Diagonally implicit Runge–Kutta methods for stiff ODEs}, author={Christopher A. Kennedy and Mark H. Carpenter}, journal={Applied Numerical Mathematics}, year={2019}, volume={146}, pages={221-244} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/SDIRK/#OrdinaryDiffEqSDIRK.ESDIRK659L2SA","page":"OrdinaryDiffEqSDIRK","title":"OrdinaryDiffEqSDIRK.ESDIRK659L2SA","text":"ESDIRK659L2SA(; chunk_size = Val{0}(),\n                autodiff = AutoForwardDiff(),\n                standardtag = Val{true}(),\n                concrete_jac = nothing,\n                diff_type = Val{:forward}(),\n                linsolve = nothing,\n                precs = DEFAULT_PRECS,\n                nlsolve = NLNewton(),\n                extrapolant = :linear,\n                controller = :PI)\n\nSDIRK Method. Optimized ESDIRK tableaus. Updates of the original KenCarp tableau expected to achieve lower error for the same steps in theory, but are still being fully evaluated in context. Currently has STABILITY ISSUES, causing it to fail the adaptive tests. Check issue https://github.com/SciML/OrdinaryDiffEq.jl/issues/1933 for more details.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ESDIRK659L2SA(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\n\nReferences\n\n@article{Kennedy2019DiagonallyIR, title={Diagonally implicit Runge–Kutta methods for stiff ODEs}, author={Christopher A. Kennedy and Mark H. Carpenter}, journal={Applied Numerical Mathematics}, year={2019}, volume={146}, pages={221-244} }\n\n\n\n\n\n","category":"type"},{"location":"solvers/dynamical_solve/#Dynamical,-Hamiltonian,-and-2nd-Order-ODE-Solvers","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Dynamical ODEs, such as those arising from Hamiltonians or second order ordinary differential equations, give rise to a special structure that can be specialized on in the solver for more efficiency. These algorithms require an ODE defined in the following ways:\n\nDynamicalODEProblem{isinplace}(f1, f2, v0, u0, tspan, p = NullParameters(); kwargs...)\nSecondOrderODEProblem{isinplace}(f, du0, u0, tspan, p = NullParameters(); kwargs...)\nHamiltonianProblem{T}(H, p0, q0, tspan, p = NullParameters(); kwargs...)\n\nThese correspond to partitioned equations of motion:\n\nfracdvdt = f_1(tu) \nfracdudt = f_2(v) \n\nor, for SecondOrderODEProblem,\n\nfracd^2udt^2 = f(upt)\n\nThe functions should be specified as f1(dv,v,u,p,t) and f2(du,v,u,p,t) (in the inplace form), where f1 is independent of v (unless specified by the solver), and f2 is independent of t and u. This includes discretizations arising from SecondOrderODEProblems where the velocity is not used in the acceleration function, and Hamiltonians where the potential is (or can be) time-dependent, but the kinetic energy is only dependent on v.\n\nNote that some methods assume that the integral of f2 is a quadratic form. That means that f2=v'*M*v, i.e. int f_2 = frac12 m v^2, giving du = v. This is equivalent to saying that the kinetic energy is related to v^2. The methods which require this assumption will lose accuracy if this assumption is violated. Methods listed below make note of this requirement with \"Requires quadratic kinetic energy\".","category":"section"},{"location":"solvers/dynamical_solve/#Recommendations","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Recommendations","text":"When energy conservation is required, use a symplectic method. Otherwise, the Runge-Kutta-Nyström methods will be more efficient. Energy is mostly conserved by Runge-Kutta-Nyström methods, but is not conserved for long-time integrations. Thus, it is suggested that for shorter integrations you use Runge-Kutta-Nyström methods as well.\n\nAs a go-to method for efficiency, DPRKN6 is a good choice. DPRKN12 is a good choice when high accuracy, like tol<1e-10 is necessary. However, DPRKN6 is the only Runge-Kutta-Nyström method with a higher order interpolant (all default to order 3 Hermite, whereas DPRKN6 is order 6th interpolant) and thus in cases where interpolation matters (ex: event handling) one should use DPRKN6. For very smooth problems with expensive acceleration function evaluations, IRKN4 can be a good choice as it minimizes the number of evaluations.\n\nFor symplectic methods, higher order algorithms are the most efficient when higher accuracy is needed, and when less accuracy is needed lower order methods do better. Optimized efficiency methods take more steps and thus have more force calculations for the same order, but have smaller error. Thus, the “optimized efficiency” algorithms are recommended if your force calculation is not too sufficiency large, while the other methods are recommended when force calculations are really large (for example, like in MD simulations VelocityVerlet is very popular since it only requires one force calculation per timestep). A good go-to method would be McAte5, and a good high order choice is KahanLi8.","category":"section"},{"location":"solvers/dynamical_solve/#Standard-ODE-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Standard ODE Integrators","text":"The standard ODE integrators will work on Dynamical ODE problems via an automatic transformation to a first-order ODE. See the ODE solvers page for more details.","category":"section"},{"location":"solvers/dynamical_solve/#Specialized-OrdinaryDiffEq.jl-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Specialized OrdinaryDiffEq.jl Integrators","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a “free” interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).","category":"section"},{"location":"solvers/dynamical_solve/#Runge-Kutta-Nyström-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Runge-Kutta-Nyström Integrators","text":"Nystrom4: 4th order explicit Runge-Kutta-Nyström method. Allows acceleration to depend on velocity. Fixed timestep only.\nIRKN3: 4th order explicit two-step Runge-Kutta-Nyström method. Fixed timestep only.\nIRKN4: 4th order explicit two-step Runge-Kutta-Nyström method. Can be more efficient for smooth problems. Fixed timestep only.\nERKN4: 4th order Runge-Kutta-Nyström method which integrates the periodic properties of the harmonic oscillator exactly. Gets extra efficiency on periodic problems.\nERKN5: 5th order Runge-Kutta-Nyström method which integrates the periodic properties of the harmonic oscillator exactly. Gets extra efficiency on periodic problems.\nERKN7: 7th order Runge-Kutta-Nyström method which integrates the periodic properties of the harmonic oscillator exactly. Gets extra efficiency on periodic problems.\nNystrom4VelocityIndependent: 4th order explicit Runge-Kutta-Nyström method. Fixed timestep only.\nNystrom5VelocityIndependent: 5th order explicit Runge-Kutta-Nyström method. Fixed timestep only.\nDPRKN4: 4th order explicit adaptive Runge-Kutta-Nyström method.\nDPRKN5: 5th order explicit adaptive Runge-Kutta-Nyström method.\nDPRKN6: 6th order explicit adaptive Runge-Kutta-Nyström method. Free 6th order interpolant.\nDPRKN6FM: 6th order explicit adaptive Runge-Kutta-Nyström method.\nDPRKN8: 8th order explicit adaptive Runge-Kutta-Nyström method.\nDPRKN12: 12th order explicit adaptive Runge-Kutta-Nyström method.","category":"section"},{"location":"solvers/dynamical_solve/#Symplectic-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Symplectic Integrators","text":"Note that all symplectic integrators are fixed timestep only.\n\nSymplecticEuler: First order explicit symplectic integrator\nVelocityVerlet: 2nd order explicit symplectic integrator. Requires f_2(t,u) = v, i.e. a second order ODE.\nVerletLeapfrog: 2nd order explicit symplectic integrator.\nPseudoVerletLeapfrog: 2nd order explicit symplectic integrator.\nMcAte2: Optimized efficiency 2nd order explicit symplectic integrator.\nRuth3: 3rd order explicit symplectic integrator.\nMcAte3: Optimized efficiency 3rd order explicit symplectic integrator.\nCandyRoz4: 4th order explicit symplectic integrator.\nMcAte4: 4th order explicit symplectic integrator. Requires quadratic kinetic energy.\nCalvoSanz4: Optimized efficiency 4th order explicit symplectic integrator.\nMcAte42: 4th order explicit symplectic integrator. (Broken)\nMcAte5: Optimized efficiency 5th order explicit symplectic integrator. Requires quadratic kinetic energy.\nYoshida6: 6th order explicit symplectic integrator.\nKahanLi6: Optimized efficiency 6th order explicit symplectic integrator.\nMcAte8: 8th order explicit symplectic integrator.\nKahanLi8: Optimized efficiency 8th order explicit symplectic integrator.\nSofSpa10: 10th order explicit symplectic integrator.","category":"section"},{"location":"solvers/dynamical_solve/#GeometricIntegrators.jl","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"GeometricIntegrators.jl","text":"GeometricIntegrators.jl is a set of fixed timestep algorithms written in Julia. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use GeometricIntegratorsDiffEq.jl:\n\nPkg.clone(\"https://github.com/SciML/GeometricIntegratorsDiffEq.jl\")\nimport GeometricIntegratorsDiffEq\n\nGISymplecticEulerA - First order explicit symplectic Euler A\nGISymplecticEulerB - First order explicit symplectic Euler B\nGILobattoIIIAIIIB(n) - Nth order Gauss-Labatto-IIIA-IIIB\nGILobattoIIIBIIIA(n) - Nth order Gauss-Labatto-IIIB-IIIA","category":"section"},{"location":"examples/kepler_problem/#The-Kepler-Problem","page":"The Kepler Problem","title":"The Kepler Problem","text":"The (non-dimensional) Hamiltonian mathcal H and the angular momentum L for the Kepler problem are\n\nbeginalign*\nmathcalH(q_1 p_1 q_2 p_2) = frac12(p^2_1+p^2_2)-frac1sqrtq^2_1+q^2_2 \nL = q_1 p_2 - p_1 q_2\nendalign*\n\nAlso, we know that\n\nbeginalign*\nfracmathrmd boldsymbolpmathrmd t = - frac partial mathcalHpartial boldsymbolq  \nfracmathrmd boldsymbolqmathrmd t = + frac partial mathcalHpartial boldsymbolp\nendalign*\n\nimport OrdinaryDiffEq as ODE, ForwardDiff, Plots\nimport LinearAlgebra: norm\nH(q, p) = norm(p)^2 / 2 - inv(norm(q))\nL(q, p) = q[1] * p[2] - p[1] * q[2]\n\npdot(dp, p, q, params, t) = ForwardDiff.gradient!(dp, q -> -H(q, p), q)\nqdot(dq, p, q, params, t) = ForwardDiff.gradient!(dq, p -> H(q, p), p)\n\ninitial_position = [0.4, 0]\ninitial_velocity = [0.0, 2.0]\ninitial_cond = (initial_position, initial_velocity)\ninitial_first_integrals = (H(initial_cond...), L(initial_cond...))\ntspan = (0, 20.0)\nprob = ODE.DynamicalODEProblem(pdot, qdot, initial_velocity, initial_position, tspan)\nsol = ODE.solve(prob, ODE.KahanLi6(), dt = 1 // 10);\n\nLet's plot the orbit and check the energy and angular momentum variation. We know that energy and angular momentum should be constant, and they are also called first integrals.\n\nfunction plot_orbit(sol)\n    Plots.plot(sol, idxs = (3, 4), lab = \"Orbit\", title = \"Kepler Problem Solution\")\nend\n\nfunction plot_first_integrals(sol, H, L)\n    Plots.plot(initial_first_integrals[1] .- map(u -> H(u.x[2], u.x[1]), sol.u),\n        lab = \"Energy variation\", title = \"First Integrals\")\n    Plots.plot!(initial_first_integrals[2] .- map(u -> L(u.x[2], u.x[1]), sol.u),\n        lab = \"Angular momentum variation\")\nend\nanalysis_plot(sol, H, L) = Plots.plot(plot_orbit(sol), plot_first_integrals(sol, H, L))\n\nanalysis_plot(sol, H, L)\n\nLet's try to use a Runge-Kutta-Nyström solver to solve this problem and check the first integrals' variation.\n\nsol2 = ODE.solve(prob, ODE.DPRKN6())  # dt is not necessary, because unlike symplectic\n# integrators DPRKN6 is adaptive\n@show sol2.u |> length\nanalysis_plot(sol2, H, L)\n\nLet's then try to solve the same problem by the ERKN4 solver, which is specialized for sinusoid-like periodic function\n\nsol3 = ODE.solve(prob, ODE.ERKN4()) # dt is not necessary, because unlike symplectic\n# integrators ERKN4 is adaptive\n@show sol3.u |> length\nanalysis_plot(sol3, H, L)\n\nWe can see that ERKN4 does a bad job for this problem, because this problem is not sinusoid-like.\n\nOne advantage of using DynamicalODEProblem is that it can implicitly convert the second order ODE problem to a normal system of first order ODEs, which is solvable for other ODE solvers. Let's use the Tsit5 solver for the next example.\n\nsol4 = ODE.solve(prob, ODE.Tsit5())\n@show sol4.u |> length\nanalysis_plot(sol4, H, L)","category":"section"},{"location":"examples/kepler_problem/#Note","page":"The Kepler Problem","title":"Note","text":"There is drifting for all the solutions, and high order methods are drifting less because they are more accurate.","category":"section"},{"location":"examples/kepler_problem/#Conclusion","page":"The Kepler Problem","title":"Conclusion","text":"\n\nSymplectic integrator does not conserve the energy completely at all time, but the energy can come back. In order to make sure that the energy fluctuation comes back eventually, symplectic integrator has to have a fixed time step. Despite the energy variation, symplectic integrator conserves the angular momentum perfectly.\n\nBoth Runge-Kutta-Nyström and Runge-Kutta integrator do not conserve energy nor the angular momentum, and the first integrals do not tend to come back. An advantage Runge-Kutta-Nyström integrator over symplectic integrator is that RKN integrator can have adaptivity. An advantage Runge-Kutta-Nyström integrator over Runge-Kutta integrator is that RKN integrator has less function evaluation per step. The ERKN4 solver works best for sinusoid-like solutions.","category":"section"},{"location":"examples/kepler_problem/#Manifold-Projection","page":"The Kepler Problem","title":"Manifold Projection","text":"In this example, we know that energy and angular momentum should be conserved. We can achieve this through manifold projection. As the name implies, it is a procedure to project the ODE solution to a manifold. Let's start with a base case, where manifold projection isn't being used.\n\nnote: Note\nNote that NonlinearSolve.jl is required to be imported for ManifoldProjection\n\nimport DiffEqCallbacks as CB, NonlinearSolve as NLS\n\nfunction plot_orbit2(sol)\n    Plots.plot(sol, vars = (1, 2), lab = \"Orbit\", title = \"Kepler Problem Solution\")\nend\n\nfunction plot_first_integrals2(sol, H, L)\n    Plots.plot(initial_first_integrals[1] .- map(u -> H(u[1:2], u[3:4]), sol.u),\n        lab = \"Energy variation\", title = \"First Integrals\")\n    Plots.plot!(initial_first_integrals[2] .- map(u -> L(u[1:2], u[3:4]), sol.u),\n        lab = \"Angular momentum variation\")\nend\n\nanalysis_plot2(sol, H, L) = Plots.plot(plot_orbit2(sol), plot_first_integrals2(sol, H, L))\n\nfunction hamiltonian(du, u, params, t)\n    q, p = u[1:2], u[3:4]\n    qdot(@view(du[1:2]), p, q, params, t)\n    pdot(@view(du[3:4]), p, q, params, t)\nend\n\nprob2 = ODE.ODEProblem(hamiltonian, [initial_position; initial_velocity], tspan)\nsol_ = ODE.solve(prob2, ODE.RK4(), dt = 1 // 5, adaptive = false)\nanalysis_plot2(sol_, H, L)\n\nThere is a significant fluctuation in the first integrals, when there is no manifold projection.\n\nfunction first_integrals_manifold(residual, u, p, t)\n    residual[1:2] .= initial_first_integrals[1] - H(u[1:2], u[3:4])\n    residual[3:4] .= initial_first_integrals[2] - L(u[1:2], u[3:4])\nend\n\ncb = CB.ManifoldProjection(first_integrals_manifold, autodiff = NLS.AutoForwardDiff())\nsol5 = ODE.solve(prob2, ODE.RK4(), dt = 1 // 5, adaptive = false, callback = cb)\nanalysis_plot2(sol5, H, L)\n\nWe can see that thanks to the manifold projection, the first integrals' variation is very small, although we are using RK4 which is not symplectic. But wait, what if we only project to the energy conservation manifold?\n\nfunction energy_manifold(residual, u, p, t)\n    residual[1:2] .= initial_first_integrals[1] - H(u[1:2], u[3:4])\n    residual[3:4] .= 0\nend\nenergy_cb = CB.ManifoldProjection(energy_manifold, autodiff = NLS.AutoForwardDiff())\nsol6 = ODE.solve(prob2, ODE.RK4(), dt = 1 // 5, adaptive = false, callback = energy_cb)\nanalysis_plot2(sol6, H, L)\n\nThere is almost no energy variation, but angular momentum varies quite a bit. How about only project to the angular momentum conservation manifold?\n\nfunction angular_manifold(residual, u, p, t)\n    residual[1:2] .= initial_first_integrals[2] - L(u[1:2], u[3:4])\n    residual[3:4] .= 0\nend\nangular_cb = CB.ManifoldProjection(angular_manifold, autodiff = NLS.AutoForwardDiff())\nsol7 = ODE.solve(prob2, ODE.RK4(), dt = 1 // 5, adaptive = false, callback = angular_cb)\nanalysis_plot2(sol7, H, L)\n\nAgain, we see what we expect.","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Tau-Leaping-Methods-for-Jump-Diffusion","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Tau-Leaping Methods for Jump-Diffusion","text":"Tau-leaping methods approximate jump processes by \"leaping\" over multiple potential jump events in a single time step. These methods are essential for efficiently simulating systems with jump-diffusion processes.","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Tau-Leaping-Methods","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Tau-Leaping Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#TauLeaping-Basic-Tau-Leaping","page":"Tau-Leaping Methods for Jump-Diffusion","title":"TauLeaping - Basic Tau-Leaping","text":"","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#CaoTauLeaping-Cao's-Tau-Leaping","page":"Tau-Leaping Methods for Jump-Diffusion","title":"CaoTauLeaping - Cao's Tau-Leaping","text":"","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Understanding-Jump-Diffusion-Processes","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Understanding Jump-Diffusion Processes","text":"Jump-diffusion processes combine:\n\nContinuous diffusion: Standard Brownian motion terms\nJump processes: Discontinuous jumps at random times\n\nGeneral form:\n\ndX = μ(X,t)dt + σ(X,t)dW + ∫ h(X-,z)Ñ(dt,dz)\n\nWhere:\n\nμ(X,t)dt: Drift term\nσ(X,t)dW: Diffusion term\nÑ(dt,dz): Compensated random measure (jumps)","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#When-to-Use-Tau-Leaping","page":"Tau-Leaping Methods for Jump-Diffusion","title":"When to Use Tau-Leaping","text":"Appropriate for:\n\nSystems with many small jumps\nWhen exact jump simulation is computationally prohibitive\nChemical reaction networks\nPopulation models with birth-death processes\nFinancial models with rare events\n\nNot appropriate for:\n\nSystems dominated by large, infrequent jumps\nWhen exact jump timing is critical\nSmall systems where exact methods are feasible","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Method-Characteristics","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Method Characteristics","text":"","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#TauLeaping:","page":"Tau-Leaping Methods for Jump-Diffusion","title":"TauLeaping:","text":"Basic tau-leaping approximation\nFixed tau approach\nGood for initial exploration","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#CaoTauLeaping:","page":"Tau-Leaping Methods for Jump-Diffusion","title":"CaoTauLeaping:","text":"Adaptive tau selection\nMore sophisticated error control\nBetter for production simulations","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Configuration","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Configuration","text":"Tau-leaping methods require:\n\nJump rate functions: λ(X,t) for each reaction/jump type\nJump effects: How state changes with each jump\nTau selection: Time step size strategy\n\n# Basic setup\nprob = JumpProblem(base_problem, aggregator, jumps...)\nsol = solve(prob, TauLeaping())\n\n# With adaptive tau\nsol = solve(prob, CaoTauLeaping(), tau_tol = 0.01)","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Accuracy-Considerations","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Accuracy Considerations","text":"Tau-leaping approximation quality depends on:\n\nJump frequency vs. tau size\nState change magnitude per jump\nSystem stiffness\nError tolerance requirements\n\nRule of thumb: Tau should be small enough that jump rates don't change significantly over [t, t+tau].","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Alternative-Approaches","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Alternative Approaches","text":"If tau-leaping is inadequate:\n\nExact methods: Gillespie algorithm for small systems\nHybrid methods: Combine exact and approximate regions\nMoment closure: For statistical properties only\nPiecewise deterministic: For systems with rare jumps","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Performance-Tips","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Performance Tips","text":"Vectorize jump computations when possible\nUse sparse representations for large systems\nTune tau carefully - too large gives poor accuracy, too small is inefficient\nMonitor jump frequencies to validate approximation","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#Integration-with-DifferentialEquations.jl","page":"Tau-Leaping Methods for Jump-Diffusion","title":"Integration with DifferentialEquations.jl","text":"using DifferentialEquations, StochasticDiffEq\n\n# Define base SDE\nfunction drift!(du, u, p, t)\n    # Continuous drift\nend\n\nfunction diffusion!(du, u, p, t)\n    # Continuous diffusion\nend\n\n# Define jumps\njump1 = ConstantRateJump(rate1, affect1!)\njump2 = VariableRateJump(rate2, affect2!)\n\n# Combine into jump-diffusion problem\nsde_prob = SDEProblem(drift!, diffusion!, u0, tspan)\njump_prob = JumpProblem(sde_prob, Direct(), jump1, jump2)\n\n# Solve with tau-leaping\nsol = solve(jump_prob, TauLeaping())","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#References","page":"Tau-Leaping Methods for Jump-Diffusion","title":"References","text":"Gillespie, D.T., \"Approximate accelerated stochastic simulation of chemically reacting systems\"\nCao, Y., Gillespie, D.T., Petzold, L.R., \"Efficient step size selection for the tau-leaping method\"","category":"section"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#StochasticDiffEq.TauLeaping","page":"Tau-Leaping Methods for Jump-Diffusion","title":"StochasticDiffEq.TauLeaping","text":"TauLeaping()\n\nTauLeaping: Basic Tau-Leaping Method (Jump-Diffusion)\n\nBasic tau-leaping method for approximating jump-diffusion processes by \"leaping\" over multiple potential jump events.\n\nMethod Properties\n\nProblem type: Jump-diffusion processes\nApproach: Approximate multiple jumps per time step\nTime stepping: Fixed tau approach\nAccuracy: Depends on tau selection\n\nWhen to Use\n\nJump-diffusion systems with many small jumps\nWhen exact jump simulation is computationally prohibitive\nChemical reaction networks with fast reactions\nPopulation models with high birth-death rates\nInitial exploration of jump-diffusion problems\n\nAlgorithm Description\n\nApproximates Poisson processes by assuming constant propensities over time interval tau, then sampling number of jumps from Poisson distribution.\n\nTau Selection\n\nCritical parameter: tau should be small enough that jump rates don't change significantly over [t, t+tau].\n\nReferences\n\nGillespie, D.T., \"Approximate accelerated stochastic simulation of chemically reacting systems\"\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/jumpdiffusion/tau_leaping/#StochasticDiffEq.CaoTauLeaping","page":"Tau-Leaping Methods for Jump-Diffusion","title":"StochasticDiffEq.CaoTauLeaping","text":"CaoTauLeaping()\n\nCaoTauLeaping: Cao's Adaptive Tau-Leaping Method (Jump-Diffusion)\n\nAdvanced tau-leaping method with adaptive tau selection and improved error control.\n\nMethod Properties\n\nProblem type: Jump-diffusion processes\nApproach: Adaptive tau selection with error control\nTime stepping: Adaptive tau based on error estimates\nAccuracy: Superior to basic tau-leaping\n\nWhen to Use\n\nProduction jump-diffusion simulations requiring reliability\nWhen adaptive tau selection is needed\nProblems where basic TauLeaping gives poor accuracy\nChemical reaction networks requiring precise control\n\nAlgorithm Features\n\nAdaptive tau selection based on error estimates\nBetter stability and accuracy than basic tau-leaping\nAutomatic step size control\nMore sophisticated error estimation\n\nTau Selection\n\nAutomatically adjusts tau based on:\n\nLocal error estimates\nJump rate variations\nSolution stability requirements\n\nReferences\n\nCao, Y., Gillespie, D.T., Petzold, L.R., \"Efficient step size selection for the tau-leaping method\"# Etc.\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#High-Weak-Order-Methods","page":"High Weak Order Methods","title":"High Weak Order Methods","text":"These methods are specifically designed for problems where weak convergence is more important than strong convergence. They are optimal for Monte Carlo simulations, computing expectations, moments, and other statistical properties of solutions.","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Recommended-High-Weak-Order-Methods","page":"High Weak Order Methods","title":"Recommended High Weak Order Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#DRI1-Debrabant-Rößler-Method-(Weak-Order-2)","page":"High Weak Order Methods","title":"DRI1 - Debrabant-Rößler Method (Weak Order 2)","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#DRI1NM-Debrabant-Rößler-for-Non-mixing-Diagonal-Problems","page":"High Weak Order Methods","title":"DRI1NM - Debrabant-Rößler for Non-mixing Diagonal Problems","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Other-Weak-Order-2-Methods","page":"High Weak Order Methods","title":"Other Weak Order 2 Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#RI1,-RI3,-RI5,-RI6-Rößler-Methods","page":"High Weak Order Methods","title":"RI1, RI3, RI5, RI6 - Rößler Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#RDI-Methods-Alternative-Weak-Order-2","page":"High Weak Order Methods","title":"RDI Methods - Alternative Weak Order 2","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#W2Ito1-Efficient-Weak-Order-2","page":"High Weak Order Methods","title":"W2Ito1 - Efficient Weak Order 2","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Fixed-Step-Methods","page":"High Weak Order Methods","title":"Fixed Step Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#PL1WM,-PL1WMA-Platen-Methods","page":"High Weak Order Methods","title":"PL1WM, PL1WMA - Platen Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Stratonovich-Methods","page":"High Weak Order Methods","title":"Stratonovich Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#RS1,-RS2-Rößler-Stratonovich-Methods","page":"High Weak Order Methods","title":"RS1, RS2 - Rößler Stratonovich Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#NON,-NON2-Non-commutative-Stratonovich","page":"High Weak Order Methods","title":"NON, NON2 - Non-commutative Stratonovich","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#COM-Commutative-Stratonovich","page":"High Weak Order Methods","title":"COM - Commutative Stratonovich","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Specialized-Methods","page":"High Weak Order Methods","title":"Specialized Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#SIEA,-SMEA,-SIEB,-SMEB-Tocino-Vigo-Aguiar-Methods","page":"High Weak Order Methods","title":"SIEA, SMEA, SIEB, SMEB - Tocino-Vigo-Aguiar Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Weak-vs-Strong-Convergence","page":"High Weak Order Methods","title":"Weak vs Strong Convergence","text":"Strong Convergence: Measures pathwise error E[|X(T) - Xh(T)|^p] Weak Convergence: Measures error in expectations E[f(X(T))] - E[f(Xh(T))]","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#When-to-Use-Weak-Order-Methods:","page":"High Weak Order Methods","title":"When to Use Weak Order Methods:","text":"Monte Carlo simulations\nComputing expectations and moments\nStatistical analysis of SDEs\nWhen pathwise accuracy is not critical\nLarge ensemble simulations","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Advantages:","page":"High Weak Order Methods","title":"Advantages:","text":"Often more efficient for statistical quantities\nCan use larger time steps while maintaining weak accuracy\nOptimized error constants for better practical performance","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#Method-Selection-Guide","page":"High Weak Order Methods","title":"Method Selection Guide","text":"General purpose weak order 2: DRI1\nNon-mixing diagonal: DRI1NM\nFixed step: PL1WM, RS1/RS2\nStratonovich: RS1, RS2, NON, NON2\nSpecialized applications: RI methods, RDI methods","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#References","page":"High Weak Order Methods","title":"References","text":"Debrabant, K. and Rößler A., \"Families of efficient second order Runge–Kutta methods for the weak approximation of Itô stochastic differential equations\"\nRößler A., \"Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations\"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.DRI1","page":"High Weak Order Methods","title":"StochasticDiffEq.DRI1","text":"Debrabant, K. and Rößler A., Families of efficient second order Runge–Kutta methods for the weak approximation of Itô stochastic differential equations, Applied Numerical Mathematics 59, pp. 582–594 (2009) DOI:10.1016/j.apnum.2008.03.012\n\nDRI1()\n\nDRI1: Debrabant-Rößler Implicit Method (High Weak Order)\n\nAdaptive high-order method optimized for weak convergence with minimized error constants. Excellent for Monte Carlo simulations and moment calculations.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0 (optimized with minimized error constants)\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive noise)\nSDE interpretation: Itô\n\nWhen to Use\n\nOptimal for weak convergence requirements\nMonte Carlo simulations where statistical properties matter most\nComputing expectations, moments, and probability distributions\nWhen weak accuracy is more important than pathwise accuracy\nFor problems requiring diverse noise types\n\nWeak vs Strong Convergence\n\nWeak convergence: Convergence of expectations E[f(X_T)]\nStrong convergence: Pathwise convergence |XT - XT^h|\nDRI1 prioritizes weak convergence with optimized error constants\n\nAlgorithm Features\n\nMinimized error constants for better practical performance\nHandles complex noise structures including non-commuting terms\nAdaptive time stepping for efficiency\n\nReferences\n\nDebrabant, K. and Rößler A., \"Families of efficient second order Runge–Kutta methods for the weak approximation of Itô stochastic differential equations\", Applied Numerical Mathematics 59, pp. 582–594 (2009)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.DRI1NM","page":"High Weak Order Methods","title":"StochasticDiffEq.DRI1NM","text":"Debrabant, K. and Rößler A., Families of efficient second order Runge–Kutta methods for the weak approximation of Itô stochastic differential equations, Applied Numerical Mathematics 59, pp. 582–594 (2009) DOI:10.1016/j.apnum.2008.03.012\n\nDRI1NM()\n\nDRI1NM: Debrabant-Rößler Implicit Non-Mixing Method (High Weak Order)\n\nSpecialized version of DRI1 for non-mixing diagonal and scalar additive noise problems.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0 (optimized with minimized error constants)\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: Non-mixing diagonal and scalar additive noise\nSDE interpretation: Itô\n\nWhen to Use\n\nNon-mixing diagonal problems: du[k] = f(u[k]) dt + σ[k] dW[k]\nScalar additive noise problems\nWhen DRI1 is too general/expensive for the problem structure\nMonte Carlo simulations with special structure\n\nNon-Mixing Diagonal Structure\n\nOptimized for problems where:\n\ndu[1] = f₁(u[1])dt + σ₁ dW[1]\ndu[2] = f₂(u[2])dt + σ₂ dW[2]\n...\n\nEach component depends only on itself (no coupling).\n\nAlgorithm Advantages\n\nMore efficient than general DRI1 for structured problems\nExploits special structure for better performance\nMaintains weak order 2.0 with minimized constants\n\nReferences\n\nDebrabant, K. and Rößler A., \"Families of efficient second order Runge–Kutta methods for the weak approximation of Itô stochastic differential equations\", Applied Numerical Mathematics 59, pp. 582–594 (2009)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RI1","page":"High Weak Order Methods","title":"StochasticDiffEq.RI1","text":"Rößler A., Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations, SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009) DOI:10.1137/060673308\n\nRI1()\n\nRI1: Rößler Implicit Method 1 (High Weak Order)\n\nAdaptive weak order 2.0 method for Itô SDEs with deterministic order 3.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nGeneral weak convergence problems\nMonte Carlo simulations with various noise structures\nWhen weak order 2.0 is sufficient\nAlternative to DRI1 with different characteristics\n\nReferences\n\nRößler A., \"Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations\", SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RI3","page":"High Weak Order Methods","title":"StochasticDiffEq.RI3","text":"Rößler A., Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations, SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009) DOI:10.1137/060673308\n\nRI3()\n\nRI3: Rößler Implicit Method 3 (High Weak Order)\n\nAlternative adaptive weak order 2.0 method with different stability characteristics.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nAlternative to RI1 with different characteristics\nWhen RI1 performance is unsatisfactory\nBenchmarking different weak order 2.0 methods\n\nReferences\n\nRößler A., \"Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations\", SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RI5","page":"High Weak Order Methods","title":"StochasticDiffEq.RI5","text":"Rößler A., Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations, SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009) DOI:10.1137/060673308\n\nRI5()\n\nRI5: Rößler Implicit Method 5 (High Weak Order)\n\nAnother variant in the RI family of weak order 2.0 methods.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nPart of RI family comparison studies\nWhen other RI methods don't provide desired characteristics\nResearch applications requiring different RI variants\n\nReferences\n\nRößler A., \"Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations\", SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RI6","page":"High Weak Order Methods","title":"StochasticDiffEq.RI6","text":"Rößler A., Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations, SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009) DOI:10.1137/060673308\n\nRI6()\n\nRI6: Rößler Implicit Method 6 (High Weak Order)\n\nFinal method in the RI family with deterministic order 2 (lower than other RI methods).\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nWhen lower deterministic order is acceptable\nPotentially more efficient than RI1/RI3/RI5\nCompleting RI family comparisons\n\nAlgorithm Features\n\nLower deterministic order may reduce computational cost\nStill maintains weak order 2.0 for stochastic problems\nFinal variant in the comprehensive RI family\n\nReferences\n\nRößler A., \"Second Order Runge–Kutta Methods for Itô Stochastic Differential Equations\", SIAM J. Numer. Anal., 47, pp. 1713-1738 (2009)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RDI2WM","page":"High Weak Order Methods","title":"StochasticDiffEq.RDI2WM","text":"Debrabant, K. and Rößler A., Classification of Stochastic Runge–Kutta Methods for the Weak Approximation of Stochastic Differential Equations, Mathematics and Computers in Simulation 77, pp. 408-420 (2008) DOI:10.1016/j.matcom.2007.04.016\n\nRDI2WM()\n\nRDI2WM: Runge-Kutta Debrabant Implicit 2 Weak Method (High Weak Order)\n\nAdaptive weak order 2.0 method for Itô SDEs with deterministic order 2.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nWeak order 2.0 problems with adaptive stepping\nAlternative to DRI1 and RI methods\nWhen deterministic order 2.0 is sufficient\nMonte Carlo simulations requiring adaptive control\n\nReferences\n\nDebrabant, K. and Rößler A., \"Classification of Stochastic Runge–Kutta Methods for the Weak Approximation of Stochastic Differential Equations\", Mathematics and Computers in Simulation 77, pp. 408-420 (2008)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RDI3WM","page":"High Weak Order Methods","title":"StochasticDiffEq.RDI3WM","text":"Debrabant, K. and Rößler A., Classification of Stochastic Runge–Kutta Methods for the Weak Approximation of Stochastic Differential Equations, Mathematics and Computers in Simulation 77, pp. 408-420 (2008) DOI:10.1016/j.matcom.2007.04.016\n\nRDI3WM()\n\nRDI3WM: Runge-Kutta Debrabant Implicit 3 Weak Method (High Weak Order)\n\nAdaptive weak order 2.0 method with higher deterministic order 3.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nWhen both weak order 2.0 and deterministic order 3.0 are needed\nProblems with significant deterministic components\nAlternative to DRI1 with different characteristics\nHigh accuracy requirements for both stochastic and deterministic parts\n\nReferences\n\nDebrabant, K. and Rößler A., \"Classification of Stochastic Runge–Kutta Methods for the Weak Approximation of Stochastic Differential Equations\", Mathematics and Computers in Simulation 77, pp. 408-420 (2008)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RDI4WM","page":"High Weak Order Methods","title":"StochasticDiffEq.RDI4WM","text":"Debrabant, K. and Rößler A., Classification of Stochastic Runge–Kutta Methods for the Weak Approximation of Stochastic Differential Equations, Mathematics and Computers in Simulation 77, pp. 408-420 (2008) DOI:10.1016/j.matcom.2007.04.016\n\nRDI4WM()\n\nRDI4WM: Runge-Kutta Debrabant Implicit 4 Weak Method (High Weak Order)\n\nFourth variant in the RDI family with weak order 2.0 and deterministic order 3.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nFinal alternative in the RDI family\nWhen other RDI methods don't provide desired performance\nCompleting comprehensive RDI method comparisons\nResearch applications requiring all RDI variants\n\nReferences\n\nDebrabant, K. and Rößler A., \"Classification of Stochastic Runge–Kutta Methods for the Weak Approximation of Stochastic Differential Equations\", Mathematics and Computers in Simulation 77, pp. 408-420 (2008)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.W2Ito1","page":"High Weak Order Methods","title":"StochasticDiffEq.W2Ito1","text":"Tang, X., & Xiao, A., Efficient weak second-order stochastic Runge–Kutta methods for Itô stochastic differential equations, BIT Numerical Mathematics, 57, 241-260 (2017) DOI: 10.1007/s10543-016-0618-9\n\nW2Ito1()\n\nW2Ito1: Wang-Tang-Xiao Weak Order 2 Method (High Weak Order)\n\nEfficient weak second-order method for Itô SDEs with adaptive stepping.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Adaptive\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nModern efficient weak order 2.0 method\nWhen computational efficiency is important for weak convergence\nAlternative to older weak order 2.0 methods\nMonte Carlo simulations requiring good performance\n\nAlgorithm Features\n\nDesigned for computational efficiency\nGood balance of accuracy and cost for weak problems\nMore recent development than classical methods\n\nReferences\n\nTang, X., & Xiao, A., \"Efficient weak second-order stochastic Runge–Kutta methods for Itô stochastic differential equations\", BIT Numerical Mathematics, 57, 241-260 (2017)# Stratonovich sense\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.PL1WM","page":"High Weak Order Methods","title":"StochasticDiffEq.PL1WM","text":"Kloeden, P.E., Platen, E., Numerical Solution of Stochastic Differential Equations. Springer. Berlin Heidelberg (2011)\n\nPL1WM()\n\nPL1WM: Platen Weak Method 1 (High Weak Order)\n\nFixed step weak order 2.0 method from the classical Kloeden-Platen textbook.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Itô\n\nWhen to Use\n\nClassical reference implementation for weak order 2.0\nFixed step applications with predetermined step size\nEducational purposes and textbook examples\nBaseline comparison for more advanced methods\n\nAlgorithm Features\n\nWell-established classical method\nSimple implementation\nStandard reference from foundational SDE literature\n\nReferences\n\nKloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\", Springer. Berlin Heidelberg (2011)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.PL1WMA","page":"High Weak Order Methods","title":"StochasticDiffEq.PL1WMA","text":"Kloeden, P.E., Platen, E., Numerical Solution of Stochastic Differential Equations. Springer. Berlin Heidelberg (2011)\n\nPL1WMA()\n\nPL1WMA: Platen Weak Method 1 Additive (High Weak Order)\n\nSpecialized version of PL1WM optimized for additive noise problems.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: Additive noise only\nSDE interpretation: Itô\n\nWhen to Use\n\nAdditive noise problems with fixed step size\nWhen PL1WM is too general for additive structure\nClassical reference for additive noise weak methods\nEducational and benchmarking purposes\n\nAdditive Noise Structure\n\nSpecialized for SDEs of the form:\n\ndu = f(u,t)dt + σ(t) dW\n\nwhere diffusion σ doesn't depend on solution u.\n\nAlgorithm Features\n\nMore efficient than PL1WM for additive problems\nClassical foundation method\nSimplified implementation for additive case\n\nReferences\n\nKloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\", Springer. Berlin Heidelberg (2011)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RS1","page":"High Weak Order Methods","title":"StochasticDiffEq.RS1","text":"Rößler A., Second order Runge–Kutta methods for Stratonovich stochastic differential equations, BIT Numerical Mathematics 47, pp. 657-680 (2007) DOI:10.1007/s10543-007-0130-3\n\nRS1()\n\nRS1: Rößler Stratonovich Method 1 (High Weak Order)\n\nFixed step weak order 2.0 method specifically designed for Stratonovich SDEs.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Stratonovich\n\nWhen to Use\n\nStratonovich SDEs requiring weak order 2.0\nFixed step applications with predetermined step size\nProblems naturally formulated in Stratonovich interpretation\nWhen physical interpretation requires Stratonovich calculus\n\nStratonovich Interpretation\n\nOptimized for SDEs in Stratonovich form:\n\ndu = f(u,t)dt + g(u,t)∘dW\n\nwhere ∘ denotes Stratonovich integration.\n\nReferences\n\nRößler A., \"Second order Runge–Kutta methods for Stratonovich stochastic differential equations\", BIT Numerical Mathematics 47, pp. 657-680 (2007)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.RS2","page":"High Weak Order Methods","title":"StochasticDiffEq.RS2","text":"Rößler A., Second order Runge–Kutta methods for Stratonovich stochastic differential equations, BIT Numerical Mathematics 47, pp. 657-680 (2007) DOI:10.1007/s10543-007-0130-3\n\nRS2()\n\nRS2: Rößler Stratonovich Method 2 (High Weak Order)\n\nAlternative fixed step weak order 2.0 method for Stratonovich SDEs with higher deterministic order.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 3.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: All forms (diagonal, non-diagonal, non-commuting, scalar additive)\nSDE interpretation: Stratonovich\n\nWhen to Use\n\nStratonovich SDEs with significant deterministic components\nWhen higher deterministic accuracy than RS1 is needed\nFixed step applications requiring better deterministic performance\nBenchmarking against RS1\n\nAlgorithm Features\n\nHigher deterministic order than RS1\nMay be more expensive per step than RS1\nBetter for problems with large deterministic components\n\nReferences\n\nRößler A., \"Second order Runge–Kutta methods for Stratonovich stochastic differential equations\", BIT Numerical Mathematics 47, pp. 657-680 (2007)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.NON","page":"High Weak Order Methods","title":"StochasticDiffEq.NON","text":"Komori, Y., Weak second-order stochastic Runge–Kutta methods for non-commutative stochastic differential equations, Journal of Computational and Applied Mathematics 206, pp. 158 – 173 (2007) DOI:10.1016/j.cam.2006.06.006\n\nNON: High Weak Order Method Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 4). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.NON2","page":"High Weak Order Methods","title":"StochasticDiffEq.NON2","text":"NON2()\n\nNON2: Enhanced Non-commutative Stratonovich Method (High Weak Order)\n\nImproved version of the NON method with enhanced efficiency for non-commutative Stratonovich SDEs.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nTime stepping: Fixed step size\nNoise types: Non-commutative noise\nSDE interpretation: Stratonovich\n\nWhen to Use\n\nEnhanced version of NON with better efficiency\nNon-commutative Stratonovich SDEs requiring improved performance\nWhen NON is too expensive or inefficient\nModern alternative to classical NON method\n\nAlgorithm Features\n\nMore efficient than original NON method\nMaintains weak order 2.0 convergence\nEnhanced computational techniques\n\nReferences\n\nKomori, Y., & Burrage, K., \"Supplement: Efficient weak second order stochastic Runge–Kutta methods for non-commutative Stratonovich stochastic differential equations\", Journal of computational and applied mathematics, 235(17), pp. 5326-5329 (2011)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.COM","page":"High Weak Order Methods","title":"StochasticDiffEq.COM","text":"COM()\n\nCOM: Commutative Stratonovich Method (High Weak Order)\n\nFixed step method optimized for commutative Stratonovich SDEs.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: Depends on implementation\nTime stepping: Fixed step size\nNoise types: Commutative noise only\nSDE interpretation: Stratonovich\n\nWhen to Use\n\nCommutative Stratonovich SDEs\nWhen noise terms satisfy commutativity conditions\nMore efficient alternative to NON for commutative cases\nFixed step applications with commutative structure\n\nCommutative Noise\n\nOptimized for Stratonovich SDEs where:\n\n[g_i, g_j] = g_i(∂g_j/∂x) - g_j(∂g_i/∂x) = 0\n\nfor all noise terms.\n\nAlgorithm Features\n\nMore efficient than NON for commutative cases\nExploits commutativity for computational savings\nSpecialized for Stratonovich interpretation\n\nReferences\n\nKomori, Y., \"Weak order stochastic Runge–Kutta methods for commutative stochastic differential equations\", Journal of Computational and Applied Mathematics 203, pp. 57 – 79 (2007)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.SIEA","page":"High Weak Order Methods","title":"StochasticDiffEq.SIEA","text":"Tocino, A. and Vigo-Aguiar, J., Weak Second Order Conditions for Stochastic Runge- Kutta Methods, SIAM Journal on Scientific Computing 24, pp. 507 - 523 (2002) DOI:10.1137/S1064827501387814\n\nSIEA()\n\nSIEA: Stochastic Improved Euler A Method (High Weak Order)\n\nStochastic generalization of the improved Euler method for Itô SDEs.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: Diagonal and scalar additive noise\nSDE interpretation: Itô\n\nWhen to Use\n\nFixed step applications with diagonal/scalar additive noise\nWhen stochastic version of improved Euler is desired\nEducational purposes (connection to classical methods)\nBaseline for Tocino-Vigo-Aguiar method comparisons\n\nAlgorithm Features\n\nBased on classical improved Euler method\nSpecialized for additive noise structures\nSimple and well-understood foundation\n\nReferences\n\nTocino, A. and Vigo-Aguiar, J., \"Weak Second Order Conditions for Stochastic Runge-Kutta Methods\", SIAM Journal on Scientific Computing 24, pp. 507-523 (2002)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.SMEA","page":"High Weak Order Methods","title":"StochasticDiffEq.SMEA","text":"Tocino, A. and Vigo-Aguiar, J., Weak Second Order Conditions for Stochastic Runge- Kutta Methods, SIAM Journal on Scientific Computing 24, pp. 507 - 523 (2002) DOI:10.1137/S1064827501387814\n\nSMEA()\n\nSMEA: Stochastic Modified Euler A Method (High Weak Order)\n\nStochastic generalization of the modified Euler method for Itô SDEs.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: Diagonal and scalar additive noise\nSDE interpretation: Itô\n\nWhen to Use\n\nFixed step applications with diagonal/scalar additive noise\nWhen stochastic version of modified Euler is desired\nAlternative to SIEA with different characteristics\nEducational and comparison purposes\n\nAlgorithm Features\n\nBased on classical modified Euler method\nDifferent approach than SIEA for same problem class\nSpecialized for additive noise structures\n\nReferences\n\nTocino, A. and Vigo-Aguiar, J., \"Weak Second Order Conditions for Stochastic Runge-Kutta Methods\", SIAM Journal on Scientific Computing 24, pp. 507-523 (2002)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.SIEB","page":"High Weak Order Methods","title":"StochasticDiffEq.SIEB","text":"Tocino, A. and Vigo-Aguiar, J., Weak Second Order Conditions for Stochastic Runge- Kutta Methods, SIAM Journal on Scientific Computing 24, pp. 507 - 523 (2002) DOI:10.1137/S1064827501387814\n\nSIEB()\n\nSIEB: Stochastic Improved Euler B Method (High Weak Order)\n\nAlternative stochastic generalization of the improved Euler method.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: Diagonal and scalar additive noise\nSDE interpretation: Itô\n\nWhen to Use\n\nAlternative to SIEA with different coefficients\nFixed step applications requiring different stability properties\nComparing different improved Euler generalizations\nWhen SIEA performance is unsatisfactory\n\nAlgorithm Features\n\nVariant B of stochastic improved Euler approach\nDifferent coefficients than SIEA\nMay have different stability or accuracy characteristics\n\nReferences\n\nTocino, A. and Vigo-Aguiar, J., \"Weak Second Order Conditions for Stochastic Runge-Kutta Methods\", SIAM Journal on Scientific Computing 24, pp. 507-523 (2002)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/high_weak_order/#StochasticDiffEq.SMEB","page":"High Weak Order Methods","title":"StochasticDiffEq.SMEB","text":"Tocino, A. and Vigo-Aguiar, J., Weak Second Order Conditions for Stochastic Runge- Kutta Methods, SIAM Journal on Scientific Computing 24, pp. 507 - 523 (2002) DOI:10.1137/S1064827501387814\n\nSMEB()\n\nSMEB: Stochastic Modified Euler B Method (High Weak Order)\n\nAlternative stochastic generalization of the modified Euler method.\n\nMethod Properties\n\nStrong Order: Not optimized for strong convergence\nWeak Order: 2.0\nDeterministic Order: 2.0 (when noise = 0)\nTime stepping: Fixed step size\nNoise types: Diagonal and scalar additive noise\nSDE interpretation: Itô\n\nWhen to Use\n\nAlternative to SMEA with different coefficients\nFixed step applications requiring different characteristics\nCompleting Tocino-Vigo-Aguiar method family comparisons\nWhen SMEA performance is unsatisfactory\n\nAlgorithm Features\n\nVariant B of stochastic modified Euler approach\nDifferent coefficients than SMEA\nCompletes the family of Tocino-Vigo-Aguiar methods\n\nReferences\n\nTocino, A. and Vigo-Aguiar, J., \"Weak Second Order Conditions for Stochastic Runge-Kutta Methods\", SIAM Journal on Scientific Computing 24, pp. 507-523 (2002)\n\n\n\n\n\n","category":"type"},{"location":"basics/common_solver_opts/#solver_options","page":"Common Solver Options (Solve Keyword Arguments)","title":"Common Solver Options (Solve Keyword Arguments)","text":"","category":"section"},{"location":"basics/common_solver_opts/#CommonSolve.solve-Tuple{SciMLBase.AbstractDEProblem, Vararg{Any}}","page":"Common Solver Options (Solve Keyword Arguments)","title":"CommonSolve.solve","text":"solve(prob::AbstractDEProblem, alg::Union{AbstractDEAlgorithm,Nothing}; kwargs...)\n\nArguments\n\nThe only positional argument is alg which is optional. By default, alg = nothing. If alg = nothing, then solve dispatches to the DifferentialEquations.jl automated algorithm selection (if using DifferentialEquations was done, otherwise it will error with a MethodError).\n\nKeyword Arguments\n\nThe DifferentialEquations.jl universe has a large set of common arguments available for the solve function. These arguments apply to solve on any problem type and are only limited by limitations of the specific implementations.\n\nMany of the defaults depend on the algorithm or the package the algorithm derives from. Not all of the interface is provided by every algorithm. For more detailed information on the defaults and the available options for specific algorithms / packages, see the manual pages for the solvers of specific problems. To see whether a specific package is compatible with the use of a given option, see the Solver Compatibility Chart\n\nDefault Algorithm Hinting\n\nTo help choose the default algorithm, the keyword argument alg_hints is provided to solve. alg_hints is a Vector{Symbol} which describe the problem at a high level to the solver. The options are:\n\n:auto vs :nonstiff vs :stiff - Denotes the equation as nonstiff/stiff. :auto allow the default handling algorithm to choose stiffness detection algorithms. The default handling defaults to using :auto.\n\nCurrently unused options include:\n\n:interpolant - Denotes that a high-precision interpolation is important.\n:memorybound - Denotes that the solver will be memory bound.\n\nThis functionality is derived via the benchmarks in SciMLBenchmarks.jl\n\nSDE Specific Alghints\n\n:additive - Denotes that the underlying SDE has additive noise.\n:stratonovich - Denotes that the solution should adhere to the Stratonovich interpretation.\n\nOutput Control\n\nThese arguments control the output behavior of the solvers. It defaults to maximum output to give the best interactive user experience, but can be reduced all the way to only saving the solution at the final timepoint.\n\nThe following options are all related to output control. See the \"Examples\" section at the end of this page for some example usage.\n\ndense: Denotes whether to save the extra pieces required for dense (continuous) output. Default is save_everystep && isempty(saveat) for algorithms which have the ability to produce dense output, i.e. by default it's true unless the user has turned off saving on steps or has chosen a saveat value. If dense=false, the solution still acts like a function, and sol(t) is a linear interpolation between the saved time points.\nsaveat: Denotes specific times to save the solution at, during the solving phase. The solver will save at each of the timepoints in this array in the most efficient manner available to the solver. If only saveat is given, then the arguments save_everystep and dense are false by default. If saveat is given a number, then it will automatically expand to tspan[1]:saveat:tspan[2]. For methods where interpolation is not possible, saveat may be equivalent to tstops. The default value is [].\nsave_idxs: Denotes the indices for the components of the equation to save. Defaults to saving all indices. For example, if you are solving a 3-dimensional ODE, and given save_idxs = [1, 3], only the first and third components of the solution will be outputted. Notice that of course in this case the outputted solution will be two-dimensional.\ntstops: Denotes extra times that the timestepping algorithm must step to. This should be used to help the solver deal with discontinuities and singularities, since stepping exactly at the time of the discontinuity will improve accuracy. If a method cannot change timesteps (fixed timestep multistep methods), then tstops will use an interpolation, matching the behavior of saveat. If a method cannot change timesteps and also cannot interpolate, then tstops must be a multiple of dt or else an error will be thrown. tstops may also be a function tstops(p, tspan), accepting the parameter object and tspan, returning the vector of time points to stop at. Default is [].\nd_discontinuities: Denotes locations of discontinuities in low order derivatives. This will force FSAL algorithms which assume derivative continuity to re-evaluate the derivatives at the point of discontinuity. The default is [].\nsave_everystep: Saves the result at every step. Default is true if isempty(saveat).\nsave_on: Denotes whether intermediate solutions are saved. This overrides the settings of dense, saveat and save_everystep and is used by some applications to manually turn off saving temporarily. Everyday use of the solvers should leave this unchanged. Defaults to true.\nsave_start: Denotes whether the initial condition should be included in the solution type as the first timepoint. Defaults to true.\nsave_end: Denotes whether the final timepoint is forced to be saved, regardless of the other saving settings. Defaults to true.\ninitialize_save: Denotes whether to save after the callback initialization phase (when u_modified=true). Defaults to true.\n\nNote that dense requires save_everystep=true and saveat=false. If you need additional saving while keeping dense output, see the SavingCallback in the Callback Library.\n\nStepsize Control\n\nThese arguments control the timestepping routines.\n\nBasic Stepsize Control\n\nThese are the standard options for controlling stepping behavior. Error estimates do the comparison\n\nerr_scaled = err(abstol + max(uprevu)*reltol)\n\nThe scaled error is guaranteed to be <1 for a given local error estimate (note: error estimates are local unless the method specifies otherwise). abstol controls the non-scaling error and thus can be thought of as the error around zero. reltol scales with the size of the dependent variables and so one can interpret reltol=1e-3 as roughly being (locally) correct to 3 digits. Note tolerances can be specified element-wise by passing a vector whose size matches u0.\n\nadaptive: Turns on adaptive timestepping for appropriate methods. Default is true.\nabstol: Absolute tolerance in adaptive timestepping. This is the tolerance on local error estimates, not necessarily the global error (though these quantities are related). Defaults to 1e-6 on deterministic equations (ODEs/DDEs/DAEs) and 1e-2 on stochastic equations (SDEs/RODEs).\nreltol: Relative tolerance in adaptive timestepping.  This is the tolerance on local error estimates, not necessarily the global error (though these quantities are related). Defaults to 1e-3 on deterministic equations (ODEs/DDEs/DAEs) and 1e-2 on stochastic equations (SDEs/RODEs).\ndt: Sets the initial stepsize. This is also the stepsize for fixed timestep methods. Defaults to an automatic choice if the method is adaptive.\ndtmax: Maximum dt for adaptive timestepping. Defaults are package-dependent.\ndtmin: Minimum dt for adaptive timestepping. Defaults are package-dependent.\nforce_dtmin: Declares whether to continue, forcing the minimum dt usage. Default is false, which has the solver throw a warning and exit early when encountering the minimum dt. Setting this true allows the solver to continue, never letting dt go below dtmin (and ignoring error tolerances in those cases). Note that true is not compatible with most interop packages.\n\nFixed Stepsize Usage\n\nNote that if a method does not have adaptivity, the following rules apply:\n\nIf dt is set, then the algorithm will step with size dt each iteration.\nIf tstops and dt are both set, then the algorithm will step with either a size dt, or use a smaller step to hit the tstops point.\nIf tstops is set without dt, then the algorithm will step directly to each value in tstops\nIf neither dt nor tstops are set, the solver will throw an error.\n\nAdvanced Adaptive Stepsize Control\n\nThese arguments control more advanced parts of the internals of adaptive timestepping and are mostly used to make it more efficient on specific problems. For detailed explanations of the timestepping algorithms, see the timestepping descriptions\n\ninternalnorm: The norm function internalnorm(u,t) which error estimates are calculated. Required are two dispatches: one dispatch for the state variable and the other on the elements of the state variable (scalar norm). Defaults are package-dependent.\ncontroller: Possible examples are IController, PIController, PIDController, PredictiveController. Default is algorithm-dependent.\ngamma: The risk-factor γ in the q equation for adaptive timestepping of the controllers using it. Default is algorithm-dependent.\nbeta1: The Lund stabilization α parameter. Default is algorithm-dependent.\nbeta2: The Lund stabilization β parameter. Default is algorithm-dependent.\nqmax: Defines the maximum value possible for the adaptive q. Default is algorithm-dependent.\nqmin: Defines the minimum value possible for the adaptive q. Default is algorithm-dependent.\nqsteady_min: Defines the minimum for the range around 1 where the timestep is held constant. Default is algorithm-dependent.\nqsteady_max: Defines the maximum for the range around 1 where the timestep is held constant. Default is algorithm-dependent.\nqoldinit: The initial qold in stabilization stepping. Default is algorithm-dependent.\nfailfactor: The amount to decrease the timestep by if the Newton iterations of an implicit method fail. Default is 2.\n\nMemory Optimizations\n\ncalck: Turns on and off the internal ability for intermediate interpolations (also known as intermediate density). Not the same as dense, which is post-solution interpolation. This defaults to dense || !isempty(saveat) ||  \"no custom callback is given\". This can be used to turn off interpolations (to save memory) if one isn't using interpolations when a custom callback is used. Another case where this may be used is to turn on interpolations for usage in the integrator interface even when interpolations are used nowhere else. Note that this is only required if the algorithm doesn't have a free or lazy interpolation (DP8()). If calck = false, saveat cannot be used. The rare keyword calck can be useful in event handling.\nalias: an AbstractAliasSpecifier object that holds fields specifying which variables to alias when solving. For example, to tell an ODE solver to alias the u0 array, you can use an ODEAliases object,  and the alias_u0 keyword argument, e.g. solve(prob,alias = ODEAliases(alias_u0 = true)).  For more information on what can be aliased for each problem type, see the documentation for the AbstractAliasSpecifier associated with that problem type. Set to true to alias every variable possible, or to false to disable aliasing. Defaults to an AbstractAliasSpecifier instance with nothing for all fields, which tells the solver to use the default behavior.\n\nMiscellaneous\n\nmaxiters: Maximum number of iterations before stopping. Defaults to 1e5.\ncallback: Specifies a callback. Defaults to a callback function which performs the saving routine. For more information, see the Event Handling and Callback Functions manual page.\ninitializealg: The initialization algorithm for DAEs and ODEs with constraints. Available options include:\nDefaultInit() (default): Automatically chooses the best initialization algorithm\nCheckInit(): Only checks that initial conditions are consistent, errors if not\nNoInit(): Skip initialization completely (for when you know conditions are consistent)\nOverrideInit(): Use problem's initialization_data (typically from ModelingToolkit)\nBrownBasicInit(): Brown's basic initialization algorithm for index-1 DAEs\nShampineCollocationInit(): Shampine's collocation initialization for general DAEs\nSee the DAE initialization documentation for more details.\nisoutofdomain: Specifies a function isoutofdomain(u,p,t) where, when it returns true, it will reject the timestep. Disabled by default.\nunstable_check: Specifies a function unstable_check(dt,u,p,t) where, when it returns true, it will cause the solver to exit and throw a warning. Defaults to any(isnan,u), i.e. checking if any value is a NaN.\nverbose: Toggles whether warnings are thrown when the solver exits early. Defaults to true.\nmerge_callbacks: Toggles whether to merge prob.callback with the solve keyword argument callback. Defaults to true.\nwrap: Toggles whether to wrap the solution if prob.problem_type has a preferred alternate wrapper type for the solution. Useful when speed, but not shape of solution is important. Defaults to Val(true). Val(false) will cancel wrapping the solution.\nu0: The initial condition, overrides the one defined in the problem struct. Defaults to nothing (no override, use the u0 defined in prob).\np: The parameters, overrides the one defined in the problem struct. Defaults to nothing (no override, use the p defined in prob).\n\nProgress Monitoring\n\nThese arguments control the usage of the progressbar in ProgressLogging.jl compatible environments. For information on setting up progress bars in VS Code and other environments, see the progress bar documentation.\n\nprogress: Turns on/off the Juno progressbar. Default is false.\nprogress_steps: Numbers of steps between updates of the progress bar. Default is 1000.\nprogress_name: Controls the name of the progressbar. Default is the name of the problem type.\nprogress_message: Controls the message with the progressbar. Defaults to showing dt, t, the maximum of u.\nprogress_id: Controls the ID of the progress log message to distinguish simultaneous simulations.\n\nError Calculations\n\nIf you are using the test problems (ex: ODETestProblem), then the following options control the errors which are calculated:\n\ntimeseries_errors: Turns on and off the calculation of errors at the steps which were taken, such as the l2 error. Default is true.\ndense_errors: Turns on and off the calculation of errors at the steps which require dense output and calculate the error at 100 evenly-spaced points throughout tspan. An example is the L2 error. Default is false.\n\nSensitivity Algorithms (sensealg)\n\nsensealg is used for choosing the way the automatic differentiation is performed. For more information, see the documentation for SciMLSensitivity: https://docs.sciml.ai/SciMLSensitivity/stable/\n\nExamples\n\nThe following lines are examples of how one could use the configuration of solve(). For these examples a 3-dimensional ODE problem is assumed, however the extension to other types is straightforward.\n\nsolve(prob, AlgorithmName()) : The \"default\" setting, with a user-specified algorithm (given by AlgorithmName()). All parameters get their default values. This means that the solution is saved at the steps the Algorithm stops internally and dense output is enabled if the chosen algorithm allows for it.\nAll other integration parameters (e.g. stepsize) are chosen automatically.\nsolve(prob, saveat = 0.01, abstol = 1e-9, reltol = 1e-9) : Standard setting for accurate output at specified (and equidistant) time intervals, used for e.g. Fourier Transform. The solution is given every 0.01 time units, starting from tspan[1]. The solver used is Tsit5() since no keyword alg_hits is given.\nsolve(prob, maxiters = 1e7, progress = true, save_idxs = [1]) : Using longer maximum number of solver iterations can be useful when a given tspan is very long. This example only saves the first of the variables of the system, either to save size or because the user does not care about the others. Finally, with progress = true you are enabling the progress bar.\n\n\n\n\n\n","category":"method"},{"location":"api/ordinarydiffeq/explicit/Feagin/#OrdinaryDiffEqFeagin","page":"OrdinaryDiffEqFeagin","title":"OrdinaryDiffEqFeagin","text":"Ultra-high-order explicit Runge-Kutta methods for non-stiff problems at extremely low tolerances (< 1e-30). These methods are designed for applications requiring extreme precision, typically used with higher-precision number types like BigFloat.","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Key-Properties","page":"OrdinaryDiffEqFeagin","title":"Key Properties","text":"Feagin methods provide:\n\nUltra-high-order accuracy (10th, 12th, and 14th order)\nExtreme precision capabilities for very low tolerance requirements\nCompatibility with arbitrary precision arithmetic (BigFloat, Float128)\nSpecialized for very demanding applications requiring maximum accuracy","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#When-to-Use-Feagin-Methods","page":"OrdinaryDiffEqFeagin","title":"When to Use Feagin Methods","text":"These methods are recommended for:\n\nExtremely low tolerance problems (< 1e-30)\nArbitrary precision arithmetic applications using BigFloat or Float128\nUltra-high precision requirements where standard methods are insufficient\nResearch applications requiring maximum possible accuracy\nLong-time integration where error accumulation must be minimized to extreme levels","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Important-Limitations","page":"OrdinaryDiffEqFeagin","title":"Important Limitations","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Theoretical-vs-Practical-Performance","page":"OrdinaryDiffEqFeagin","title":"Theoretical vs Practical Performance","text":"Very good theoretical efficiency due to high order and optimized coefficients\nPoor practical performance in benchmarks due to bad error estimators and adaptivity issues\nGenerally recommend Vern9 instead as it tends to be more efficient in practice despite lower theoretical order","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Performance-Considerations","page":"OrdinaryDiffEqFeagin","title":"Performance Considerations","text":"May be less efficient than Vern9 even for very low tolerance problems\nOutperformed by extrapolation methods at extremely low tolerances due to adaptive order\nPotential efficiency for >128-bit numbers but no practical cases found yet where this is actually true\nShould always be tested against Vern9 and extrapolation methods","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Solver-Selection-Guide","page":"OrdinaryDiffEqFeagin","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Extreme-precision-(-1e-30)","page":"OrdinaryDiffEqFeagin","title":"Extreme precision (< 1e-30)","text":"Feagin14: 14th-order method for maximum accuracy\nFeagin12: 12th-order method, balance of accuracy and efficiency\nFeagin10: 10th-order method for moderate extreme precision","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Strongly-recommended-alternatives","page":"OrdinaryDiffEqFeagin","title":"Strongly recommended alternatives","text":"For most very low tolerance problems: Use Vern9 first (more efficient in practice despite lower theoretical order)\nFor extremely low tolerances: Consider extrapolation methods for adaptive order\nFor >128-bit precision: These methods may be more efficient, but no practical cases found yet\nAlways benchmark: Compare performance with Vern9 and extrapolation methods before choosing Feagin methods","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Usage-Guidelines","page":"OrdinaryDiffEqFeagin","title":"Usage Guidelines","text":"Best with BigFloat or Float128 number types\nUseful in Float128 precision range but test against other algorithms\nConsider problem-specific characteristics when choosing order level\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqFeagin\", \"Feagin14\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#Full-list-of-solvers","page":"OrdinaryDiffEqFeagin","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Feagin/#OrdinaryDiffEqFeagin.Feagin10","page":"OrdinaryDiffEqFeagin","title":"OrdinaryDiffEqFeagin.Feagin10","text":"Feagin10(; step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nExplicit Runge-Kutta Method.  Feagin's 10th-order method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{feagin2012high, title={High-order explicit Runge-Kutta methods using m-symmetry}, author={Feagin, Terry}, year={2012}, publisher={Neural, Parallel \\& Scientific Computations} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Feagin/#OrdinaryDiffEqFeagin.Feagin12","page":"OrdinaryDiffEqFeagin","title":"OrdinaryDiffEqFeagin.Feagin12","text":"Feagin12(; step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nExplicit Runge-Kutta Method.  Feagin's 12th-order method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{feagin2012high, title={High-order explicit Runge-Kutta methods using m-symmetry}, author={Feagin, Terry}, year={2012}, publisher={Neural, Parallel \\& Scientific Computations} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Feagin/#OrdinaryDiffEqFeagin.Feagin14","page":"OrdinaryDiffEqFeagin","title":"OrdinaryDiffEqFeagin.Feagin14","text":"Feagin14(; step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nExplicit Runge-Kutta Method.  Feagin's 14th-order method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{feagin2009explicit, title={An Explicit Runge-Kutta Method of Order Fourteen}, author={Feagin, Terry}, year={2009}, publisher={Numerical Algorithms} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/Extrapolation/#StiffExtrapolation","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation","text":"Solvers based on within method parallelism. These solvers perform well for medium sized systems of ordinary differential equations, of about 20 to 500 equations, at low tolerances.\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqExtrapolation\", \"ImplicitEulerBarycentricExtrapolation\")","category":"section"},{"location":"api/ordinarydiffeq/implicit/Extrapolation/#Full-list-of-solvers","page":"OrdinaryDiffEqExtrapolation","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/Extrapolation/#OrdinaryDiffEqExtrapolation.ImplicitEulerExtrapolation","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation.ImplicitEulerExtrapolation","text":"ImplicitEulerExtrapolation(; chunk_size = Val{0}(),\n                             autodiff = AutoForwardDiff(),\n                             standardtag = Val{true}(),\n                             concrete_jac = nothing,\n                             diff_type = Val{:forward},\n                             linsolve = nothing,\n                             precs = DEFAULT_PRECS,\n                             max_order = 12,\n                             min_order = 3,\n                             init_order = 5,\n                             thread = OrdinaryDiffEq.False(),\n                             sequence = :harmonic)\n\nParallelized Explicit Extrapolation Method. Extrapolation of implicit Euler method with Romberg sequence. Similar to Hairer's SEULEX.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ImplicitEulerExtrapolation(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nmax_order: maximum order of the adaptive order algorithm.\nmin_order: minimum order of the adaptive order algorithm.\ninit_order: initial order of the adaptive order algorithm.\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch.\n\nReferences\n\n@inproceedings{elrod2022parallelizing,   title={Parallelizing explicit and implicit extrapolation methods for ordinary differential equations},   author={Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher and others},   booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},   pages={1–9},   year={2022},   organization={IEEE}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/Extrapolation/#OrdinaryDiffEqExtrapolation.ImplicitDeuflhardExtrapolation","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation.ImplicitDeuflhardExtrapolation","text":"ImplicitDeuflhardExtrapolation(; chunk_size = Val{0}(),\n                                 autodiff = AutoForwardDiff(),\n                                 standardtag = Val{true}(),\n                                 concrete_jac = nothing,\n                                 diff_type = Val{:forward},\n                                 linsolve = nothing,\n                                 precs = DEFAULT_PRECS,\n                                 max_order = 10,\n                                 min_order = 1,\n                                 init_order = 5,\n                                 thread = OrdinaryDiffEq.False(),\n                                 sequence = :harmonic)\n\nParallelized Explicit Extrapolation Method. Midpoint extrapolation using Barycentric coordinates.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ImplicitDeuflhardExtrapolation(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nmax_order: maximum order of the adaptive order algorithm.\nmin_order: minimum order of the adaptive order algorithm.\ninit_order: initial order of the adaptive order algorithm.\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch.\n\nReferences\n\n@inproceedings{elrod2022parallelizing,   title={Parallelizing explicit and implicit extrapolation methods for ordinary differential equations},   author={Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher and others},   booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},   pages={1–9},   year={2022},   organization={IEEE}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/Extrapolation/#OrdinaryDiffEqExtrapolation.ImplicitHairerWannerExtrapolation","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation.ImplicitHairerWannerExtrapolation","text":"ImplicitHairerWannerExtrapolation(; chunk_size = Val{0}(),\n                                    autodiff = AutoForwardDiff(),\n                                    standardtag = Val{true}(),\n                                    concrete_jac = nothing,\n                                    diff_type = Val{:forward},\n                                    linsolve = nothing,\n                                    precs = DEFAULT_PRECS,\n                                    max_order = 10,\n                                    min_order = 2,\n                                    init_order = 5,\n                                    thread = OrdinaryDiffEq.False(),\n                                    sequence = :harmonic)\n\nParallelized Explicit Extrapolation Method. Midpoint extrapolation using Barycentric coordinates,     following Hairer's SODEX in the adaptivity behavior.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ImplicitHairerWannerExtrapolation(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nmax_order: maximum order of the adaptive order algorithm.\nmin_order: minimum order of the adaptive order algorithm.\ninit_order: initial order of the adaptive order algorithm.\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch.\n\nReferences\n\n@inproceedings{elrod2022parallelizing,   title={Parallelizing explicit and implicit extrapolation methods for ordinary differential equations},   author={Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher and others},   booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},   pages={1–9},   year={2022},   organization={IEEE}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/Extrapolation/#OrdinaryDiffEqExtrapolation.ImplicitEulerBarycentricExtrapolation","page":"OrdinaryDiffEqExtrapolation","title":"OrdinaryDiffEqExtrapolation.ImplicitEulerBarycentricExtrapolation","text":"ImplicitEulerBarycentricExtrapolation(; chunk_size = Val{0}(),\n                                        autodiff = AutoForwardDiff(),\n                                        standardtag = Val{true}(),\n                                        concrete_jac = nothing,\n                                        diff_type = Val{:forward},\n                                        linsolve = nothing,\n                                        precs = DEFAULT_PRECS,\n                                        max_order = 10,\n                                        min_order = 3,\n                                        init_order = 5,\n                                        thread = OrdinaryDiffEq.False(),\n                                        sequence = :harmonic,\n                                        sequence_factor = 2)\n\nParallelized Explicit Extrapolation Method. Euler extrapolation using Barycentric coordinates,     following Hairer's SODEX in the adaptivity behavior.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ImplicitEulerBarycentricExtrapolation(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nmax_order: maximum order of the adaptive order algorithm.\nmin_order: minimum order of the adaptive order algorithm.\ninit_order: initial order of the adaptive order algorithm.\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nsequence: the step-number sequences, also called the subdividing sequence. Possible values are :harmonic, :romberg or :bulirsch.\nsequence_factor: denotes which even multiple of sequence to take while evaluating internal discretizations.\n\nReferences\n\n@inproceedings{elrod2022parallelizing,   title={Parallelizing explicit and implicit extrapolation methods for ordinary differential equations},   author={Elrod, Chris and Ma, Yingbo and Althaus, Konstantin and Rackauckas, Christopher and others},   booktitle={2022 IEEE High Performance Extreme Computing Conference (HPEC)},   pages={1–9},   year={2022},   organization={IEEE}}\n\n\n\n\n\n","category":"type"},{"location":"basics/plot/#plot","page":"Plot Functions","title":"Plot Functions","text":"","category":"section"},{"location":"basics/plot/#Standard-Plots-Using-the-Plot-Recipe","page":"Plot Functions","title":"Standard Plots Using the Plot Recipe","text":"Plotting functionality is provided by recipes to Plots.jl. To plot solutions, simply call the plot(type) after importing Plots.jl and the plotter will generate appropriate plots.\n\n#]add Plots # You need to install Plots.jl before your first time using it!\nimport Plots\nPlots.plot(sol) # Plots the solution\n\nMany of the types defined in the DiffEq universe, such as ODESolution, ConvergenceSimulation WorkPrecision, etc. have plot recipes to handle the default plotting behavior. Plots can be customized using all the keyword arguments provided by Plots.jl. For example, we can change the plotting backend to the GR package and put a title on the plot by doing:\n\ngr()\nPlots.plot(sol, title = \"I Love DiffEqs!\")\n\nThen to save the plot, use savefig, for example:\n\nsavefig(\"myplot.png\")","category":"section"},{"location":"basics/plot/#Density","page":"Plot Functions","title":"Density","text":"If the problem was solved with dense=true, then denseplot controls whether to use the dense function for generating the plot, and plotdensity is the number of evenly-spaced points (in time) to plot. For example:\n\nPlots.plot(sol, denseplot = false)\n\nmeans “only plot the points which the solver stepped to”, while:\n\nPlots.plot(sol, plotdensity = 1000)\n\nmeans to plot 1000 points using the dense function (since denseplot=true by default).","category":"section"},{"location":"basics/plot/#plot_vars","page":"Plot Functions","title":"Choosing Variables","text":"In the plot command, one can choose the variables to be plotted in each plot. The master form is:\n\nidxs = [(f1, 0, 1), (f2, 1, 3), (f3, 4, 5)]\n\nwhich could be used to plot f1(var₀, var₁), f2(var₁, var₃), and f3(var₄, var₅), all on the same graph. (0 is considered to be time, or the independent variable). Functions f1, f2 and f3 should take in scalars and return a tuple. If no function is given, for example,\n\nidxs = [(0, 1), (1, 3), (4, 5)]\n\nthis would mean “plot var₁(t) vs t (time), var₃(var₁) vs var₁, and var₅(var₄) vs var₄ all on the same graph, putting the independent variables (t, var₁ and var₄) on the x-axis.” While this can be used for everything, the following conveniences are provided:\n\nEverywhere in a tuple position where we only find an integer, this variable is plotted as a function of time.  For example, the list above is equivalent to:\nidxs = [1, (1, 3), (4, 5)]\nand\nidxs = [1, 3, 4]\nis the most concise way to plot the variables 1, 3, and 4 as a function of time.\nIt is possible to omit the list if only one plot is wanted: (2,3) and 4 are respectively equivalent to [(2,3)] and [(0,4)].\nA tuple containing one or several lists will be expanded by associating corresponding elements of the lists with each other:\nidxs = ([1, 2, 3], [4, 5, 6])\nis equivalent to\nidxs = [(1, 4), (2, 5), (3, 6)]\nand\nidxs = (1, [2, 3, 4])\nis equivalent to\nidxs = [(1, 2), (1, 3), (1, 4)]\nInstead of using integers, one can use the symbols from a ParameterizedFunction. For example, idxs=(:x,:y) will replace the symbols with the integer values for components :x and :y.\nn-dimensional groupings are allowed. For example, (1,2,3,4,5) would be a 5-dimensional plot between the associated variables.","category":"section"},{"location":"basics/plot/#Complex-Numbers-and-High-Dimensional-Plots","page":"Plot Functions","title":"Complex Numbers and High Dimensional Plots","text":"The recipe library DimensionalPlotRecipes.jl is provided for extra functionality on high dimensional numbers (complex numbers) and other high dimensional plots. See the README for more details on the extra controls that exist.","category":"section"},{"location":"basics/plot/#Timespan","page":"Plot Functions","title":"Timespan","text":"A plotting timespan can be chosen by the tspan argument in plot. For example:\n\nPlots.plot(sol, tspan = (0.0, 40.0))\n\nonly plots between t=0.0 and t=40.0. If denseplot=true these bounds will be respected exactly. Otherwise, the first point inside and last point inside the interval will be plotted, i.e. no points outside the interval will be plotted.","category":"section"},{"location":"basics/plot/#Example","page":"Plot Functions","title":"Example","text":"import DifferentialEquations as DE, Plots\nfunction lorenz(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nu0 = [1.0, 5.0, 10.0]\ntspan = (0.0, 100.0)\np = (10.0, 28.0, 8 / 3)\nprob = DE.ODEProblem(lorenz, u0, tspan, p)\nsol = DE.solve(prob)\nxyzt = Plots.plot(sol, plotdensity = 10000, lw = 1.5)\nxy = Plots.plot(sol, plotdensity = 10000, idxs = (1, 2))\nxz = Plots.plot(sol, plotdensity = 10000, idxs = (1, 3))\nyz = Plots.plot(sol, plotdensity = 10000, idxs = (2, 3))\nxyz = Plots.plot(sol, plotdensity = 10000, idxs = (1, 2, 3))\nPlots.plot(Plots.plot(xyzt, xyz), Plots.plot(xy, xz, yz, layout = (1, 3), w = 1), layout = (\n    2, 1))\n\nAn example using the functions:\n\nf(x, y, z) = (sqrt(x^2 + y^2 + z^2), x)\nPlots.plot(sol, idxs = (f, 1, 2, 3))\n\nor the norm over time:\n\nf(t, x, y, z) = (t, sqrt(x^2 + y^2 + z^2))\nPlots.plot(sol, idxs = (f, 0, 1, 2, 3))","category":"section"},{"location":"basics/plot/#Animations","page":"Plot Functions","title":"Animations","text":"Using the iterator interface over the solutions, animations can also be generated via the animate(sol) command. One can choose the filename to save to via animate(sol,filename), while the frames per second fps and the density of steps to show every can be specified via keyword arguments. The rest of the arguments will be directly passed to the plot recipe to be handled as normal. For example, we can animate our solution with a larger line-width which saves every 4th frame via:\n\n#]add ImageMagick # You may need to install ImageMagick.jl before your first time using it!\n#using ImageMagick # Some installations require using ImageMagick for good animations\nanimate(sol, lw = 3, every = 4)\n\nPlease see Plots.jl's documentation for more information on the available attributes.","category":"section"},{"location":"basics/plot/#Plotting-Without-the-Plot-Recipe","page":"Plot Functions","title":"Plotting Without the Plot Recipe","text":"What if you don't want to use Plots.jl? Odd choice, but that's okay! If the differential equation was described by a vector of values, then the solution object acts as an AbstractMatrix sol[i,j] for the ith variable at timepoint j. You can use this to plot solutions. For example, in PyPlot, Gadfly, GR, etc., you can do the following to plot the timeseries:\n\nPlots.plot(sol.t, sol')\n\nsince these plot along the columns, and sol' has the timeseries along the column. Phase plots can be done similarly, for example:\n\nPlots.plot(sol[i, :], sol[j, :], sol[k, :])\n\nis a 3D phase plot between variables i, j, and k.\n\nNotice that this does not use the interpolation. When not using the plot recipe, the interpolation must be done manually. For example:\n\nn = 101 #number of timepoints\nts = range(0, stop = 1, length = n)\nPlots.plot(sol(ts, idxs = i), sol(ts, idxs = j), sol(ts, idxs = k))\n\nis the phase space using values 0.01 apart in time.","category":"section"},{"location":"#DifferentialEquations.jl:-Efficient-Differential-Equation-Solving-in-Julia","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","text":"This is a suite for numerically solving differential equations written in Julia and available for use in Julia, Python, and R. The purpose of this package is to supply efficient Julia implementations of solvers for various differential equations. Equations within the realm of this package include:\n\nDiscrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations)\nOrdinary differential equations (ODEs)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods)\nStochastic ordinary differential equations (SODEs or SDEs)\nStochastic differential-algebraic equations (SDAEs)\nRandom differential equations (RODEs or RDEs)\nDifferential algebraic equations (DAEs)\nDelay differential equations (DDEs)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions)\n(Stochastic) partial differential equations ((S)PDEs) (with both finite difference and finite element methods)\n\nThe well-optimized DifferentialEquations solvers benchmark as some of the fastest implementations, using classic algorithms and ones from recent research which routinely outperform the “standard” C/Fortran methods, and include algorithms optimized for high-precision and HPC applications. At the same time, it wraps the classic C/Fortran methods, making it easy to switch over to them whenever necessary. Solving differential equations with different methods from different languages and packages can be done by changing one line of code, allowing for easy benchmarking to ensure you are using the fastest method possible.\n\nDifferentialEquations.jl integrates with the Julia package sphere with:\n\nGPU acceleration through CUDA.jl and DiffEqGPU.jl\nAutomated sparsity detection with Symbolics.jl\nAutomatic Jacobian coloring with SparseDiffTools.jl, allowing for fast solutions to problems with sparse or structured (Tridiagonal, Banded, BlockBanded, etc.) Jacobians\nAllowing the specification of linear solvers for maximal efficiency with LinearSolve.jl\nProgress meter integration with the Visual Studio Code IDE for estimated time to solution\nAutomatic plotting of time series and phase plots\nBuilt-in interpolations\nWraps for common C/Fortran methods like Sundials and Hairer's radau\nArbitrary precision with BigFloats and ArbNumerics.jl\nArbitrary array types, allowing the definition of differential equations on matrices and distributed arrays\nUnit checked arithmetic with Unitful\n\nAdditionally, DifferentialEquations.jl comes with built-in analysis features, including:\n\nForward and Adjoint Sensitivity Analysis (Automatic Differentiation) for fast gradient computations\nParameter Estimation and Bayesian Analysis\nNeural differential equations with DiffEqFlux.jl for efficient scientific machine learning (scientific ML) and scientific AI.\nAutomatic distributed, multithreaded, and GPU Parallel Ensemble Simulations\nGlobal Sensitivity Analysis\nUncertainty Quantification","category":"section"},{"location":"#Contributing","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Contributing","text":"If you're interested in contributing, please see the Developer Documentation.\nPlease refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"section"},{"location":"#Supporting-and-Citing","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Supporting and Citing","text":"The software in this ecosystem was developed as part of academic research. If you would like to help support it, please star the repository, as such metrics may help us secure funding in the future. If you use SciML software as part of your research, teaching, or other activities, we would be grateful if you could cite our work as necessary for any use of DifferentialEquations.jl or the packages that are maintained as part of its suite (OrdinaryDiffEq.jl, Sundials.jl, DiffEqDevTools.jl, etc.).\n\n@article{rackauckas2017differentialequations,\n  title={Differential{E}quations.jl--a performant and feature-rich ecosystem for solving differential equations in {J}ulia},\n  author={Rackauckas, Christopher and Nie, Qing},\n  journal={Journal of Open Research Software},\n  volume={5},\n  number={1},\n  year={2017},\n  publisher={Ubiquity Press}\n}\n\nAdditionally, many of the solvers utilize novel algorithms, and if these algorithms are used, we ask that you cite the methods. Please see our citation page for guidelines.","category":"section"},{"location":"#Getting-Started:-Installation-And-First-Steps","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Getting Started: Installation And First Steps","text":"","category":"section"},{"location":"#Installing-from-Julia","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Installing from Julia","text":"To install the package, use the following command inside the Julia REPL:\n\nimport Pkg\nPkg.add(\"DifferentialEquations\")\n\nTo load the package, use the command:\n\nimport DifferentialEquations as DE\n\nThis will add solvers and dependencies for all kinds of Differential Equations (e.g. ODEs or SDEs etc., see the Supported Equations section below). If you are interested in only one type of equation solver of DifferentialEquations.jl or simply want a more lightweight version, see the Reduced Compile Time and Low Dependency Usage page.\n\nTo understand the package in more detail, check out the following tutorials in this manual. It is highly recommended that new users start with the ODE tutorial. Example IJulia notebooks can also be found in SciMLTutorials.jl. If you find any example where there appears to be an error, please open an issue.\n\nFor the most up-to-date information on using the package, please join the Julia Zulip channel.\n\nUsing the bleeding edge for the latest features and development is only recommended for power users. Information on how to get to the bleeding edge is found in the developer documentation.","category":"section"},{"location":"#Installing-from-Python","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Installing from Python","text":"Use of DifferentialEquations.jl from the Python programming language is available through the diffeqpy module. To install diffeqpy, use pip:\n\npip install diffeqpy\n\nUsing diffeqpy requires that Julia is installed and in the path, along with DifferentialEquations.jl and PyCall.jl. To install Julia, download a generic binary from the JuliaLang site and add it to your path. To install Julia packages required for diffeqpy, open up Python interpreter then run:\n\n>>> import diffeqpy\n>>> diffeqpy.install()\n\nand you're good! In addition, to improve the performance of your code, it is recommended that you use Numba to JIT compile your derivative functions. To install Numba, use:\n\npip install numba\n\ndiffeqpy supports the majority of DifferentialEquations.jl with very similar syntax, see the diffeqpy README for more details. One important point to note is that Numba is generally an order of magnitude slower than Julia in terms of  the generated differential equation solver code, and thus it is recommended to use julia.Main.eval for Julia-side derivative function implementations for maximal efficiency. See this blog post for more information.","category":"section"},{"location":"#Installing-from-R","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Installing from R","text":"Use of DifferentialEquations.jl from the R programming language is available through the diffeqr module. diffeqr is registered into CRAN. Thus to add the package, use:\n\ninstall.packages(\"diffeqr\")\n\nTo install the master branch of the package (for developers), use:\n\ndevtools::install_github('SciML/diffeqr', build_vignettes=T)\n\nYou will need a working installation of Julia in your path. To install Julia, download a generic binary from the JuliaLang site and add it to your path. The download and installation of DifferentialEquations.jl will happen on the first invocation of diffeqr::diffeq_setup().\n\nCurrently, use from R supports a subset of DifferentialEquations.jl which is documented through CRAN.","category":"section"},{"location":"#Video-Tutorial","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Video Tutorial","text":"(Image: Video Tutorial)","category":"section"},{"location":"#Tutorials","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Tutorials","text":"The following tutorials will introduce you to the functionality of DifferentialEquations.jl. More examples can be found by checking out the IJulia notebooks in the examples folder.\n\nPages = [\n    \"tutorials/ode_example.md\",\n    \"tutorials/sde_example.md\",\n    \"tutorials/dde_example.md\",\n    \"tutorials/dae_example.md\",\n    \"tutorials/discrete_stochastic_example.md\",\n    \"tutorials/jump_diffusion.md\",\n    \"tutorials/bvp_example.md\",\n    \"tutorials/additional.md\"\n    ]\nDepth = 2","category":"section"},{"location":"#Removing-and-Reducing-Compile-Times","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Removing and Reducing Compile Times","text":"In some situations, one may wish to decrease the compile time associated with DifferentialEquations.jl usage. If that's the case, there are two strategies to employ. One strategy is to use the low dependency usage. DifferentialEquations.jl is a metapackage composed of many smaller packages, and thus one could directly use a single component, such as OrdinaryDiffEq.jl for the pure Julia ODE solvers, and decrease the compile times by ignoring the rest (note: the interface is exactly the same, except using a solver apart from those in OrdinaryDiffEq.jl will error). We recommend that downstream packages rely solely on the packages they need.\n\nThe other strategy is to use PackageCompiler.jl to create a system image that precompiles the whole package. To do this, one simply does:\n\nimport PackageCompiler\nPackageCompiler.create_sysimage([:DifferentialEquations, :Plots]; replace_default = true)\n\nNote that there are some drawbacks to adding a package in your system image. For example, the package will never update until you manually rebuild the system image again. For more information on the consequences, see this portion of the PackageCompiler manual.","category":"section"},{"location":"#Basics","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Basics","text":"These pages introduce you to the core of DifferentialEquations.jl and the common interface. It explains the general workflow, options which are generally available, and the general tools for analysis.\n\nPages = [\n    \"basics/overview.md\",\n    \"basics/common_solver_opts.md\",\n    \"basics/solution.md\",\n    \"basics/plot.md\",\n    \"basics/integrator.md\",\n    \"basics/problem.md\",\n    \"basics/faq.md\",\n    \"basics/compatibility_chart.md\"\n    ]\nDepth = 2","category":"section"},{"location":"#Problem-Types","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Problem Types","text":"These pages describe building the problem types to define differential equations for the solvers, and the special features of the different solution types.\n\nPages = [\n  \"types/discrete_types.md\",\n  \"types/ode_types.md\",\n  \"types/dynamical_types.md\",\n  \"types/split_ode_types.md\",\n  \"types/steady_state_types.md\",\n  \"types/bvp_types.md\",\n  \"types/sde_types.md\",\n  \"types/rode_types.md\",\n  \"types/dde_types.md\",\n  \"types/dae_types.md\",\n  \"types/jump_types.md\",\n]\nDepth = 2","category":"section"},{"location":"#Solver-Algorithms","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Solver Algorithms","text":"These pages describe the solvers and available algorithms in detail.\n\nPages = [\n  \"solvers/discrete_solve.md\",\n  \"solvers/ode_solve.md\",\n  \"solvers/dynamical_solve.md\",\n  \"solvers/split_ode_solve.md\",\n  \"solvers/steady_state_solve.md\",\n  \"solvers/bvp_solve.md\",\n  \"solvers/jump_solve.md\",\n  \"solvers/sde_solve.md\",\n  \"solvers/rode_solve.md\",\n  \"solvers/dde_solve.md\",\n  \"solvers/dae_solve.md\",\n  \"solvers/benchmarks.md\"\n]\nDepth = 2","category":"section"},{"location":"#Additional-Features","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Additional Features","text":"These sections discuss extra performance enhancements, event handling, and other in-depth features.\n\nPages = [\n    \"features/performance_overloads.md\",\n    \"features/diffeq_arrays.md\",\n    \"features/diffeq_operator.md\",\n    \"features/noise_process.md\",\n    \"features/linear_nonlinear.md\",\n    \"features/callback_functions.md\",\n    \"features/callback_library.md\",\n    \"features/ensemble.md\",\n    \"features/io.md\",\n    \"features/low_dep.md\",\n    \"features/progress_bar.md\"\n]\nDepth = 2","category":"section"},{"location":"#Extra-Details","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Extra Details","text":"These are just assorted extra explanations for the curious.\n\nPages = [\n    \"extras/timestepping.md\"\n]\nDepth = 2","category":"section"},{"location":"#Acknowledgements","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Acknowledgements","text":"","category":"section"},{"location":"#Core-Contributors","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Core Contributors","text":"JuliaDiffEq and DifferentialEquations.jl has been a collaborative effort by many individuals. Significant contributions have been made by the following individuals:\n\nChris Rackauckas (@ChrisRackauckas) (lead developer)\nYingbo Ma (@YingboMa)\nDavid Widmann (@devmotion)\nHendrik Ranocha (@ranocha)\nEthan Levien (@elevien)\nTom Short (@tshort)\n@dextorious\nSamuel Isaacson (@isaacsas)","category":"section"},{"location":"#Google-Summer-of-Code-Alumni","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Google Summer of Code Alumni","text":"Yingbo Ma (@YingboMa)\nShivin Srivastava (@shivin9)\nAyush Pandey (@Ayush-iitkgp)\nXingjian Guo (@MSeeker1340)\nShubham Maddhashiya (@sipah00)\nVaibhav Kumar Dixit (@Vaibhavdixit02)","category":"section"},{"location":"#Reproducibility","page":"DifferentialEquations.jl: Efficient Differential Equation Solving in Julia","title":"Reproducibility","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>\n\nimport Pkg # hide\nPkg.status() # hide\n\n</details>\n\n<details><summary>and using this machine and Julia version.</summary>\n\nimport InteractiveUtils # hide\nInteractiveUtils.versioninfo() # hide\n\n</details>\n\n<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>\n\nimport Pkg # hide\nPkg.status(; mode = Pkg.PKGMODE_MANIFEST) # hide\n\n</details>\n\nimport TOML\nimport Markdown\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink_manifest = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n                \"/assets/Manifest.toml\"\nlink_project = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n               \"/assets/Project.toml\"\nMarkdown.parse(\"\"\"You can also download the\n[manifest]($link_manifest)\nfile and the\n[project]($link_project)\nfile.\n\"\"\")","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#OrdinaryDiffEqFIRK","page":"OrdinaryDiffEqFIRK","title":"OrdinaryDiffEqFIRK","text":"Fully Implicit Runge-Kutta (FIRK) methods for stiff differential equations requiring very high accuracy. These methods solve a fully coupled implicit system at each timestep, providing superior accuracy and stability compared to diagonally implicit methods.\n\nwarning: Real Numbers Only\nFIRK methods should only be used for problems defined on real numbers, not complex numbers.","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Key-Properties","page":"OrdinaryDiffEqFIRK","title":"Key Properties","text":"FIRK methods provide:\n\nHighest-order implicit methods (excluding extrapolation)\nSuperior accuracy for very low tolerance requirements (≤ 1e-9)\nA-stable and L-stable behavior for stiff problems\nHigher order per stage than SDIRK methods (order 2s+1 for s stages)\nSpecial geometric properties (some methods are symplectic)\nExcellent for small to medium systems with high accuracy requirements","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#When-to-Use-FIRK-Methods","page":"OrdinaryDiffEqFIRK","title":"When to Use FIRK Methods","text":"These methods are recommended for:\n\nVery low tolerance problems (1e-9 and below) where accuracy is paramount\nSmall to medium stiff systems (< 200 equations)\nProblems requiring highest possible accuracy for implicit methods\nStiff problems where SDIRK order limitations (max order 5) are insufficient\nApplications where computational cost is acceptable for maximum accuracy","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Mathematical-Background","page":"OrdinaryDiffEqFIRK","title":"Mathematical Background","text":"RadauIIA methods are based on Gaussian collocation and achieve order 2s+1 for s stages, making them among the highest-order implicit methods available. They represent the ODE analog of Gaussian quadrature. For more details on recent advances in FIRK methods, see our paper: High-Order Adaptive Time Stepping for the Incompressible Navier-Stokes Equations.","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Computational-Considerations","page":"OrdinaryDiffEqFIRK","title":"Computational Considerations","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Advantages","page":"OrdinaryDiffEqFIRK","title":"Advantages","text":"Higher accuracy per stage than diagonal methods\nBetter multithreading for small systems due to larger linear algebra operations\nNo order restrictions like SDIRK methods (which max out at order 5)","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Disadvantages","page":"OrdinaryDiffEqFIRK","title":"Disadvantages","text":"Limited to real-valued problems - cannot be used for complex number systems\nHigher implementation complexity compared to SDIRK methods","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqFIRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#High-accuracy-requirements","page":"OrdinaryDiffEqFIRK","title":"High accuracy requirements","text":"AdaptiveRadau: Recommended - adaptive order method that automatically selects optimal order\nRadauIIA5: 5th-order method, good balance of accuracy and efficiency\nRadauIIA9: 9th-order method for extremely high accuracy requirements\nRadauIIA3: 3rd-order method for moderate accuracy needs","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#System-size-considerations","page":"OrdinaryDiffEqFIRK","title":"System size considerations","text":"Systems < 200: FIRK methods are competitive due to better multithreading\nSystems > 200: Consider SDIRK or BDF methods instead","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Performance-Guidelines","page":"OrdinaryDiffEqFIRK","title":"Performance Guidelines","text":"Best for tolerances ≤ 1e-9 where high accuracy justifies the cost\nMost efficient on small to medium systems where linear algebra cost is manageable\nShould be tested against parallel implicit extrapolation methods which specialize in similar regimes\nCompare with high-order SDIRK methods for borderline cases\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqFIRK\", \"RadauIIA5\")","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#Full-list-of-solvers","page":"OrdinaryDiffEqFIRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/FIRK/#OrdinaryDiffEqFIRK.RadauIIA3","page":"OrdinaryDiffEqFIRK","title":"OrdinaryDiffEqFIRK.RadauIIA3","text":"RadauIIA3(; chunk_size = Val{0}(),\n            autodiff = AutoForwardDiff(),\n            standardtag = Val{true}(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward},\n            linsolve = nothing,\n            precs = DEFAULT_PRECS,\n            extrapolant = :dense,\n            smooth_est = true,\n            step_limiter! = trivial_limiter!)\n\nFully-Implicit Runge-Kutta Method. An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency. Similar to Hairer's SEULEX.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify RadauIIA3(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nextrapolant: TBD\nsmooth_est: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{hairer1999stiff, title={Stiff differential equations solved by Radau methods}, author={Hairer, Ernst and Wanner, Gerhard}, journal={Journal of Computational and Applied Mathematics}, volume={111}, number={1-2}, pages={93–111}, year={1999}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/FIRK/#OrdinaryDiffEqFIRK.RadauIIA5","page":"OrdinaryDiffEqFIRK","title":"OrdinaryDiffEqFIRK.RadauIIA5","text":"RadauIIA5(; chunk_size = Val{0}(),\n            autodiff = AutoForwardDiff(),\n            standardtag = Val{true}(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward},\n            linsolve = nothing,\n            precs = DEFAULT_PRECS,\n            extrapolant = :dense,\n            smooth_est = true,\n            step_limiter! = trivial_limiter!)\n\nFully-Implicit Runge-Kutta Method. An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency. 5th order method with excellent numerical stability. Good for highly stiff systems, problems requiring high-order implicit integration, systems with complex eigenvalue structures. Best for low tolerance stiff problems (<1e-9).\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify RadauIIA5(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nextrapolant: TBD\nsmooth_est: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{hairer1999stiff, title={Stiff differential equations solved by Radau methods}, author={Hairer, Ernst and Wanner, Gerhard}, journal={Journal of Computational and Applied Mathematics}, volume={111}, number={1-2}, pages={93–111}, year={1999}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/FIRK/#OrdinaryDiffEqFIRK.RadauIIA9","page":"OrdinaryDiffEqFIRK","title":"OrdinaryDiffEqFIRK.RadauIIA9","text":"RadauIIA9(; chunk_size = Val{0}(),\n            autodiff = AutoForwardDiff(),\n            standardtag = Val{true}(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward},\n            linsolve = nothing,\n            precs = DEFAULT_PRECS,\n            extrapolant = :dense,\n            smooth_est = true,\n            step_limiter! = trivial_limiter!)\n\nFully-Implicit Runge-Kutta Method. An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency. Similar to Hairer's SEULEX.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify RadauIIA9(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nextrapolant: TBD\nsmooth_est: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{hairer1999stiff, title={Stiff differential equations solved by Radau methods}, author={Hairer, Ernst and Wanner, Gerhard}, journal={Journal of Computational and Applied Mathematics}, volume={111}, number={1-2}, pages={93–111}, year={1999}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK","text":"Symplectic integrators are specialized methods for solving Hamiltonian systems and second-order differential equations that preserve important geometric properties of the phase space. These methods are essential for long-time integration of conservative mechanical systems.","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Key-Properties","page":"OrdinaryDiffEqSymplecticRK","title":"Key Properties","text":"Symplectic integrators provide:\n\nExact conservation of symplectic structure in phase space\nBounded energy error over long time periods\nExcellent long-time stability without secular drift\nPreservation of periodic orbits and other geometric structures\nLinear energy drift instead of quadratic (much better than standard methods)","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#When-to-Use-Symplectic-Methods","page":"OrdinaryDiffEqSymplecticRK","title":"When to Use Symplectic Methods","text":"Symplectic integrators are essential for:\n\nHamiltonian systems and conservative mechanical problems\nMolecular dynamics and N-body simulations\nCelestial mechanics and orbital computations\nPlasma physics and charged particle dynamics\nLong-time integration where energy conservation is critical\nOscillatory problems requiring preservation of periodic structure\nClassical mechanics problems with known analytical properties","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Mathematical-Background","page":"OrdinaryDiffEqSymplecticRK","title":"Mathematical Background","text":"For a Hamiltonian system with energy H(p,q), symplectic integrators preserve the symplectic structure dp ∧ dq. While standard integrators have energy error growing quadratically over time, symplectic methods maintain bounded energy with only linear drift, making them superior for long-time integration.","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqSymplecticRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#First-order-methods","page":"OrdinaryDiffEqSymplecticRK","title":"First-order methods","text":"SymplecticEuler: First-order, simplest symplectic method. Only recommended when the dynamics function f is not differentiable.","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Second-order-methods","page":"OrdinaryDiffEqSymplecticRK","title":"Second-order methods","text":"McAte2: Optimized second-order McLachlan-Atela method, recommended for most applications\nVelocityVerlet: Second-order, common choice for molecular dynamics but less efficient in terms of accuracy than McAte2\nVerletLeapfrog: Second-order, kick-drift-kick formulation\nLeapfrogDriftKickDrift: Alternative second-order leapfrog\nPseudoVerletLeapfrog: Modified Verlet scheme","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Third-order-methods","page":"OrdinaryDiffEqSymplecticRK","title":"Third-order methods","text":"Ruth3: Third-order method\nMcAte3: Optimized third-order McLachlan-Atela method","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Fourth-order-methods","page":"OrdinaryDiffEqSymplecticRK","title":"Fourth-order methods","text":"CandyRoz4: Fourth-order method\nMcAte4: Fourth-order McLachlan-Atela (requires quadratic kinetic energy)\nCalvoSanz4: Optimized fourth-order method\nMcAte42: Alternative fourth-order method (BROKEN)","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Higher-order-methods","page":"OrdinaryDiffEqSymplecticRK","title":"Higher-order methods","text":"McAte5: Fifth-order McLachlan-Atela method\nYoshida6: Sixth-order method\nKahanLi6: Optimized sixth-order method\nMcAte8: Eighth-order McLachlan-Atela method\nKahanLi8: Optimized eighth-order method\nSofSpa10: Tenth-order method for highest precision","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Method-Selection-Guidelines","page":"OrdinaryDiffEqSymplecticRK","title":"Method Selection Guidelines","text":"For most applications: McAte2 (second-order, optimal efficiency)\nFor molecular dynamics (common choice): VelocityVerlet (less efficient than McAte2 but widely used)\nFor non-differentiable dynamics: SymplecticEuler (first-order, only when necessary)\nFor computational efficiency: McAte2 or McAte3","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Important-Note-on-Chaotic-Systems","page":"OrdinaryDiffEqSymplecticRK","title":"Important Note on Chaotic Systems","text":"Most N-body problems (molecular dynamics, astrophysics) are chaotic systems where solutions diverge onto shadow trajectories. In such cases, higher-order methods provide no practical advantage because the true error remains O(1) for sufficiently long integrations - exactly the scenarios where symplectic methods are most needed. The geometric properties preserved by symplectic integrators are more important than high-order accuracy for chaotic systems.\n\nFor more information on chaos and accuracy in numerical integration, see: How Chaotic is Chaos? How Some AI for Science (SciML) Papers are Overstating Accuracy Claims","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Installation","page":"OrdinaryDiffEqSymplecticRK","title":"Installation","text":"To be able to access the solvers in OrdinaryDiffEqSymplecticRK, you must first install them use the Julia package manager:\n\nusing Pkg\nPkg.add(\"OrdinaryDiffEqSymplecticRK\")\n\nThis will only install the solvers listed at the bottom of this page. If you want to explore other solvers for your problem, you will need to install some of the other libraries listed in the navigation bar on the left.","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Example-usage","page":"OrdinaryDiffEqSymplecticRK","title":"Example usage","text":"using OrdinaryDiffEqSymplecticRK\nfunction HH_acceleration!(dv, v, u, p, t)\n    x, y = u\n    dx, dy = dv\n    dv[1] = -x - 2 * x * y\n    dv[2] = y^2 - y - x^2\nend\ninitial_positions = [0.0, 0.1]\ninitial_velocities = [0.5, 0.0]\ntspan = (0.0, 1.0)\nprob = SecondOrderODEProblem(HH_acceleration!, initial_velocities, initial_positions, tspan)\nsol = solve(prob, KahanLi8(), dt = 1 / 10)","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#Full-list-of-solvers","page":"OrdinaryDiffEqSymplecticRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.SymplecticEuler","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.SymplecticEuler","text":"SymplecticEuler()\n\nSymplectic Runge-Kutta Methods First order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\nhttps://en.wikipedia.org/wiki/Semi-implicitEulermethod\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.VelocityVerlet","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.VelocityVerlet","text":"VelocityVerlet()\n\nSymplectic Runge-Kutta Methods 2nd order explicit symplectic integrator. Requires f_2(t,u) = v, i.e. a second order ODE.\n\nKeyword Arguments\n\nReferences\n\n@article{verlet1967computer, title={Computer\" experiments\" on classical fluids. I. Thermodynamical properties of Lennard-Jones molecules}, author={Verlet, Loup}, journal={Physical review}, volume={159}, number={1}, pages={98}, year={1967}, publisher={APS} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.VerletLeapfrog","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.VerletLeapfrog","text":"VerletLeapfrog()\n\nSymplectic Runge-Kutta Methods 2nd order explicit symplectic integrator. Kick-drift-kick form. Requires only one evaluation of f1 per step.\n\nKeyword Arguments\n\nReferences\n\n@article{monaghan2005, \ttitle = {Smoothed particle hydrodynamics}, \tauthor = {Monaghan, Joseph J.}, \tyear = {2005}, \tjournal = {Reports on Progress in Physics}, \tvolume = {68}, \tnumber = {8}, \tpages = {1703–1759}, \tdoi = {10.1088/0034-4885/68/8/R01}, }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.LeapfrogDriftKickDrift","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.LeapfrogDriftKickDrift","text":"LeapfrogDriftKickDrift()\n\nSymplectic Runge-Kutta Methods 2nd order explicit symplectic integrator. Drift-kick-drift form of VerletLeapfrog designed to work when f1 depends on v. Requires two evaluation of f1 per step.\n\nKeyword Arguments\n\nReferences\n\n@article{monaghan2005, \ttitle = {Smoothed particle hydrodynamics}, \tauthor = {Monaghan, Joseph J.}, \tyear = {2005}, \tjournal = {Reports on Progress in Physics}, \tvolume = {68}, \tnumber = {8}, \tpages = {1703–1759}, \tdoi = {10.1088/0034-4885/68/8/R01}, }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.PseudoVerletLeapfrog","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.PseudoVerletLeapfrog","text":"PseudoVerletLeapfrog()\n\nSymplectic Runge-Kutta Methods 2nd order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{verlet1967computer, title={Computer\" experiments\" on classical fluids. I. Thermodynamical properties of Lennard-Jones molecules}, author={Verlet, Loup}, journal={Physical review}, volume={159}, number={1}, pages={98}, year={1967}, publisher={APS} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.McAte2","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.McAte2","text":"McAte2()\n\nSymplectic Runge-Kutta Methods Optimized efficiency 2nd order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{mclachlan1992accuracy, title={The accuracy of symplectic integrators}, author={McLachlan, Robert I and Atela, Pau}, journal={Nonlinearity}, volume={5}, number={2}, pages={541}, year={1992}, publisher={IOP Publishing} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.Ruth3","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.Ruth3","text":"Ruth3()\n\nSymplectic Runge-Kutta Methods 3rd order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{ruth1983canonical, title={A canonical integration technique}, author={Ruth, Ronald D}, journal={IEEE Trans. Nucl. Sci.}, volume={30}, number={CERN-LEP-TH-83-14}, pages={2669–2671}, year={1983}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.McAte3","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.McAte3","text":"McAte3()\n\nSymplectic Runge-Kutta Methods Optimized efficiency 3rd order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{mclachlan1992accuracy, title={The accuracy of symplectic integrators}, author={McLachlan, Robert I and Atela, Pau}, journal={Nonlinearity}, volume={5}, number={2}, pages={541}, year={1992}, publisher={IOP Publishing} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.CandyRoz4","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.CandyRoz4","text":"CandyRoz4()\n\nSymplectic Runge-Kutta Methods 4th order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{candy1991symplectic, itle={A symplectic integration algorithm for separable Hamiltonian functions}, uthor={Candy, J and Rozmus, W}, ournal={Journal of Computational Physics}, olume={92}, umber={1}, ages={230–256}, ear={1991}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.McAte4","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.McAte4","text":"McAte4()\n\nSymplectic Runge-Kutta Methods 4th order explicit symplectic integrator. Requires quadratic kinetic energy.\n\nKeyword Arguments\n\nReferences\n\n@article{mclachlan1992accuracy, title={The accuracy of symplectic integrators}, author={McLachlan, Robert I and Atela, Pau}, journal={Nonlinearity}, volume={5}, number={2}, pages={541}, year={1992}, publisher={IOP Publishing} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.CalvoSanz4","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.CalvoSanz4","text":"CalvoSanz4()\n\nSymplectic Runge-Kutta Methods Optimized efficiency 4th order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{sanz1993symplectic, title={Symplectic numerical methods for Hamiltonian problems}, author={Sanz-Serna, Jes{'u}s Maria and Calvo, Mari-Paz}, journal={International Journal of Modern Physics C}, volume={4}, number={02}, pages={385–392}, year={1993}, publisher={World Scientific} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.McAte42","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.McAte42","text":"McAte42()\n\nSymplectic Runge-Kutta Methods 4th order explicit symplectic integrator. BROKEN\n\nKeyword Arguments\n\nReferences\n\n@article{mclachlan1992accuracy, title={The accuracy of symplectic integrators}, author={McLachlan, Robert I and Atela, Pau}, journal={Nonlinearity}, volume={5}, number={2}, pages={541}, year={1992}, publisher={IOP Publishing} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.McAte5","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.McAte5","text":"McAte5()\n\nSymplectic Runge-Kutta Methods Optimized efficiency 5th order explicit symplectic integrator. Requires quadratic kinetic energy.\n\nKeyword Arguments\n\nReferences\n\n@article{mclachlan1992accuracy, title={The accuracy of symplectic integrators}, author={McLachlan, Robert I and Atela, Pau}, journal={Nonlinearity}, volume={5}, number={2}, pages={541}, year={1992}, publisher={IOP Publishing} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.Yoshida6","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.Yoshida6","text":"Yoshida6()\n\nSymplectic Runge-Kutta Methods 6th order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{yoshida1990construction, title={Construction of higher order symplectic integrators}, author={Yoshida, Haruo}, journal={Physics letters A}, volume={150}, number={5-7}, pages={262–268}, year={1990}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.KahanLi6","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.KahanLi6","text":"KahanLi6()\n\nSymplectic Runge-Kutta Methods Optimized efficiency 6th order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{yoshida1990construction, title={Construction of higher order symplectic integrators}, author={Yoshida, Haruo}, journal={Physics letters A}, volume={150}, number={5-7}, pages={262–268}, year={1990}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.McAte8","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.McAte8","text":"McAte8()\n\nSymplectic Runge-Kutta Methods 8th order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{mclachlan1995numerical, title={On the numerical integration of ordinary differential equations by symmetric composition methods}, author={McLachlan, Robert I}, journal={SIAM Journal on Scientific Computing}, volume={16}, number={1}, pages={151–168}, year={1995}, publisher={SIAM} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.KahanLi8","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.KahanLi8","text":"KahanLi8()\n\nSymplectic Runge-Kutta Methods Optimized efficiency 8th order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{kahan1997composition, title={Composition constants for raising the orders of unconventional schemes for ordinary differential equations}, author={Kahan, William and Li, Ren-Cang}, journal={Mathematics of computation}, volume={66}, number={219}, pages={1089–1099}, year={1997}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/SymplecticRK/#OrdinaryDiffEqSymplecticRK.SofSpa10","page":"OrdinaryDiffEqSymplecticRK","title":"OrdinaryDiffEqSymplecticRK.SofSpa10","text":"SofSpa10()\n\nSymplectic Runge-Kutta Methods 10th order explicit symplectic integrator.\n\nKeyword Arguments\n\nReferences\n\n@article{sofroniou2005derivation, title={Derivation of symmetric composition constants for symmetric integrators}, author={Sofroniou, Mark and Spaletta, Giulia}, journal={Optimization Methods and Software}, volume={20}, number={4-5}, pages={597–613}, year={2005}, publisher={Taylor \\& Francis}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock","text":"Rosenbrock methods for mass matrix differential-algebraic equations (DAEs) and stiff ODEs with singular mass matrices. These methods provide efficient integration for moderately stiff systems with algebraic constraints, offering excellent performance for small to medium-sized DAE problems.","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Key-Properties","page":"OrdinaryDiffEqRosenbrock","title":"Key Properties","text":"Mass matrix Rosenbrock methods provide:\n\nDAE capability for index-1 differential-algebraic equations\nW-method efficiency using approximate Jacobians for computational savings\nMass matrix support for singular and non-diagonal mass matrices\nModerate to high order accuracy (2nd to 6th order available)\nGood stability properties with stiffly accurate behavior\nEmbedded error estimation for adaptive timestepping","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#When-to-Use-Mass-Matrix-Rosenbrock-Methods","page":"OrdinaryDiffEqRosenbrock","title":"When to Use Mass Matrix Rosenbrock Methods","text":"These methods are recommended for:\n\nIndex-1 DAE systems with moderate stiffness\nSmall to medium constrained systems (< 1000 equations)\nSemi-explicit DAEs arising from discretized PDEs\nProblems requiring good accuracy with moderate computational cost\nDAEs with moderate nonlinearity where W-methods are efficient\nElectrical circuits and mechanical systems with constraints\n\nwarn: Warn\nIn order to use OrdinaryDiffEqRosenbrock with DAEs that require a non-trivial consistent initialization, a nonlinear solver is required and thus using OrdinaryDiffEqNonlinearSolve is required or you must pass an initializealg with a valid nlsolve choice.","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Mathematical-Background","page":"OrdinaryDiffEqRosenbrock","title":"Mathematical Background","text":"Mass matrix DAEs have the form: M du/dt = f(u,t)\n\nRosenbrock methods linearize around the current solution and solve linear systems of the form: (M/γh - J) k_i = ...\n\nwhere J is the Jacobian of f and γ is a method parameter.","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Solver-Selection-Guide","page":"OrdinaryDiffEqRosenbrock","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Recommended-Methods-by-Tolerance","page":"OrdinaryDiffEqRosenbrock","title":"Recommended Methods by Tolerance","text":"High tolerances (>1e-2): Rosenbrock23 - efficient low-order method\nMedium tolerances (1e-8 to 1e-2): Rodas5P - most efficient choice, or Rodas4P for higher reliability\nLow tolerances (<1e-8): Rodas5Pe or higher-order alternatives","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Method-families","page":"OrdinaryDiffEqRosenbrock","title":"Method families","text":"Rodas5P: Recommended - Most efficient 5th-order method for general use\nRodas4P: More reliable 4th-order alternative\nRosenbrock23: Good for high tolerance problems\nRodas5: Standard 5th-order method without embedded pair optimization","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Performance-Guidelines","page":"OrdinaryDiffEqRosenbrock","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#When-mass-matrix-Rosenbrock-methods-excel","page":"OrdinaryDiffEqRosenbrock","title":"When mass matrix Rosenbrock methods excel","text":"Small to medium DAE systems (< 1000 equations)\nModerately stiff problems where full BDF methods are overkill\nProblems with efficient Jacobian computation or finite difference approximation\nIndex-1 DAEs with well-conditioned mass matrices\nSemi-explicit index-1 problems from spatial discretizations","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#System-size-considerations","page":"OrdinaryDiffEqRosenbrock","title":"System size considerations","text":"Small systems (< 100): Rosenbrock methods often outperform multistep methods\nMedium systems (100-1000): Good performance with proper linear algebra\nLarge systems (> 1000): Consider BDF methods instead","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Important-DAE-Considerations","page":"OrdinaryDiffEqRosenbrock","title":"Important DAE Considerations","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Initial-conditions","page":"OrdinaryDiffEqRosenbrock","title":"Initial conditions","text":"Must be consistent with algebraic constraints\nConsistent initialization may require nonlinear solver\nIndex-1 assumption for reliable performance","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Mass-matrix-requirements","page":"OrdinaryDiffEqRosenbrock","title":"Mass matrix requirements","text":"Index-1 DAE structure for optimal performance\nNon-singular leading submatrix for differential variables\nWell-conditioned constraint equations","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Alternative-Approaches","page":"OrdinaryDiffEqRosenbrock","title":"Alternative Approaches","text":"Consider these alternatives:\n\nMass matrix BDF methods for larger or highly stiff DAE systems\nImplicit Runge-Kutta methods for higher accuracy requirements\nStandard Rosenbrock methods for regular ODEs without constraints\nIMEX methods if natural explicit/implicit splitting exists","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Example-Usage","page":"OrdinaryDiffEqRosenbrock","title":"Example Usage","text":"using LinearAlgebra: Diagonal\nfunction rober(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁ * y₁ + k₃ * y₂ * y₃\n    du[2] = k₁ * y₁ - k₃ * y₂ * y₃ - k₂ * y₂^2\n    du[3] = y₁ + y₂ + y₃ - 1\n    nothing\nend\nM = Diagonal([1.0, 1.0, 0])  # Singular mass matrix\nf = ODEFunction(rober, mass_matrix = M)\nprob_mm = ODEProblem(f, [1.0, 0.0, 0.0], (0.0, 1e5), (0.04, 3e7, 1e4))\nsol = solve(prob_mm, Rodas5(), reltol = 1e-8, abstol = 1e-8)\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqRosenbrock\", \"Rodas5P\")","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#Full-list-of-solvers","page":"OrdinaryDiffEqRosenbrock","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rosenbrock23-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rosenbrock23","text":"Rosenbrock23(; chunk_size = Val{0}(),\n               standardtag = Val{true}(),\n               autodiff = AutoForwardDiff(),\n               concrete_jac = nothing,\n               diff_type = Val{:forward}(),\n               linsolve = nothing,\n               precs = DEFAULT_PRECS,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rosenbrock23(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nShampine L.F. and Reichelt M., (1997) The MATLAB ODE Suite, SIAM Journal of Scientific Computing, 18 (1), pp. 1-22.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rosenbrock32-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rosenbrock32","text":"Rosenbrock32(; chunk_size = Val{0}(),\n               standardtag = Val{true}(),\n               autodiff = AutoForwardDiff(),\n               concrete_jac = nothing,\n               diff_type = Val{:forward}(),\n               linsolve = nothing,\n               precs = DEFAULT_PRECS,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  An Order 3/2 A-Stable Rosenbrock-W method which is good for mildly stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rosenbrock32(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nShampine L.F. and Reichelt M., (1997) The MATLAB ODE Suite, SIAM Journal of Scientific Computing, 18 (1), pp. 1-22.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3P-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3P","text":"ROS3P(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n            ForwardDiff default function-specific tags. For more information, see\n            [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n            Defaults to `Val{true}()`.\n        - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n            to specify whether to use automatic differentiation via\n            [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n            differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n            Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n            `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n            To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n            `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n        - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n            `nothing`, which means it will be chosen true/false depending on circumstances\n            of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n        - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n          For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n          `ROS3P(linsolve = KLUFactorization()`).\n           When `nothing` is passed, uses `DefaultLinearSolver`.\n        - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n          can be used as a left or right preconditioner.\n          Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n          function where the arguments are defined as:\n            - `W`: the current Jacobian of the nonlinear system. Specified as either\n                ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                representation of the operator. Users can construct the W-matrix on demand\n                by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                the `jac_prototype`.\n            - `du`: the current ODE derivative\n            - `u`: the current ODE state\n            - `p`: the ODE parameters\n            - `t`: the current ODE time\n            - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                the last call to `precs`. It is recommended that this is checked to only\n                update the preconditioner when `newW == true`.\n            - `Plprev`: the previous `Pl`.\n            - `Prprev`: the previous `Pr`.\n            - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                Solver-dependent and subject to change.\n          The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n          To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n          which is not used. Additionally, `precs` must supply the dispatch:\n          ```julia\n          Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n          ```\n          which is used in the solver setup phase to construct the integrator\n          type with the preconditioners `(Pl,Pr)`.\n          The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n          is defined as:\n          ```julia\n          DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n          ```\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nLang, J. & Verwer, ROS3P—An Accurate Third-Order Rosenbrock Solver Designed for Parabolic Problems J. BIT Numerical Mathematics (2001) 41: 731. doi:10.1023/A:1021900219772\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas3-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas3","text":"Rodas3(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Rodas3(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  3rd order A-stable and stiffly stable Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSandu, Verwer, Van Loon, Carmichael, Potra, Dabdub, Seinfeld, Benchmarking stiff ode solvers for atmospheric chemistry problems-I.  implicit vs explicit, Atmospheric Environment, 31(19), 3151-3166, 1997.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas23W-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas23W","text":"Rodas23W(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  An Order 2/3 L-Stable Rosenbrock-W method for stiff ODEs and DAEs in mass matrix form. 2nd order stiff-aware interpolation and additional error test for interpolation.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas23W(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G., Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas3P-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas3P","text":"Rodas3P(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `Rodas3P(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  3rd order A-stable and stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant and additional error test for interpolation. Keeps accuracy on discretizations of linear parabolic PDEs.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G., Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas4-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas4","text":"Rodas4(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Rodas4(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas42-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas42","text":"Rodas42(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `Rodas42(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas4P-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas4P","text":"Rodas4P(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `Rodas4P(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach, G., Rentrop, P., An adaptive method of lines approach for modelling flow and transport in rivers.  Adaptive method of lines , Wouver, A. Vande, Sauces, Ph., Schiesser, W.E. (ed.),S. 181-205,Chapman & Hall/CRC, 2001,\nSteinebach, G., Order-reduction of ROW-methods for DAEs and method of lines  applications.  Preprint-Nr. 1741, FB Mathematik, TH Darmstadt, 1995.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas4P2-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas4P2","text":"Rodas4P2(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems. It is an improvement of Rodas4P and in case of inexact Jacobians a second order W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas4P2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G., Improvement of Rosenbrock-Wanner Method RODASP, In: Reis T., Grundel S., Schöps S. (eds)  Progress in Differential-Algebraic Equations II. Differential-Algebraic Equations Forum. Springer, Cham., 165-184, 2020.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5","text":"Rodas5(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Rodas5(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner Method.  A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nDi Marzo G. RODAS5(4) – Méthodes de Rosenbrock d'ordre 5(4) adaptées aux problèmes différentiels-algébriques. MSc mathematics thesis, Faculty of Science, University of Geneva, Switzerland.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5P-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5P","text":"Rodas5P(; chunk_size = Val{0}(),\n          standardtag = Val{true}(),\n          autodiff = AutoForwardDiff(),\n          concrete_jac = nothing,\n          diff_type = Val{:forward}(),\n          linsolve = nothing,\n          precs = DEFAULT_PRECS,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant. Has improved stability in the adaptive time stepping embedding.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas5P(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G. Construction of Rosenbrock–Wanner method Rodas5P and numerical benchmarks within the Julia Differential Equations package. In: BIT Numerical Mathematics, 63(2), 2023. doi:10.1007/s10543-023-00967-x\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5Pe-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5Pe","text":"Rodas5Pe(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  Variant of Rodas5P with modified embedded scheme.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas5Pe(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G. Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Rodas5Pr-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Rodas5Pr","text":"Rodas5Pr(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  Variant of Rodas5P with additional residual control.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Rodas5Pr(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nSteinebach G. Rosenbrock methods within OrdinaryDiffEq.jl - Overview, recent developments and applications - Preprint 2024. Proceedings of the JuliaCon Conferences. https://proceedings.juliacon.org/papers/eb04326e1de8fa819a3595b376508a40\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.RosenbrockW6S4OS-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.RosenbrockW6S4OS","text":"RosenbrockW6S4OS(; chunk_size = Val{0}(),\n                   standardtag = Val{true}(),\n                   autodiff = AutoForwardDiff(),\n                   concrete_jac = nothing,\n                   diff_type = Val{:forward}(),\n                   linsolve = nothing,\n                   precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable Rosenbrock-W method (fixed step only).\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify RosenbrockW6S4OS(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nhttps://doi.org/10.1016/j.cam.2009.09.017\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS2-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS2","text":"ROS2(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n           ForwardDiff default function-specific tags. For more information, see\n           [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n           Defaults to `Val{true}()`.\n       - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n           to specify whether to use automatic differentiation via\n           [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n           differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n           Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n           `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n           To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n           `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n       - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n           `nothing`, which means it will be chosen true/false depending on circumstances\n           of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n       - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n         For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n         `ROS2(linsolve = KLUFactorization()`).\n          When `nothing` is passed, uses `DefaultLinearSolver`.\n       - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n         can be used as a left or right preconditioner.\n         Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n         function where the arguments are defined as:\n           - `W`: the current Jacobian of the nonlinear system. Specified as either\n               ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n               commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n               representation of the operator. Users can construct the W-matrix on demand\n               by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n               the `jac_prototype`.\n           - `du`: the current ODE derivative\n           - `u`: the current ODE state\n           - `p`: the ODE parameters\n           - `t`: the current ODE time\n           - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n               the last call to `precs`. It is recommended that this is checked to only\n               update the preconditioner when `newW == true`.\n           - `Plprev`: the previous `Pl`.\n           - `Prprev`: the previous `Pr`.\n           - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n               Solver-dependent and subject to change.\n         The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n         To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n         which is not used. Additionally, `precs` must supply the dispatch:\n         ```julia\n         Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n         ```\n         which is used in the solver setup phase to construct the integrator\n         type with the preconditioners `(Pl,Pr)`.\n         The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n         is defined as:\n         ```julia\n         DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n         ```)\n\nRosenbrock-Wanner Method.  A 2nd order L-stable Rosenbrock method with 2 internal stages.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nJ. G. Verwer et al. (1999): A second-order Rosenbrock method applied to photochemical dispersion problems https://doi.org/10.1137/S1064827597326651\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS2PR-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS2PR","text":"ROS2PR(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `ROS2PR(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```)\n\nRosenbrock-Wanner Method.  2nd order stiffly accurate Rosenbrock method with 3 internal stages with (Rinf=0). For problems with medium stiffness the convergence behaviour is very poor and it is recommended to use ROS2S instead.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS2S-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS2S","text":"ROS2S(; chunk_size = Val{0}(),\n        standardtag = Val{true}(),\n        autodiff = AutoForwardDiff(),\n        concrete_jac = nothing,\n        diff_type = Val{:forward}(),\n        linsolve = nothing,\n        precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  2nd order stiffly accurate Rosenbrock-Wanner W-method with 3 internal stages with B_PR consistent of order 2 with (Rinf=0).\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS2S(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3","text":"ROS3(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n           ForwardDiff default function-specific tags. For more information, see\n           [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n           Defaults to `Val{true}()`.\n       - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n           to specify whether to use automatic differentiation via\n           [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n           differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n           Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n           `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n           To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n           `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n       - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n           `nothing`, which means it will be chosen true/false depending on circumstances\n           of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n       - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n         For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n         `ROS3(linsolve = KLUFactorization()`).\n          When `nothing` is passed, uses `DefaultLinearSolver`.\n       - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n         can be used as a left or right preconditioner.\n         Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n         function where the arguments are defined as:\n           - `W`: the current Jacobian of the nonlinear system. Specified as either\n               ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n               commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n               representation of the operator. Users can construct the W-matrix on demand\n               by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n               the `jac_prototype`.\n           - `du`: the current ODE derivative\n           - `u`: the current ODE state\n           - `p`: the ODE parameters\n           - `t`: the current ODE time\n           - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n               the last call to `precs`. It is recommended that this is checked to only\n               update the preconditioner when `newW == true`.\n           - `Plprev`: the previous `Pl`.\n           - `Prprev`: the previous `Pr`.\n           - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n               Solver-dependent and subject to change.\n         The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n         To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n         which is not used. Additionally, `precs` must supply the dispatch:\n         ```julia\n         Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n         ```\n         which is used in the solver setup phase to construct the integrator\n         type with the preconditioners `(Pl,Pr)`.\n         The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n         is defined as:\n         ```julia\n         DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n         ```)\n\nRosenbrock-Wanner Method.  3rd order L-stable Rosenbrock method with 3 internal stages with an embedded strongly A-stable 2nd order method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3PR-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3PR","text":"ROS3PR(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `ROS3PR(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```)\n\nRosenbrock-Wanner Method.  3nd order stiffly accurate Rosenbrock method with 3 internal stages with B_PR consistent of order 3, which is strongly A-stable with Rinf~=-0.73.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Scholz4_7-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Scholz4_7","text":"Scholz4_7(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n                ForwardDiff default function-specific tags. For more information, see\n                [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n                Defaults to `Val{true}()`.\n            - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n                to specify whether to use automatic differentiation via\n                [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n                differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n                Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n                `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n                To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n                `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n            - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n                `nothing`, which means it will be chosen true/false depending on circumstances\n                of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n            - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n              For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n              `Scholz4_7(linsolve = KLUFactorization()`).\n               When `nothing` is passed, uses `DefaultLinearSolver`.\n            - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n              can be used as a left or right preconditioner.\n              Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n              function where the arguments are defined as:\n                - `W`: the current Jacobian of the nonlinear system. Specified as either\n                    ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                    commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                    representation of the operator. Users can construct the W-matrix on demand\n                    by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                    the `jac_prototype`.\n                - `du`: the current ODE derivative\n                - `u`: the current ODE state\n                - `p`: the ODE parameters\n                - `t`: the current ODE time\n                - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                    the last call to `precs`. It is recommended that this is checked to only\n                    update the preconditioner when `newW == true`.\n                - `Plprev`: the previous `Pl`.\n                - `Prprev`: the previous `Pr`.\n                - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                    Solver-dependent and subject to change.\n              The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n              To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n              which is not used. Additionally, `precs` must supply the dispatch:\n              ```julia\n              Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n              ```\n              which is used in the solver setup phase to construct the integrator\n              type with the preconditioners `(Pl,Pr)`.\n              The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n              is defined as:\n              ```julia\n              DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n              ```)\n\nRosenbrock-Wanner Method.  3nd order stiffly accurate Rosenbrock method with 3 internal stages with B_PR consistent of order 3, which is strongly A-stable with Rinf~=-0.73. Convergence with order 4 for the stiff case, but has a poor accuracy.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW1a-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW1a","text":"ROS34PW1a(; chunk_size = Val{0}(),\n            standardtag = Val{true}(),\n            autodiff = AutoForwardDiff(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable Rosenbrock-W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW1a(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW1b-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW1b","text":"ROS34PW1b(; chunk_size = Val{0}(),\n            standardtag = Val{true}(),\n            autodiff = AutoForwardDiff(),\n            concrete_jac = nothing,\n            diff_type = Val{:forward}(),\n            linsolve = nothing,\n            precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order L-stable Rosenbrock-W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW1b(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW2-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW2","text":"ROS34PW2(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PW3-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PW3","text":"ROS34PW3(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PW3(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nRang, Joachim and Angermann, L (2005): New Rosenbrock W-methods of order 3 for partial differential algebraic equations of index 1. BIT Numerical Mathematics, 45, 761–787.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS34PRw-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS34PRw","text":"ROS34PRw(; chunk_size = Val{0}(),\n           standardtag = Val{true}(),\n           autodiff = AutoForwardDiff(),\n           concrete_jac = nothing,\n           diff_type = Val{:forward}(),\n           linsolve = nothing,\n           precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  3rd order stiffly accurate Rosenbrock-Wanner W-method with 4 internal stages, B_PR consistent of order 2. The order of convergence decreases if medium stiff problems are considered.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROS34PRw(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nJoachim Rang, Improved traditional Rosenbrock–Wanner methods for stiff ODEs and DAEs, Journal of Computational and Applied Mathematics, https://doi.org/10.1016/j.cam.2015.03.010\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3PRL-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3PRL","text":"ROS3PRL(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n              ForwardDiff default function-specific tags. For more information, see\n              [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n              Defaults to `Val{true}()`.\n          - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n              to specify whether to use automatic differentiation via\n              [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n              differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n              Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n              `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n              To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n              `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n          - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n              `nothing`, which means it will be chosen true/false depending on circumstances\n              of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n          - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n            For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n            `ROS3PRL(linsolve = KLUFactorization()`).\n             When `nothing` is passed, uses `DefaultLinearSolver`.\n          - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n            can be used as a left or right preconditioner.\n            Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n            function where the arguments are defined as:\n              - `W`: the current Jacobian of the nonlinear system. Specified as either\n                  ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                  commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                  representation of the operator. Users can construct the W-matrix on demand\n                  by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                  the `jac_prototype`.\n              - `du`: the current ODE derivative\n              - `u`: the current ODE state\n              - `p`: the ODE parameters\n              - `t`: the current ODE time\n              - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                  the last call to `precs`. It is recommended that this is checked to only\n                  update the preconditioner when `newW == true`.\n              - `Plprev`: the previous `Pl`.\n              - `Prprev`: the previous `Pr`.\n              - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                  Solver-dependent and subject to change.\n            The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n            To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n            which is not used. Additionally, `precs` must supply the dispatch:\n            ```julia\n            Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n            ```\n            which is used in the solver setup phase to construct the integrator\n            type with the preconditioners `(Pl,Pr)`.\n            The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n            is defined as:\n            ```julia\n            DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n            ```)\n\nRosenbrock-Wanner Method.  3rd order stiffly accurate Rosenbrock method with 4 internal stages, B_PR consistent of order 2 with Rinf=0. The order of convergence decreases if medium stiff problems are considered, but it has good results for very stiff cases.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROS3PRL2-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROS3PRL2","text":"ROS3PRL2(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n               ForwardDiff default function-specific tags. For more information, see\n               [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n               Defaults to `Val{true}()`.\n           - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n               to specify whether to use automatic differentiation via\n               [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n               differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n               Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n               `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n               To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n               `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n           - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n               `nothing`, which means it will be chosen true/false depending on circumstances\n               of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n           - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n             For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n             `ROS3PRL2(linsolve = KLUFactorization()`).\n              When `nothing` is passed, uses `DefaultLinearSolver`.\n           - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n             can be used as a left or right preconditioner.\n             Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n             function where the arguments are defined as:\n               - `W`: the current Jacobian of the nonlinear system. Specified as either\n                   ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                   commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                   representation of the operator. Users can construct the W-matrix on demand\n                   by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                   the `jac_prototype`.\n               - `du`: the current ODE derivative\n               - `u`: the current ODE state\n               - `p`: the ODE parameters\n               - `t`: the current ODE time\n               - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                   the last call to `precs`. It is recommended that this is checked to only\n                   update the preconditioner when `newW == true`.\n               - `Plprev`: the previous `Pl`.\n               - `Prprev`: the previous `Pr`.\n               - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                   Solver-dependent and subject to change.\n             The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n             To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n             which is not used. Additionally, `precs` must supply the dispatch:\n             ```julia\n             Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n             ```\n             which is used in the solver setup phase to construct the integrator\n             type with the preconditioners `(Pl,Pr)`.\n             The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n             is defined as:\n             ```julia\n             DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n             ```)\n\nRosenbrock-Wanner Method.  3rd order stiffly accurate Rosenbrock method with 4 internal stages, B_PR consistent of order 3. The order of convergence does NOT decreases if medium stiff problems are considered as it does for ROS3PRL.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nRang, Joachim (2014): The Prothero and Robinson example: Convergence studies for Runge-Kutta and Rosenbrock-Wanner methods. https://doi.org/10.24355/dbbs.084-201408121139-0\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.ROK4a-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.ROK4a","text":"ROK4a(; chunk_size = Val{0}(),\n        standardtag = Val{true}(),\n        autodiff = AutoForwardDiff(),\n        concrete_jac = nothing,\n        diff_type = Val{:forward}(),\n        linsolve = nothing,\n        precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  4rd order L-stable Rosenbrock-Krylov method with 4 internal stages, with a 3rd order embedded method which is strongly A-stable with Rinf~=0.55. (when using exact Jacobians)\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ROK4a(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nTranquilli, Paul and Sandu, Adrian (2014): Rosenbrock–Krylov Methods for Large Systems of Differential Equations https://doi.org/10.1137/130923336\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.RosShamp4-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.RosShamp4","text":"RosShamp4(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n                ForwardDiff default function-specific tags. For more information, see\n                [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n                Defaults to `Val{true}()`.\n            - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n                to specify whether to use automatic differentiation via\n                [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n                differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n                Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n                `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n                To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n                `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n            - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n                `nothing`, which means it will be chosen true/false depending on circumstances\n                of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n            - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n              For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n              `RosShamp4(linsolve = KLUFactorization()`).\n               When `nothing` is passed, uses `DefaultLinearSolver`.\n            - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n              can be used as a left or right preconditioner.\n              Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n              function where the arguments are defined as:\n                - `W`: the current Jacobian of the nonlinear system. Specified as either\n                    ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                    commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                    representation of the operator. Users can construct the W-matrix on demand\n                    by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                    the `jac_prototype`.\n                - `du`: the current ODE derivative\n                - `u`: the current ODE state\n                - `p`: the ODE parameters\n                - `t`: the current ODE time\n                - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                    the last call to `precs`. It is recommended that this is checked to only\n                    update the preconditioner when `newW == true`.\n                - `Plprev`: the previous `Pl`.\n                - `Prprev`: the previous `Pr`.\n                - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                    Solver-dependent and subject to change.\n              The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n              To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n              which is not used. Additionally, `precs` must supply the dispatch:\n              ```julia\n              Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n              ```\n              which is used in the solver setup phase to construct the integrator\n              type with the preconditioners `(Pl,Pr)`.\n              The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n              is defined as:\n              ```julia\n              DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n              ```)\n\nRosenbrock-Wanner Method.  An A-stable 4th order Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nL. F. Shampine, Implementation of Rosenbrock Methods, ACM Transactions on Mathematical Software (TOMS), 8: 2, 93-113. doi:10.1145/355993.355994\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Veldd4-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Veldd4","text":"Veldd4(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n             ForwardDiff default function-specific tags. For more information, see\n             [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n             Defaults to `Val{true}()`.\n         - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n             to specify whether to use automatic differentiation via\n             [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n             differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n             Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n             `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n             To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n             `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n         - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n             `nothing`, which means it will be chosen true/false depending on circumstances\n             of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n         - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n           For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n           `Veldd4(linsolve = KLUFactorization()`).\n            When `nothing` is passed, uses `DefaultLinearSolver`.\n         - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n           can be used as a left or right preconditioner.\n           Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n           function where the arguments are defined as:\n             - `W`: the current Jacobian of the nonlinear system. Specified as either\n                 ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                 commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                 representation of the operator. Users can construct the W-matrix on demand\n                 by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                 the `jac_prototype`.\n             - `du`: the current ODE derivative\n             - `u`: the current ODE state\n             - `p`: the ODE parameters\n             - `t`: the current ODE time\n             - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                 the last call to `precs`. It is recommended that this is checked to only\n                 update the preconditioner when `newW == true`.\n             - `Plprev`: the previous `Pl`.\n             - `Prprev`: the previous `Pr`.\n             - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                 Solver-dependent and subject to change.\n           The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n           To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n           which is not used. Additionally, `precs` must supply the dispatch:\n           ```julia\n           Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n           ```\n           which is used in the solver setup phase to construct the integrator\n           type with the preconditioners `(Pl,Pr)`.\n           The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n           is defined as:\n           ```julia\n           DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n           ```)\n\nRosenbrock-Wanner Method.  A 4th order D-stable Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nvan Veldhuizen, D-stability and Kaps-Rentrop-methods, M. Computing (1984) 32: 229. doi:10.1007/BF02243574\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Velds4-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Velds4","text":"Velds4(; chunk_size = Val{0}(),\n         standardtag = Val{true}(),\n         autodiff = AutoForwardDiff(),\n         concrete_jac = nothing,\n         diff_type = Val{:forward}(),\n         linsolve = nothing,\n         precs = DEFAULT_PRECS)\n\nRosenbrock-Wanner-W(olfbrandt) Method.  A 4th order A-stable Rosenbrock method.\n\nKeyword Arguments\n\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify Velds4(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n\nReferences\n\nvan Veldhuizen, D-stability and Kaps-Rentrop-methods, M. Computing (1984) 32: 229. doi:10.1007/BF02243574\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.GRK4T-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.GRK4T","text":"GRK4T(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n            ForwardDiff default function-specific tags. For more information, see\n            [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n            Defaults to `Val{true}()`.\n        - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n            to specify whether to use automatic differentiation via\n            [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n            differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n            Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n            `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n            To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n            `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n        - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n            `nothing`, which means it will be chosen true/false depending on circumstances\n            of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n        - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n          For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n          `GRK4T(linsolve = KLUFactorization()`).\n           When `nothing` is passed, uses `DefaultLinearSolver`.\n        - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n          can be used as a left or right preconditioner.\n          Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n          function where the arguments are defined as:\n            - `W`: the current Jacobian of the nonlinear system. Specified as either\n                ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                representation of the operator. Users can construct the W-matrix on demand\n                by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                the `jac_prototype`.\n            - `du`: the current ODE derivative\n            - `u`: the current ODE state\n            - `p`: the ODE parameters\n            - `t`: the current ODE time\n            - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                the last call to `precs`. It is recommended that this is checked to only\n                update the preconditioner when `newW == true`.\n            - `Plprev`: the previous `Pl`.\n            - `Prprev`: the previous `Pr`.\n            - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                Solver-dependent and subject to change.\n          The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n          To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n          which is not used. Additionally, `precs` must supply the dispatch:\n          ```julia\n          Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n          ```\n          which is used in the solver setup phase to construct the integrator\n          type with the preconditioners `(Pl,Pr)`.\n          The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n          is defined as:\n          ```julia\n          DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n          ```)\n\nRosenbrock-Wanner Method.  An efficient 4th order Rosenbrock method.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nKaps, P. & Rentrop, Generalized Runge-Kutta methods of order four with stepsize control for stiff ordinary differential equations. P. Numer. Math. (1979) 33: 55. doi:10.1007/BF01396495\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.GRK4A-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.GRK4A","text":"GRK4A(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n            ForwardDiff default function-specific tags. For more information, see\n            [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n            Defaults to `Val{true}()`.\n        - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n            to specify whether to use automatic differentiation via\n            [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n            differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n            Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n            `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n            To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n            `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n        - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n            `nothing`, which means it will be chosen true/false depending on circumstances\n            of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n        - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n          For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n          `GRK4A(linsolve = KLUFactorization()`).\n           When `nothing` is passed, uses `DefaultLinearSolver`.\n        - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n          can be used as a left or right preconditioner.\n          Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n          function where the arguments are defined as:\n            - `W`: the current Jacobian of the nonlinear system. Specified as either\n                ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                representation of the operator. Users can construct the W-matrix on demand\n                by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                the `jac_prototype`.\n            - `du`: the current ODE derivative\n            - `u`: the current ODE state\n            - `p`: the ODE parameters\n            - `t`: the current ODE time\n            - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                the last call to `precs`. It is recommended that this is checked to only\n                update the preconditioner when `newW == true`.\n            - `Plprev`: the previous `Pl`.\n            - `Prprev`: the previous `Pr`.\n            - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                Solver-dependent and subject to change.\n          The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n          To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n          which is not used. Additionally, `precs` must supply the dispatch:\n          ```julia\n          Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n          ```\n          which is used in the solver setup phase to construct the integrator\n          type with the preconditioners `(Pl,Pr)`.\n          The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n          is defined as:\n          ```julia\n          DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n          ```)\n\nRosenbrock-Wanner Method.  An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nKaps, P. & Rentrop, Generalized Runge-Kutta methods of order four with stepsize control for stiff ordinary differential equations. P. Numer. Math. (1979) 33: 55. doi:10.1007/BF01396495\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/Rosenbrock/#OrdinaryDiffEqRosenbrock.Ros4LStab-api-ordinarydiffeq-massmatrixdae-Rosenbrock","page":"OrdinaryDiffEqRosenbrock","title":"OrdinaryDiffEqRosenbrock.Ros4LStab","text":"Ros4LStab(; - `standardtag`: Specifies whether to use package-specific tags instead of the\n                ForwardDiff default function-specific tags. For more information, see\n                [this blog post](https://www.stochasticlifestyle.com/improved-forwarddiff-jl-stacktraces-with-package-tags/).\n                Defaults to `Val{true}()`.\n            - `autodiff`: Uses [ADTypes.jl](https://sciml.github.io/ADTypes.jl/stable/) \n                to specify whether to use automatic differentiation via\n                [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl) or finite\n                differencing via [FiniteDiff.jl](https://github.com/JuliaDiff/FiniteDiff.jl). \n                Defaults to `AutoForwardDiff()` for automatic differentiation, which by default uses\n                `chunksize = 0`, and thus uses the internal ForwardDiff.jl algorithm for the choice.\n                To use `FiniteDiff.jl`, the `AutoFiniteDiff()` ADType can be used, which has a keyword argument\n                `fdtype` with default value `Val{:forward}()`, and alternatives `Val{:central}()` and `Val{:complex}()`.\n            - `concrete_jac`: Specifies whether a Jacobian should be constructed. Defaults to\n                `nothing`, which means it will be chosen true/false depending on circumstances\n                of the solver, such as whether a Krylov subspace method is used for `linsolve`.\n            - `linsolve`: Any [LinearSolve.jl](https://github.com/SciML/LinearSolve.jl) compatible linear solver.\n              For example, to use [KLU.jl](https://github.com/JuliaSparse/KLU.jl), specify\n              `Ros4LStab(linsolve = KLUFactorization()`).\n               When `nothing` is passed, uses `DefaultLinearSolver`.\n            - `precs`: Any [LinearSolve.jl-compatible preconditioner](https://docs.sciml.ai/LinearSolve/stable/basics/Preconditioners/)\n              can be used as a left or right preconditioner.\n              Preconditioners are specified by the `Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata)`\n              function where the arguments are defined as:\n                - `W`: the current Jacobian of the nonlinear system. Specified as either\n                    ``I - \\gamma J`` or ``I/\\gamma - J`` depending on the algorithm. This will\n                    commonly be a `WOperator` type defined by OrdinaryDiffEq.jl. It is a lazy\n                    representation of the operator. Users can construct the W-matrix on demand\n                    by calling `convert(AbstractMatrix,W)` to receive an `AbstractMatrix` matching\n                    the `jac_prototype`.\n                - `du`: the current ODE derivative\n                - `u`: the current ODE state\n                - `p`: the ODE parameters\n                - `t`: the current ODE time\n                - `newW`: a `Bool` which specifies whether the `W` matrix has been updated since\n                    the last call to `precs`. It is recommended that this is checked to only\n                    update the preconditioner when `newW == true`.\n                - `Plprev`: the previous `Pl`.\n                - `Prprev`: the previous `Pr`.\n                - `solverdata`: Optional extra data the solvers can give to the `precs` function.\n                    Solver-dependent and subject to change.\n              The return is a tuple `(Pl,Pr)` of the LinearSolve.jl-compatible preconditioners.\n              To specify one-sided preconditioning, simply return `nothing` for the preconditioner\n              which is not used. Additionally, `precs` must supply the dispatch:\n              ```julia\n              Pl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n              ```\n              which is used in the solver setup phase to construct the integrator\n              type with the preconditioners `(Pl,Pr)`.\n              The default is `precs=DEFAULT_PRECS` where the default preconditioner function\n              is defined as:\n              ```julia\n              DEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n              ```)\n\nRosenbrock-Wanner Method.  A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\n\nKeyword Arguments\n\nchunk_size: TBD\nstandardtag: TBD\nautodiff: boolean to control if the Jacobian should be computed via AD or not\nconcrete_jac: function of the form jac!(J, u, p, t)\ndiff_type: TBD\nlinsolve: custom solver for the inner linear systems\nprecs: custom preconditioner for the inner linear solver\n\nReferences\n\nE. Hairer, G. Wanner, Solving ordinary differential equations II, stiff and differential-algebraic problems. Computational mathematics (2nd revised ed.), Springer (1996)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRA/SRI-Methods-Stochastic-Runge-Kutta","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRA/SRI Methods - Stochastic Runge-Kutta","text":"The SRA (Stochastic Runge-Kutta for Additive noise) and SRI (Stochastic Runge-Kutta for Itô) methods provide high-order adaptive solvers for different noise structures. These are among the most effective methods for their respective problem classes.","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#Recommended-Methods","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"Recommended Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SOSRI-Stability-Optimized-SRI-(Recommended)","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SOSRI - Stability-Optimized SRI (Recommended)","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SOSRA-Stability-Optimized-SRA-(Optimal-for-Additive-Noise)","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SOSRA - Stability-Optimized SRA (Optimal for Additive Noise)","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#Alternative-SRI-Methods","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"Alternative SRI Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRIW1-SRI-Weak-Order-2","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRIW1 - SRI Weak Order 2","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRIW2-SRI-Weak-Order-3","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRIW2 - SRI Weak Order 3","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SOSRI2-Alternative-Stability-Optimized-SRI","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SOSRI2 - Alternative Stability-Optimized SRI","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#Alternative-SRA-Methods","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"Alternative SRA Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRA1-Original-SRA-Method","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRA1 - Original SRA Method","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRA2-SRA-Method-Version-2","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRA2 - SRA Method Version 2","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRA3-SRA-Method-with-Weak-Order-3","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRA3 - SRA Method with Weak Order 3","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SOSRA2-Alternative-Stability-Optimized-SRA","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SOSRA2 - Alternative Stability-Optimized SRA","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#Configurable-Methods","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"Configurable Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRA-Configurable-SRA-with-Custom-Tableaux","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRA - Configurable SRA with Custom Tableaux","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#SRI-Configurable-SRI-with-Custom-Tableaux","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"SRI - Configurable SRI with Custom Tableaux","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#Method-Selection-Guide","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"Method Selection Guide","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#For-Diagonal/Scalar-Noise:","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"For Diagonal/Scalar Noise:","text":"First choice: SOSRI - Best overall performance and stability\nAlternative: SRIW1 - Standard SRI method\nHigh weak order: SRIW2 - When weak order 3 is needed","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#For-Additive-Noise:","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"For Additive Noise:","text":"First choice: SOSRA - Optimal for additive noise structure\nAlternative: SRA1 - Standard SRA method\nHigh weak order: SRA3 - When weak order 3 is needed","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#Performance-Characteristics:","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"Performance Characteristics:","text":"SOSRI/SOSRA: Stability-optimized, robust to high tolerances\nSRIWx/SRAx: Standard methods with proven theoretical properties\nSRA/SRI: Allow custom tableaux for specialized applications","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#Theoretical-Foundation","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"Theoretical Foundation","text":"The SRA and SRI methods are based on stochastic Runge-Kutta theory:\n\nSRA Methods exploit the additive noise structure:\n\ndu = f(u,t)dt + σ(t)dW\n\nWhere the diffusion σ doesn't depend on the solution u.\n\nSRI Methods handle the general diagonal case:\n\ndu = f(u,t)dt + g(u,t)dW\n\nWhere each component has independent noise.\n\nBoth method families achieve:\n\nStrong order 1.5 convergence\nWeak order 2.0 or higher\nAdaptive time stepping with embedded error estimation\nA-stable or L-stable properties (for optimized versions)","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#References","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"References","text":"Rößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SOSRI","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SOSRI","text":"SOSRI()\n\nSOSRI: Stability-Optimized SRI Method (Nonstiff) - Recommended\n\nThe Stability-Optimized Stochastic Runge-Kutta method. This is the recommended method for general-purpose solving of diagonal/scalar Itô SDEs.\n\nMethod Properties\n\nStrong Order: 1.5 (for diagonal/scalar noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Diagonal and scalar noise only\nSDE interpretation: Itô\nStability: Optimized for high tolerances and robust to mild stiffness\n\nWhen to Use\n\nRecommended as first choice for diagonal/scalar Itô SDEs\nWhen high accuracy is required (strong order 1.5)\nFor problems with mild stiffness\nWhen using high tolerances (method is stable)\nFor most general SDE applications\n\nAlgorithm Description\n\nSOSRI is a stability-optimized version of the SRI methods with specially chosen coefficients to improve stability properties. It provides excellent performance for the most common class of SDE problems.\n\nRestrictions\n\nOnly works with diagonal or scalar noise\nFor non-diagonal noise, use other methods like RKMilCommute or LambaEM\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SOSRA","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SOSRA","text":"SOSRA()\n\nSOSRA: Stability-Optimized SRA Method (Nonstiff) - Optimal for Additive Noise\n\nStability-optimized adaptive Stochastic Runge-Kutta method for additive noise problems. This is the optimal choice for additive noise SDEs.\n\nMethod Properties\n\nStrong Order: 1.5 (for additive noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Additive noise (diagonal, non-diagonal, and scalar)\nSDE interpretation: Both Itô and Stratonovich\nStability: Optimized for high tolerances and robust to stiffness\n\nWhen to Use\n\nOptimal choice for additive noise problems: du = f(u,p,t)dt + σ dW\nWhen the diffusion term is independent of the solution u\nFor problems requiring high accuracy with additive noise\nWhen using high tolerances (method is stable)\nFor both Itô and Stratonovich interpretations\n\nAlgorithm Description\n\nSOSRA is a stability-optimized version of the SRA (Stochastic Runge-Kutta for Additive noise) methods. It exploits the special structure of additive noise to achieve better performance and stability.\n\nAdditive Noise Structure\n\nSpecialized for SDEs of the form:\n\ndu = f(u,p,t)dt + σ(t) dW\n\nwhere the diffusion σ does not depend on the solution u.\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SRIW1","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SRIW1","text":"Rößler A., Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations, SIAM J. Numer. Anal., 48 (3), pp. 922–952. DOI:10.1137/09076636X\n\nSRIW1()\n\nSRIW1: Stochastic Runge-Kutta W1 Method (Nonstiff)\n\nAdaptive stochastic Runge-Kutta method with strong order 1.5 and weak order 2.0 for diagonal/scalar Itô SDEs.\n\nMethod Properties\n\nStrong Order: 1.5 (for diagonal/scalar noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Diagonal and scalar noise only\nSDE interpretation: Itô\n\nWhen to Use\n\nStandard choice for diagonal/scalar Itô SDEs\nWhen proven theoretical properties are important\nAlternative to SOSRI when stability optimization is not needed\nFor problems requiring exactly weak order 2.0\n\nAlgorithm Features\n\nEmbedded error estimation for adaptive stepping\nWell-established theoretical foundation\nGood balance of accuracy and efficiency\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SRIW2","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SRIW2","text":"Rößler A., Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations, SIAM J. Numer. Anal., 48 (3), pp. 922–952. DOI:10.1137/09076636X\n\nSRIW2()\n\nSRIW2: Stochastic Runge-Kutta W2 Method (Nonstiff)\n\nAdaptive stochastic Runge-Kutta method with strong order 1.5 and weak order 3.0 for diagonal/scalar Itô SDEs.\n\nMethod Properties\n\nStrong Order: 1.5 (for diagonal/scalar noise)\nWeak Order: 3.0\nTime stepping: Adaptive\nNoise types: Diagonal and scalar noise only\nSDE interpretation: Itô\n\nWhen to Use\n\nWhen weak order 3.0 convergence is required\nFor Monte Carlo simulations needing high weak accuracy\nProblems where weak convergence is more important than strong\nWhen computational cost per step is acceptable for higher weak order\n\nAlgorithm Features\n\nHighest weak order in the SRI family\nMore expensive per step than SRIW1\nExcellent for statistical calculations and expectations\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SOSRI2","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SOSRI2","text":"SOSRI2()\n\nSOSRI2: Alternative Stability-Optimized SRI Method (Nonstiff)\n\nAlternative stability-optimized adaptive strong order 1.5 method with different stability characteristics than SOSRI.\n\nMethod Properties\n\nStrong Order: 1.5 (for diagonal/scalar noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Diagonal and scalar noise only\nSDE interpretation: Itô\nStability: Optimized for high tolerances and robust to stiffness\n\nWhen to Use\n\nAlternative to SOSRI with different stability properties\nWhen SOSRI performance is unsatisfactory\nFor benchmarking stability-optimized methods\nProblems requiring different stability characteristics\n\nAlgorithm Features\n\nDifferent stability optimization than SOSRI\nMay perform better on certain problem types\nMaintains high tolerance robustness\n\nReferences\n\nStability-optimized SRI methods\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SRA1","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SRA1","text":"Rößler A., Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations, SIAM J. Numer. Anal., 48 (3), pp. 922–952. DOI:10.1137/09076636X\n\nSRA1()\n\nSRA1: Stochastic Runge-Kutta A1 Method (Nonstiff)\n\nAdaptive strong order 1.5 method for additive Itô and Stratonovich SDEs with weak order 2.\n\nMethod Properties\n\nStrong Order: 1.5 (for additive noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Additive noise (diagonal, non-diagonal, and scalar)\nSDE interpretation: Both Itô and Stratonovich\n\nWhen to Use\n\nStandard choice for additive noise problems\nWhen proven theoretical properties are important\nAlternative to SOSRA when stability optimization is not needed\nFor both Itô and Stratonovich problems with additive noise\n\nAdditive Noise Structure\n\nSpecialized for SDEs of the form:\n\ndu = f(u,p,t)dt + σ(p,t) dW\n\nwhere diffusion σ doesn't depend on solution u.\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SRA2","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SRA2","text":"Rößler A., Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations, SIAM J. Numer. Anal., 48 (3), pp. 922–952. DOI:10.1137/09076636X\n\nSRA2()\n\nSRA2: Stochastic Runge-Kutta A2 Method (Nonstiff)\n\nAlternative adaptive strong order 1.5 method for additive noise problems with different coefficients.\n\nMethod Properties\n\nStrong Order: 1.5 (for additive noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Additive noise (diagonal, non-diagonal, and scalar)\nSDE interpretation: Both Itô and Stratonovich\n\nWhen to Use\n\nAlternative to SRA1 with different stability/accuracy characteristics\nWhen SRA1 performance is unsatisfactory\nFor benchmarking different SRA variants\nResearch and comparison studies\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SRA3","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SRA3","text":"Rößler A., Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations, SIAM J. Numer. Anal., 48 (3), pp. 922–952. DOI:10.1137/09076636X\n\nSRA3()\n\nSRA3: Stochastic Runge-Kutta A3 Method (Nonstiff)\n\nAdaptive strong order 1.5 method for additive noise problems with weak order 3.\n\nMethod Properties\n\nStrong Order: 1.5 (for additive noise)\nWeak Order: 3.0\nTime stepping: Adaptive\nNoise types: Additive noise (non-diagonal and scalar)\nSDE interpretation: Both Itô and Stratonovich\n\nWhen to Use\n\nWhen weak order 3.0 convergence is required for additive noise\nFor Monte Carlo simulations needing highest weak accuracy\nProblems where weak convergence dominates computational cost\nWhen computational cost per step is acceptable for higher weak order\n\nRestrictions\n\nDoes not handle diagonal additive noise (use SRA1/SRA2 instead)\nLimited to non-diagonal and scalar additive noise structures\n\nAlgorithm Features\n\nHighest weak order in the SRA family\nMore expensive per step than SRA1/SRA2\nExcellent for statistical calculations requiring high weak accuracy\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SOSRA2","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SOSRA2","text":"SOSRA2()\n\nSOSRA2: Stability-Optimized SRA Method Version 2 (Nonstiff)\n\nAlternative stability-optimized adaptive SRA method for additive noise problems.\n\nMethod Properties\n\nStrong Order: 1.5 (for additive noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Additive noise (diagonal, non-diagonal, and scalar)\nSDE interpretation: Both Itô and Stratonovich\nStability: Optimized for high tolerances and robust to stiffness\n\nWhen to Use\n\nAlternative to SOSRA for additive noise problems\nDifferent stability characteristics may be preferred for specific problems\nWhen SOSRA performance is unsatisfactory\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SRA","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SRA","text":"Rößler A., Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations, SIAM J. Numer. Anal., 48 (3), pp. 922–952. DOI:10.1137/09076636X\n\nSRA(;tableau=constructSRA1())\n\nSRA: Configurable Stochastic Runge-Kutta for Additive Noise (Nonstiff)\n\nConfigurable adaptive strong order 1.5 method for additive noise problems with customizable tableaux.\n\nMethod Properties\n\nStrong Order: 1.5 (for additive noise)\nWeak Order: Depends on tableau (typically 2.0)\nTime stepping: Adaptive\nNoise types: Additive noise (diagonal, non-diagonal, and scalar)\nSDE interpretation: Both Itô and Stratonovich\n\nParameters\n\ntableau: Tableau specification (default: constructSRA1())\n\nWhen to Use\n\nWhen custom tableaux are needed for additive noise problems\nFor research and experimentation with SRA methods\nWhen default methods don't provide desired characteristics\nFor benchmarking different SRA variants\n\nAvailable Tableaux\n\nconstructSRA1(): Default SRA1 tableau\nCustom tableaux can be constructed for specialized applications\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/sra_sri_methods/#StochasticDiffEq.SRI","page":"SRA/SRI Methods - Stochastic Runge-Kutta","title":"StochasticDiffEq.SRI","text":"Rößler A., Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations, SIAM J. Numer. Anal., 48 (3), pp. 922–952. DOI:10.1137/09076636X\n\nSRI(;tableau=constructSRIW1(), error_terms=4)\n\nSRI: Configurable Stochastic Runge-Kutta for Itô SDEs (Nonstiff)\n\nConfigurable adaptive strong order 1.5 method for diagonal/scalar Itô SDEs with customizable tableaux.\n\nMethod Properties\n\nStrong Order: 1.5 (for diagonal/scalar noise)\nWeak Order: Depends on tableau (typically 2.0)\nTime stepping: Adaptive\nNoise types: Diagonal and scalar noise only\nSDE interpretation: Itô\n\nParameters\n\ntableau: Tableau specification (default: constructSRIW1())\nerror_terms::Int = 4: Number of error terms for adaptive stepping\n\nWhen to Use\n\nWhen custom tableaux are needed for diagonal/scalar problems\nFor research and experimentation with SRI methods\nWhen default methods don't provide desired characteristics\nFor benchmarking different SRI variants\n\nAvailable Tableaux\n\nconstructSRIW1(): Default SRIW1 tableau\nCustom tableaux can be constructed for specialized applications\n\nReferences\n\nRößler A., \"Runge–Kutta Methods for the Strong Approximation of Solutions of Stochastic Differential Equations\", SIAM J. Numer. Anal., 48 (3), pp. 922–952\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK","text":"Low-order explicit Runge-Kutta methods for non-stiff differential equations. Most of the time, you should use Tsit5, which is the most common and efficient low-order RK method. The alternative methods provided here are for special circumstances where Tsit5 is not suitable.","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Key-Properties","page":"OrdinaryDiffEqLowOrderRK","title":"Key Properties","text":"Low-order explicit RK methods offer:\n\nComputational efficiency at higher tolerances (>1e-6)\nRobust error control for difficult non-stiff problems\nSpecialized interpolation properties for applications requiring dense output\nLower-order derivatives requirements for non-smooth functions\nGood performance for specific problem types","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#When-to-Use-Alternative-Low-Order-RK-Methods","page":"OrdinaryDiffEqLowOrderRK","title":"When to Use Alternative Low-Order RK Methods","text":"Choose these methods instead of Tsit5 when:\n\nODE function f is not differentiable to 5th order - use lower-order methods (the more discontinuous, the lower the order needed)\nHeavy use of interpolations - OwrenZen methods have superior interpolation convergence\nDelay differential equations - OwrenZen methods are most efficient (see SciMLBenchmarks)\nVery high tolerances (>1e-3) - BS3 is more efficient than Tsit5\nQuadratic polynomial ODEs - SIR54 is optimized for these systems\nEducational purposes - simpler methods for understanding algorithms","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqLowOrderRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Primary-recommendation","page":"OrdinaryDiffEqLowOrderRK","title":"Primary recommendation","text":"For most problems, use Tsit5 instead of these methods.","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#High-tolerances-(1e-3)","page":"OrdinaryDiffEqLowOrderRK","title":"High tolerances (>1e-3)","text":"BS3: Third-order Bogacki-Shampine method, most efficient for very high tolerances","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Superior-interpolation-needs","page":"OrdinaryDiffEqLowOrderRK","title":"Superior interpolation needs","text":"OwrenZen3: Third-order with excellent interpolation convergence\nOwrenZen5: Fifth-order with excellent interpolation, optimal for DDEs\nOwrenZen4: Fourth-order interpolation-optimized method","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Non-smooth-or-discontinuous-ODEs","page":"OrdinaryDiffEqLowOrderRK","title":"Non-smooth or discontinuous ODEs","text":"BS3: Third-order for mildly non-smooth functions\nHeun: Second-order for more discontinuous functions (not generally recommended)\nEuler: First-order for highly discontinuous problems","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Robust-error-control-alternatives","page":"OrdinaryDiffEqLowOrderRK","title":"Robust error control alternatives","text":"BS5: Fifth-order with very robust error estimation\nDP5: Fifth-order Dormand-Prince method, classical alternative to Tsit5","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Specialized-applications","page":"OrdinaryDiffEqLowOrderRK","title":"Specialized applications","text":"RK4: Fourth-order with special residual error control, good for DDEs. Note: Uses adaptive timestepping by default - set adaptive=false in solve() for traditional fixed-step RK4\nSIR54: Fifth-order optimized for ODEs defined by quadratic polynomials (e.g., SIR-type epidemiological models)\nStepanov5: Fifth-order method with enhanced stability properties and optimized error constants\nRalston: Second-order with optimized error constants","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Periodic-and-oscillatory-problems","page":"OrdinaryDiffEqLowOrderRK","title":"Periodic and oscillatory problems","text":"Anas5: Fifth-order optimized for periodic problems with minimal phase error\nFRK65: Sixth-order zero dissipation method for oscillatory problems","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Advanced-specialized-methods","page":"OrdinaryDiffEqLowOrderRK","title":"Advanced specialized methods","text":"RKO65: Sixth-order optimized method\nMSRK5, MSRK6: Multi-stage methods for specific applications\nPSRK4p7q6, PSRK3p5q4, PSRK3p6q5: Pseudo-symplectic methods\nAlshina2, Alshina3, Alshina6: Methods with optimized parameters\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqLowOrderRK\", \"BS3\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#Full-list-of-solvers","page":"OrdinaryDiffEqLowOrderRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Euler","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Euler","text":"Euler()\n\nExplicit Runge-Kutta Method. The canonical forward Euler method. Fixed timestep only.\n\nKeyword Arguments\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I. Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics, Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Heun","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Heun","text":"Heun(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n       step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n       thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  The second order Heun's method. Uses embedded Euler method for adaptivity.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I. Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics, Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Ralston","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Ralston","text":"Ralston(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  The optimized second order midpoint method. Uses embedded Euler method for adaptivity.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I. Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics, Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Midpoint","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Midpoint","text":"Midpoint(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  The second order midpoint method. Uses embedded Euler method for adaptivity.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I. Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics, Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.RK4","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.RK4","text":"RK4(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  The canonical Runge-Kutta Order 4 method. Uses a defect control for adaptive stepping using maximum error over the whole interval. Classic fourth-order method. Good for medium accuracy calculations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{shampine2005solving,       title={Solving ODEs and DDEs with residual control},       author={Shampine, LF},       journal={Applied Numerical Mathematics},       volume={52},       number={1},       pages={113–127},       year={2005},       publisher={Elsevier}       }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.BS3","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.BS3","text":"BS3(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Bogacki-Shampine 3/2 method. Third-order adaptive method using embedded Euler method for adaptivity. Recommended for non-stiff problems at moderate tolerances.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{bogacki19893,     title={A 3 (2) pair of Runge-Kutta formulas},     author={Bogacki, Przemyslaw and Shampine, Lawrence F},     journal={Applied Mathematics Letters},     volume={2},     number={4},     pages={321–325},     year={1989},     publisher={Elsevier}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.OwrenZen3","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.OwrenZen3","text":"OwrenZen3(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Owren-Zennaro optimized interpolation 3/2 method (free 3rd order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{owren1992derivation,     title={Derivation of efficient, continuous, explicit Runge–Kutta methods},     author={Owren, Brynjulf and Zennaro, Marino},     journal={SIAM journal on scientific and statistical computing},     volume={13},     number={6},     pages={1488–1501},     year={1992},     publisher={SIAM}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.OwrenZen4","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.OwrenZen4","text":"OwrenZen4(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Owren-Zennaro optimized interpolation 4/3 method (free 4th order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{owren1992derivation,     title={Derivation of efficient, continuous, explicit Runge–Kutta methods},     author={Owren, Brynjulf and Zennaro, Marino},     journal={SIAM journal on scientific and statistical computing},     volume={13},     number={6},     pages={1488–1501},     year={1992},     publisher={SIAM}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.OwrenZen5","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.OwrenZen5","text":"OwrenZen5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Owren-Zennaro optimized interpolation 5/4 method (free 5th order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{owren1992derivation,     title={Derivation of efficient, continuous, explicit Runge–Kutta methods},     author={Owren, Brynjulf and Zennaro, Marino},     journal={SIAM journal on scientific and statistical computing},     volume={13},     number={6},     pages={1488–1501},     year={1992},     publisher={SIAM}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.BS5","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.BS5","text":"BS5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      thread = OrdinaryDiffEq.False(),\n      lazy = true)\n\nExplicit Runge-Kutta Method.  Bogacki-Shampine 5/4 Runge-Kutta method. (lazy 5th order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nlazy: determines if the lazy interpolant is used.\n\nReferences\n\n@article{bogacki1996efficient,     title={An efficient runge-kutta (4, 5) pair},     author={Bogacki, P and Shampine, Lawrence F},     journal={Computers \\& Mathematics with Applications},     volume={32},     number={6},     pages={15–28},     year={1996},     publisher={Elsevier}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.DP5","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.DP5","text":"DP5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Dormand-Prince's 5/4 Runge-Kutta method. (free 4th order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{dormand1980family,     title={A family of embedded Runge-Kutta formulae},     author={Dormand, John R and Prince, Peter J},     journal={Journal of computational and applied mathematics},     volume={6},     number={1},     pages={19–26},     year={1980},     publisher={Elsevier}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Anas5","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Anas5","text":"Anas5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False(),\n        w = 1)\n\nExplicit Runge-Kutta Method.  4th order Runge-Kutta method designed for periodic problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nw: a periodicity estimate, which when accurate the method becomes 5th order\n\n(and is otherwise 4th order with less error for better estimates).\n\nReferences\n\n@article{anastassi2005optimized, title={An optimized Runge–Kutta method for the solution of orbital problems}, author={Anastassi, ZA and Simos, TE}, journal={Journal of Computational and Applied Mathematics}, volume={175}, number={1}, pages={1–9}, year={2005}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.RKO65","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.RKO65","text":"RKO65(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Tsitouras' Runge-Kutta-Oliver 6 stage 5th order method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nTsitouras, Ch. \"Explicit Runge–Kutta methods for starting integration of     Lane–Emden problem.\" Applied Mathematics and Computation 354 (2019): 353-364.     doi: https://doi.org/10.1016/j.amc.2019.02.047\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.FRK65","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.FRK65","text":"FRK65(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False(),\n        omega = 0.0)\n\nExplicit Runge-Kutta Method.  Zero Dissipation Runge-Kutta of 6th order.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nomega: a periodicity phase estimate,\n\nwhen accurate this method results in zero numerical dissipation.\n\nReferences\n\n@article{medvedev2018fitted, title={Fitted modifications of Runge-Kutta pairs of orders 6 (5)}, author={Medvedev, Maxim A and Simos, TE and Tsitouras, Ch}, journal={Mathematical Methods in the Applied Sciences}, volume={41}, number={16}, pages={6184–6194}, year={2018}, publisher={Wiley Online Library}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.RKM","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.RKM","text":"RKM(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Method designed to have good stability properties when applied to pseudospectral discretizations of hyperbolic partial differential equaitons.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{mead1999optimal,   title={Optimal Runge–Kutta methods for first order pseudospectral operators},   author={Mead, JL and Renaut, RA},   journal={Journal of Computational Physics},   volume={152},   number={1},   pages={404–419},   year={1999},   publisher={Elsevier} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.MSRK5","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.MSRK5","text":"MSRK5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  5th order method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nMisha Stepanov - https://arxiv.org/pdf/2202.08443.pdf : Figure 3.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.MSRK6","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.MSRK6","text":"MSRK6(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  6th order method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nMisha Stepanov - https://arxiv.org/pdf/2202.08443.pdf : Table4\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.PSRK4p7q6","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.PSRK4p7q6","text":"PSRK4p7q6(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  6-stage Pseudo-Symplectic method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Aubry1998,     author = {A. Aubry and P. Chartier},     journal = {BIT Numer. Math.},     title =  {Pseudo-symplectic {R}unge-{K}utta methods},     volume = {38},     PAGES = {439-461},     year = {1998},     },     @article{Capuano2017,     title = {Explicit {R}unge–{K}utta schemes for incompressible flow with improved energy-conservation properties},     journal = {J. Comput. Phys.},     volume = {328},     pages = {86-94},     year = {2017},     issn = {0021-9991},     doi = {https://doi.org/10.1016/j.jcp.2016.10.040},     author = {F. Capuano and G. Coppola and L. Rández and L. {de Luca}},}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.PSRK3p5q4","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.PSRK3p5q4","text":"PSRK3p5q4(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  4-stage Pseudo-Symplectic method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Aubry1998,     author = {A. Aubry and P. Chartier},     journal = {BIT Numer. Math.},     title =  {Pseudo-symplectic {R}unge-{K}utta methods},     year = {1998},     },     @article{Capuano2017,     title = {Explicit {R}unge–{K}utta schemes for incompressible flow with improved energy-conservation properties},     journal = {J. Comput. Phys.},     year = {2017},     author = {F. Capuano and G. Coppola and L. Rández and L. {de Luca}},}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.PSRK3p6q5","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.PSRK3p6q5","text":"PSRK3p6q5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  5-stage Pseudo-Symplectic method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Aubry1998,     author = {A. Aubry and P. Chartier},     journal = {BIT Numer. Math.},     title =  {Pseudo-symplectic {R}unge-{K}utta methods},     year = {1998},     },     @article{Capuano2017,     title = {Explicit {R}unge–{K}utta schemes for incompressible flow with improved energy-conservation properties},     journal = {J. Comput. Phys.},     year = {2017},     author = {F. Capuano and G. Coppola and L. Rández and L. {de Luca}},}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Stepanov5","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Stepanov5","text":"Stepanov5(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  5th order method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Stepanov2021Embedded5,     title={Embedded (4, 5) pairs of explicit 7-stage Runge–Kutta methods with FSAL property},     author={Misha Stepanov},     journal={Calcolo},     year={2021},     volume={59}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.SIR54","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.SIR54","text":"SIR54(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  5th order method suited for SIR-type epidemic models.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Kovalnogov2020RungeKuttaPS,     title={Runge–Kutta pairs suited for SIR‐type epidemic models},     author={Vladislav N. Kovalnogov and Theodore E. Simos and Ch. Tsitouras},     journal={Mathematical Methods in the Applied Sciences},     year={2020},     volume={44},     pages={5210 - 5216}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Alshina2","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Alshina2","text":"Alshina2(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  2nd order, 2-stage Method with optimal parameters.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Alshina2008,     doi = {10.1134/s0965542508030068},     url = {https://doi.org/10.1134/s0965542508030068},     year = {2008},     month = mar,     publisher = {Pleiades Publishing Ltd},     volume = {48},     number = {3},     pages = {395–405},     author = {E. A. Alshina and E. M. Zaks and N. N. Kalitkin},     title = {Optimal first- to sixth-order accurate Runge-Kutta schemes},     journal = {Computational Mathematics and Mathematical Physics}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Alshina3","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Alshina3","text":"Alshina3(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  3rd order, 3-stage Method with optimal parameters.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Alshina2008,     doi = {10.1134/s0965542508030068},     url = {https://doi.org/10.1134/s0965542508030068},     year = {2008},     month = mar,     publisher = {Pleiades Publishing Ltd},     volume = {48},     number = {3},     pages = {395–405},     author = {E. A. Alshina and E. M. Zaks and N. N. Kalitkin},     title = {Optimal first- to sixth-order accurate Runge-Kutta schemes},     journal = {Computational Mathematics and Mathematical Physics}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowOrderRK/#OrdinaryDiffEqLowOrderRK.Alshina6","page":"OrdinaryDiffEqLowOrderRK","title":"OrdinaryDiffEqLowOrderRK.Alshina6","text":"Alshina6(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  6th order, 7-stage Method with optimal parameters.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{Alshina2008,     doi = {10.1134/s0965542508030068},     url = {https://doi.org/10.1134/s0965542508030068},     year = {2008},     month = mar,     publisher = {Pleiades Publishing Ltd},     volume = {48},     number = {3},     pages = {395–405},     author = {E. A. Alshina and E. M. Zaks and N. N. Kalitkin},     title = {Optimal first- to sixth-order accurate Runge-Kutta schemes},     journal = {Computational Mathematics and Mathematical Physics}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF","text":"BDF (Backward Differentiation Formula) methods for mass matrix differential-algebraic equations (DAEs) and stiff ODEs with singular mass matrices. These methods provide robust, high-order integration for systems with algebraic constraints and mixed differential-algebraic structure.","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Key-Properties","page":"OrdinaryDiffEqBDF","title":"Key Properties","text":"Mass matrix BDF methods provide:\n\nDAE capability for index-1 differential-algebraic equations\nMass matrix support for singular and non-diagonal mass matrices\nHigh-order accuracy up to 5th order with good stability\nL-stable behavior for stiff problems with excellent damping\nAutomatic differentiation for efficient Jacobian computation\nVariable order and stepsize adaptation for efficiency","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#When-to-Use-Mass-Matrix-BDF-Methods","page":"OrdinaryDiffEqBDF","title":"When to Use Mass Matrix BDF Methods","text":"These methods are recommended for:\n\nDifferential-algebraic equations (DAEs) with index-1 structure\nConstrained mechanical systems with holonomic constraints\nElectrical circuit simulation with algebraic loop equations\nChemical reaction networks with conservation constraints\nMultibody dynamics with kinematic constraints\nSemi-explicit DAEs arising from spatial discretizations","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Mathematical-Background","page":"OrdinaryDiffEqBDF","title":"Mathematical Background","text":"Mass matrix DAEs have the form: M du/dt = f(u,t)\n\nwhere M is a potentially singular mass matrix. When M is singular, some equations become algebraic constraints rather than differential equations, leading to a DAE system.","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Problem-Formulation","page":"OrdinaryDiffEqBDF","title":"Problem Formulation","text":"Use ODEFunction with a mass_matrix:\n\nusing LinearAlgebra: Diagonal\nfunction rober(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁ * y₁ + k₃ * y₂ * y₃\n    du[2] = k₁ * y₁ - k₃ * y₂ * y₃ - k₂ * y₂^2\n    du[3] = y₁ + y₂ + y₃ - 1\n    nothing\nend\nM = Diagonal([1.0, 1.0, 0])  # Singular mass matrix\nf = ODEFunction(rober, mass_matrix = M)\nprob_mm = ODEProblem(f, [1.0, 0.0, 0.0], (0.0, 1e5), (0.04, 3e7, 1e4))\nsol = solve(prob_mm, FBDF(), reltol = 1e-8, abstol = 1e-8)","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Solver-Selection-Guide","page":"OrdinaryDiffEqBDF","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Recommended-Methods","page":"OrdinaryDiffEqBDF","title":"Recommended Methods","text":"FBDF: Recommended - Fixed leading coefficient BDF with excellent stability\nQNDF: Quasi-constant stepsize Nordsieck BDF with good efficiency\nQBDF: Alternative quasi-constant stepsize BDF formulation","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Specific-order-methods","page":"OrdinaryDiffEqBDF","title":"Specific order methods","text":"QNDF1: First-order method for simple problems\nQNDF2: Second-order method balancing accuracy and stability\nQBDF1, QBDF2: Alternative second-order formulations\nABDF2: Adams-type BDF for specific applications\nMEBDF2: Modified extended BDF for enhanced stability","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Performance-Guidelines","page":"OrdinaryDiffEqBDF","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#When-mass-matrix-BDF-methods-excel","page":"OrdinaryDiffEqBDF","title":"When mass matrix BDF methods excel","text":"Index-1 DAE systems with well-separated differential and algebraic variables\nLarge stiff systems with algebraic constraints\nProblems with conservation laws naturally expressed as constraints\nMultiphysics simulations combining differential and algebraic equations\nSystems where constraints are essential to the physics","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Mass-matrix-considerations","page":"OrdinaryDiffEqBDF","title":"Mass matrix considerations","text":"Singular mass matrices require consistent initial conditions\nIndex determination affects solver performance and stability\nConstraint violations may accumulate and require projection\nWell-conditioned problems generally perform better","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Important-Considerations","page":"OrdinaryDiffEqBDF","title":"Important Considerations","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Initial-conditions","page":"OrdinaryDiffEqBDF","title":"Initial conditions","text":"Must be consistent with algebraic constraints\nUse initialization procedures if constraints are not satisfied initially\nIndex-1 assumption requires that constraints uniquely determine algebraic variables","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Numerical-challenges","page":"OrdinaryDiffEqBDF","title":"Numerical challenges","text":"Constraint drift may occur over long integrations\nIndex higher than 1 not directly supported\nIll-conditioned mass matrices can cause numerical difficulties\nDiscontinuities in constraints require special handling","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Alternative-Approaches","page":"OrdinaryDiffEqBDF","title":"Alternative Approaches","text":"Consider these alternatives:\n\nImplicit Runge-Kutta methods for higher accuracy requirements\nRosenbrock methods for moderately stiff DAEs\nProjection methods for constraint preservation\nIndex reduction techniques for higher-index DAEs\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqBDF\", \"FBDF\")","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#Full-list-of-solvers","page":"OrdinaryDiffEqBDF","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.ABDF2-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.ABDF2","text":"ABDF2(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        κ = nothing,\n        tol = nothing,\n        nlsolve = NLNewton(),\n        smooth_est = true,\n        extrapolant = :linear,\n        controller = :Standard,\n        step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ABDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nE. Alberdi Celayaa, J. J. Anza Aguirrezabalab, P. Chatzipantelidisc. Implementation of an Adaptive BDF2 Formula and Comparison with The MATLAB Ode15s. Procedia Computer Science, 29, pp 1014-1026, 2014. doi: https://doi.org/10.1016/j.procs.2014.05.091\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.QNDF-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QNDF","text":"QNDF(; chunk_size = Val{0}(),\n       autodiff = AutoForwardDiff(),\n       standardtag = Val{true}(),\n       concrete_jac = nothing,\n       linsolve = nothing,\n       precs = DEFAULT_PRECS,\n       κ = nothing,\n       tol = nothing,\n       nlsolve = NLNewton(),\n       extrapolant = :linear,\n       kappa =  promote(-0.1850, -1 // 9, -0.0823, -0.0415, 0),\n       controller = :Standard,\n       step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order quasi-constant timestep NDF method. Similar to MATLAB's ode15s. Uses Shampine's accuracy-optimal coefficients. Performance improves with larger, more complex ODEs. Good for medium to highly stiff problems. Recommended for large systems (>1000 ODEs).\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify QNDF(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nextrapolant: TBD\nkappa: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{shampine1997matlab, title={The matlab ode suite}, author={Shampine, Lawrence F and Reichelt, Mark W}, journal={SIAM journal on scientific computing}, volume={18}, number={1}, pages={1–22}, year={1997}, publisher={SIAM} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.QNDF1-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QNDF1","text":"QNDF1(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        nlsolve = NLNewton(),\n        extrapolant = :linear,\n        kappa = -0.1850,\n        controller = :Standard,\n        step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify QNDF1(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\nkappa: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{shampine1997matlab, title={The matlab ode suite}, author={Shampine, Lawrence F and Reichelt, Mark W}, journal={SIAM journal on scientific computing}, volume={18}, number={1}, pages={1–22}, year={1997}, publisher={SIAM} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.QNDF2-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QNDF2","text":"QNDF2(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        nlsolve = NLNewton(),\n        extrapolant = :linear,\n        kappa =  -1 // 9,\n        controller = :Standard,\n        step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify QNDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\nkappa: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{shampine1997matlab, title={The matlab ode suite}, author={Shampine, Lawrence F and Reichelt, Mark W}, journal={SIAM journal on scientific computing}, volume={18}, number={1}, pages={1–22}, year={1997}, publisher={SIAM} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.QBDF-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QBDF","text":"QBDF: Multistep Method\n\nAn alias of QNDF with κ=0.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.QBDF1-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QBDF1","text":"QBDF1: Multistep Method\n\nAn alias of QNDF1 with κ=0.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.QBDF2-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QBDF2","text":"QBDF2: Multistep Method\n\nAn alias of QNDF2 with κ=0.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.MEBDF2-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.MEBDF2","text":"MEBDF2(; chunk_size = Val{0}(),\n         autodiff = AutoForwardDiff(),\n         standardtag = Val{true}(),\n         concrete_jac = nothing,\n         linsolve = nothing,\n         precs = DEFAULT_PRECS,\n         nlsolve = NLNewton(),\n         extrapolant = :constant)\n\nMultistep Method. The second order Modified Extended BDF method,     which has improved stability properties over the standard BDF.     Fixed timestep only.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify MEBDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{cash2000modified, title={Modified extended backward differentiation formulae for the numerical solution of stiff initial value problems in ODEs and DAEs}, author={Cash, JR}, journal={Journal of Computational and Applied Mathematics}, volume={125}, number={1-2}, pages={117–130}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/massmatrixdae/BDF/#OrdinaryDiffEqBDF.FBDF-api-ordinarydiffeq-massmatrixdae-BDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.FBDF","text":"FBDF(; chunk_size = Val{0}(),\n       autodiff = AutoForwardDiff(),\n       standardtag = Val{true}(),\n       concrete_jac = nothing,\n       linsolve = nothing,\n       precs = DEFAULT_PRECS,\n       κ = nothing,\n       tol = nothing,\n       nlsolve = NLNewton(),\n       extrapolant = :linear,\n       controller = :Standard,\n       step_limiter! = trivial_limiter!,\n       max_order::Val{MO} = Val{5}())\n\nMultistep Method. An adaptive order quasi-constant timestep NDF method. Fixed leading coefficient BDF. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients).\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify FBDF(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nmax_order: TBD\n\nReferences\n\n@article{shampine2002solving, title={Solving 0= F (t, y (t), y′(t)) in Matlab}, author={Shampine, Lawrence F}, year={2002}, publisher={Walter de Gruyter GmbH \\& Co. KG}}\n\n\n\n\n\n","category":"type"},{"location":"types/dde_types/#dde_prob","page":"DDE Problems","title":"DDE Problems","text":"","category":"section"},{"location":"types/dde_types/#Solution-Type","page":"DDE Problems","title":"Solution Type","text":"DDEProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"section"},{"location":"types/dde_types/#Alias-Specifier","page":"DDE Problems","title":"Alias Specifier","text":"","category":"section"},{"location":"types/dde_types/#Example-Problems","page":"DDE Problems","title":"Example Problems","text":"Example problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_dde_constant_1delay_ip, you can do something like:\n\n#] add DiffEqProblemLibrary\nimport DiffEqProblemLibrary.DDEProblemLibrary\nimport DelayDiffEq as DDE, DifferentialEquations as DE\n# load problems\nprob = DDEProblemLibrary.prob_dde_constant_1delay_ip\nsol = solve(prob, DDE.MethodOfSteps(DE.Tsit5()))","category":"section"},{"location":"types/dde_types/#DDEs-with-1-constant-delay","page":"DDE Problems","title":"DDEs with 1 constant delay","text":"","category":"section"},{"location":"types/dde_types/#DDEs-with-2-constant-delays","page":"DDE Problems","title":"DDEs with 2 constant delays","text":"","category":"section"},{"location":"types/dde_types/#DDETest-Problems","page":"DDE Problems","title":"DDETest Problems","text":"Some details:\n\n# DDEs with time dependent delays\nprob_dde_DDETST_A1, prob_dde_DDETST_A2,\n# DDEs with vanishing time dependent delays\nprob_dde_DDETST_B1, prob_dde_DDETST_B2,\n# DDEs with state dependent delays\nprob_dde_DDETST_C1, prob_dde_DDETST_C2, prob_dde_DDETST_C3, prob_dde_DDETST_C4,\n# DDEs with vanishing state dependent delays\nprob_dde_DDETST_D1, prob_dde_DDETST_D2,\n# neutral DDEs with time dependent delays\nprob_dde_DDETST_E1, prob_dde_DDETST_E2,\n# neutral DDEs with vanishing time dependent delays\nprob_dde_DDETST_F1, prob_dde_DDETST_F2, prob_dde_DDETST_F3, prob_dde_DDETST_F4, prob_dde_DDETST_F5,\n# neutral DDEs with state dependent delays\nprob_dde_DDETST_G1, prob_dde_DDETST_G2,\n# neutral DDEs with vanishing state dependent delays\nprob_dde_DDETST_H1, prob_dde_DDETST_H2, prob_dde_DDETST_H3, prob_dde_DDETST_H4","category":"section"},{"location":"types/dde_types/#Radar5-Test-Problems","page":"DDE Problems","title":"Radar5 Test Problems","text":"","category":"section"},{"location":"types/dde_types/#QS-Example","page":"DDE Problems","title":"QS Example","text":"","category":"section"},{"location":"types/dde_types/#SciMLBase.DDEProblem","page":"DDE Problems","title":"SciMLBase.DDEProblem","text":"Defines a delay differential equation (DDE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/dde_types/\n\nMathematical Specification of a DDE Problem\n\nTo define a DDE Problem, you simply need to give the function f, the initial condition u_0 at time point t_0, and the history function h which together define a DDE:\n\nbeginalign*\nfracdudt = f(uhpt)  (t geq t_0) \nu(t_0) = u_0 \nu(t)   = h(t)  (t  t_0)\nendalign*\n\nf should be specified as f(u, h, p, t) (or in-place as f(du, u, h, p, t)), u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u, and h should be specified as described below. The history function h is accessed for all delayed values. Note that we are not limited to numbers or vectors for u_0; one is allowed to provide u_0 as arbitrary matrices / higher dimension tensors as well.\n\nFunctional Forms of the History Function\n\nThe history function h can be called in the following ways:\n\nh(p, t): out-of-place calculation\nh(out, p, t): in-place calculation\nh(p, t, deriv::Type{Val{i}}): out-of-place calculation of the ith derivative\nh(out, p, t, deriv::Type{Val{i}}): in-place calculation of the ith derivative\nh(args...; idxs): calculation of h(args...) for indices idxs\n\nNote that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function f.\n\nDeclaring Lags\n\nLags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can be more efficient and accurate, and this is thus recommended.\n\nNeutral and Retarded Delay Differential Equations\n\nNote that the history function specification can be used to specify general retarded arguments, i.e. h(p,α(u,t)). Neutral delay differential equations can be specified by using the deriv value in the history interpolation. For example, h(p,t-τ, Val{1}) returns the first derivative of the history values at time t-τ.\n\nNote that algebraic equations can be specified by using a singular mass matrix.\n\nProblem Type\n\nConstructors\n\nDDEProblem(f[, u0], h, tspan[, p]; <keyword arguments>)\nDDEProblem{isinplace,specialize}(f[, u0], h, tspan[, p]; <keyword arguments>)\n\nisinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nArguments\n\nf: The function in the DDE.\nu0: The initial condition. Defaults to the value h(p, first(tspan)) of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise, it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\nDynamical Delay Differential Equations\n\nMuch like Dynamical ODEs, a Dynamical DDE is a Partitioned DDE of the form:\n\nbeginalign*\nfracdvdt = f_1(uth) \nfracdudt = f_2(vh) \nendalign*\n\nConstructors\n\nDynamicalDDEProblem(f1, f2[, v0, u0], h, tspan[, p]; <keyword arguments>)\nDynamicalDDEProblem{isinplace}(f1, f2[, v0, u0], h, tspan[, p]; <keyword arguments>)\n\nParameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nArguments\n\nf: The function in the DDE.\nv0 and u0: The initial condition. Defaults to the values h(p, first(tspan))... of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0. Must return an object with the indices 1 and 2, with the values of v and u respectively.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (v, u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise, it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\nFor dynamical and second order DDEs, the history function will return an object with the indices 1 and 2 defined, where h(p, t_prev)[1] is the value of f_2(v u h p t_mathrmprev) and h(p, t_prev)[2] is the value of f_1(v u h p t_mathrmprev) (this is for consistency with the ordering of the initial conditions in the constructor). The supplied history function must also return such a 2-index object, which can be accomplished with a tuple (v,u) or vector [v,u].\n\n2nd Order Delay Differential Equations\n\nTo define a 2nd Order DDE Problem, you simply need to give the function f and the initial condition u_0 which define an DDE:\n\nu = f(uuhpt)\n\nf should be specified as f(du,u,p,t) (or in-place as f(ddu,du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nFrom this form, a dynamical ODE:\n\nbeginalign*\nv = f(vuhpt) \nu = v\nendalign*\n\nConstructors\n\nSecondOrderDDEProblem(f[, du0, u0], h, tspan[, p]; <keyword arguments>)\nSecondOrderDDEProblem{isinplace}(f, [, du0, u0], h, tspan[, p]; <keyword arguments>)\n\nParameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.\n\nArguments\n\nf: The function in the DDE.\ndu0 and u0: The initial condition. Defaults to the values h(p, first(tspan))... of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0. Must return an object with the indices 1 and 2, with the values of v and u respectively.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (v, u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise, it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.\n\nAs above, the history function will return an object with indices 1 and 2, with the values of du and u respectively. The supplied history function must also match this return type, e.g. by returning a 2-element tuple or vector.\n\nExample Problems\n\nExample problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_dde_constant_1delay_ip, you can do something like:\n\n#] add DDEProblemLibrary\nusing DDEProblemLibrary\nprob = DDEProblemLibrary.prob_dde_constant_1delay_ip\nsol = solve(prob)\n\n\n\n\n\n","category":"type"},{"location":"types/dde_types/#SciMLBase.DDEFunction","page":"DDE Problems","title":"SciMLBase.DDEFunction","text":"struct DDEFunction{iip, specialize, F, TMM, Ta, Tt, TJ, JVP, VJP, JP, SP, TW, TWt, TPJ, O, TCV, SYS, ID} <: SciMLBase.AbstractDDEFunction{iip}\n\nA representation of a DDE function f, defined by:\n\nM fracdudt = f(uhpt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDDEFunction{iip,specialize}(f;\n                 mass_matrix = __has_mass_matrix(f) ? f.mass_matrix : I,\n                 analytic = __has_analytic(f) ? f.analytic : nothing,\n                 tgrad= __has_tgrad(f) ? f.tgrad : nothing,\n                 jac = __has_jac(f) ? f.jac : nothing,\n                 jvp = __has_jvp(f) ? f.jvp : nothing,\n                 vjp = __has_vjp(f) ? f.vjp : nothing,\n                 jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                 sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                 paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n                 colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n                 sys = __has_sys(f) ? f.sys : nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,h,p,t) or du = f(u,h,p,t). See the section on iip for more details on in-place vs out-of-place handling. The history function h acts as an interpolator over time, i.e. h(t) with options matching the solution interface, i.e. h(t; save_idxs = 2).\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://docs.sciml.ai/DiffEqDocs/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,h,p,t) or dT=tgrad(u,p,t): returns fracf(upt)t\njac(J,u,h,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,h,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivative fracdfdu v\nvjp(Jv,v,h,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivative fracdfdu^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,h,u,p,t): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DDEFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"types/dde_types/#SciMLBase.DDEAliasSpecifier","page":"DDE Problems","title":"SciMLBase.DDEAliasSpecifier","text":"DDEAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = nothing, alias_du0 = nothing, alias_tstops = nothing, alias = nothing)\n\nHolds information on what variables to alias when solving a DDE. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false .\nalias_du0::Union{Bool, Nothing}: alias the du0 array for DAEs. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias::Union{Bool, Nothing}: sets all fields of the DDEAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_1delay_ip","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_1delay_ip","text":"prob_dde_constant_1delay_ip\n\nDelay differential equation\n\nu(t) = -u(t - 1)\n\nfor t in 0 1 with history function ϕ(t) = 0 if t  0 and ϕ(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 10 can be obtained by the method of steps and is provided in this implementation.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_1delay_oop","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_1delay_oop","text":"prob_dde_constant_1delay_oop\n\nSame delay differential equation as prob_dde_constant_1delay_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_1delay_scalar","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_1delay_scalar","text":"prob_dde_constant_1delay_scalar\n\nSame delay differential equation as prob_dde_constant_1delay_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_1delay_long_ip","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_1delay_long_ip","text":"prob_dde_constant_1delay_long_ip\n\nDelay differential equation\n\nu(t) = u(t) - u(t - 15)\n\nfor t in 0 100 with history function ϕ(t) = 0 if t  0 and ϕ(0) = 1.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_1delay_long_oop","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_1delay_long_oop","text":"prob_dde_constant_1delay_long_oop\n\nSame delay differential equation as prob_dde_constant_1delay_long_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_1delay_long_scalar","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_1delay_long_scalar","text":"prob_dde_constant_1delay_long_scalar\n\nSame delay differential equation as prob_dde_constant_1delay_long_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_2delays_ip","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_2delays_ip","text":"prob_dde_constant_2delays_ip\n\nDelay differential equation\n\nu(t) = -u(t - 13) - u(t - 15)\n\nfor t in 0 1 with history function ϕ(t) = 0 if t  0 and ϕ(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 10 can be obtained by the method of steps and is provided in this implementation.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_2delays_oop","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_2delays_oop","text":"prob_dde_constant_2delays_oop\n\nSame delay differential equation as prob_dde_constant_2delays_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_2delays_scalar","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_2delays_scalar","text":"prob_dde_constant_2delays_scalar\n\nSame delay differential equation as prob_dde_constant_2delays_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_2delays_long_ip","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_2delays_long_ip","text":"prob_dde_constant_2delays_long_ip\n\nDelay differential equation\n\nu(t) = - u(t - 13) - u(t - 15)\n\nfor t in 0 100 with history function ϕ(t) = 0 if t  0 and ϕ(0) = 1.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_2delays_long_oop","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_2delays_long_oop","text":"prob_dde_constant_2delays_long_oop\n\nSame delay differential equation as prob_dde_constant_2delays_long_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_constant_2delays_long_scalar","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_constant_2delays_long_scalar","text":"prob_dde_constant_2delays_long_scalar\n\nSame delay differential equation as prob_dde_constant_2delays_long_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_A1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_A1","text":"prob_dde_DDETST_A1\n\nDelay differential equation model of blood production, given by\n\nu(t) = frac02 u(t - 14)1 + u(t - 14)^10 - 01 u(t)\n\nfor t in 0 500 and history function ϕ(t) = 05 for t  0.\n\nReferences\n\nMackey, M. C. and Glass, L. (1977). Oscillation and chaos in physiological control systems, Science (197), pp. 287-289.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_A2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_A2","text":"prob_dde_DDETST_A2\n\nDelay differential equation model of chronic granulocytic leukemia, given by\n\nbeginalign*\nu_1(t) = frac111 + sqrt10 u_1(t - 20)^54 - frac10 u_1(t)1 + 40 u_2(t) \nu_2(t) = frac100 u_1(t)1 + 40 u_2(t) - 243 u_2(t)\nendalign*\n\nfor t in 0 100 and history function\n\nbeginalign*\nϕ_1(t) = 1057670273 \nϕ_2(t) = 10307134913\nendalign*\n\nfor t  0.\n\nReferences\n\nWheldon, T., Kirk, J. and Finlay, H. (1974). Cyclical granulopoiesis in chronic granulocytic leukemia: A simulation study., Blood (43), pp. 379-387.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_B1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_B1","text":"prob_dde_DDETST_B1\n\nDelay differential equation\n\nu(t) = 1 - u(exp(1 - 1t))\n\nfor t in 01 10 with history function ϕ(t) = log t for t in (0 01.\n\nSolution\n\nThe analytical solution for t in 01 10 is\n\nu(t) = log t\n\nReferences\n\nNeves, K. W. (1975). Automatic integration of functional differential equations: An approach, ACM Trans. Math. Soft. (1), pp. 357-368.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_B2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_B2","text":"prob_dde_DDETST_B2\n\nDelay differential equation\n\nu(t) = - 1 - u(t) + 2 u(t  2)  0\n\nfor t in 0 2 log 66 with history function ϕ(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 2 log 66 is\n\nu(t) = begincases\n  2 exp(-t) - 1  textif  t in 0 2 log 2 \n  1 - 6 exp(-t)  textif   t in (2 log 2 2 log 6 \n  66 exp(-t) - 1  textif  t in (2 log 6 2 log 66\nendcases\n\nReferences\n\nNeves, K. W. and Thompson, S. (1992). Solution of systems of functional differential equations with state dependent delays, Technical Report TR-92-009, Computer Science, Radford University.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_C1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_C1","text":"prob_dde_DDETST_C1\n\nDelay differential equation\n\nu(t) = - 2 u(t - 1 - u(t)) (1 - u(t)^2)\n\nfor t in 0 30 with history function ϕ(t) = 05 for t  0.\n\nReferences\n\nPaul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_C2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_C2","text":"prob_dde_DDETST_C2\n\nDelay differential equation\n\nbeginalign*\nu_1(t) = - 2 u_1(t - u_2(t)) \nu_₂(t) = fracu_1(t - u_2(t)) - u_1(t)1 + u_1(t - u_2(t))\nendalign*\n\nfor t in 0 40 with history function\n\nbeginalign*\nϕ_1(t) = 1 \nϕ_2(t) = 05\nendalign*\n\nfor t  0.\n\nReferences\n\nPaul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_C3","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_C3","text":"prob_dde_DDETST_C3\n\nDelay differential equation model of hematopoiesis, given by\n\nbeginalign*\nu_1(t) = hats_0 u_2(t - T_1) - γ u_1(t) - Q \nu_2(t) = f(u_1(t)) - k u_2(t) \nu_3(t) = 1 - fracQ exp(γ u_3(t))hats_0 u_2(t - T_1 - u_3(t))\nendalign*\n\nfor t in 0 300 with history function ϕ_1(0) = 3325, ϕ_3(0) = 120, and\n\nϕ_2(t) = begincases\n  10   textif  t in - T_1 0\n  95  textif  t  - T_1\nendcases\n\nwhere f(y) = a  (1 + K y^r), hats_0 = 00031, T_1 = 6, γ = 0001, Q = 00275, k = 28, a = 6570, K = 00382, and r = 696.\n\nReferences\n\nMahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_C4","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_C4","text":"prob_dde_DDETST_C4\n\nDelay differential equation model of hematopoiesis, given by the same delay differential equation as prob_dde_DDETST_C3\n\nbeginalign*\nu_1(t) = hats_0 u_2(t - T_1) - γ u_1(t) - Q \nu_2(t) = f(u_1(t)) - k u_2(t) \nu_3(t) = 1 - fracQ exp(γ u_3(t))hats_0 u_2(t - T_1 - u_3(t))\nendalign*\n\nfor t in 0 100 with history function ϕ_1(0) = 35, ϕ_3(0) = 50, and ϕ_2(t) = 10 for t  0, where f(y) = a  (1 + K y^r), hats_0 = 000372, T_1 = 3, γ = 01, Q = 000178, k = 665, a = 15600, K = 00382, and r = 696.\n\nReferences\n\nMahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_D1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_D1","text":"prob_dde_DDETST_D1\n\nDelay differential equation\n\nbeginalign*\nu_1(t) = u_2(t) \nu_2(t) = - u_2(exp(1 - u_2(t))) u_2(t)^2 exp(1 - u_2(t))\nendalign*\n\nfor t in 01 5 with history function\n\nbeginalign*\nϕ_1(t) = log t \nϕ_2(t) = 1  t\nendalign*\n\nfor t in (0 01.\n\nSolution\n\nThe analytical solution for t in 01 5 is\n\nbeginalign*\nu_1(t) = log t \nu_2(t) = 1  t\nendalign*\n\nReferences\n\nNeves, K. W. (1975). Automatic integration of functional differential equations: An approach, ACM Trans. Math. Soft. (1), pp. 357-368.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_D2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_D2","text":"prob_dde_DDETST_D2\n\nDelay differential equation model of antigen antibody dynamics with fading memory, given by\n\nbeginalign*\nu_1(t) = - r_1 u_1(t) u_2(t) + r_2 u_3(t) \nu_2(t) = - r_1 u_1(t) u_2(t) + α r_1 u_1(t - u_4(t)) u_2(t - u_4(t)) \nu_3(t) =   r_1 u_1(t) u_2(t) - r_2 u_3(t) \nu_4(t) = 1 + frac3δ - u_1(t) u_2(t) - u_3(t)u_1(t - u_4(t)) u_2(t - u_4(t)) + u_3(t - u_4(t)) exp(δ u_4(t))\nendalign*\n\nfor t in 0 40 with history function\n\nbeginalign*\nϕ_1(t) = 5 \nϕ_2(t) = 01 \nϕ_3(t) = 0 \nϕ_4(t) = 0\nendalign*\n\nfor t  0, where r_1 = 002, r_2 = 0005, α = 3, and δ = 001.\n\nReferences\n\nGatica, J. and Waltman, P. (1982). A threshold model of antigen antibody dynamics with fading memory, in Lakshmikantham (ed.), Nonlinear phenomena in mathematical science, Academic Press, New York, pp. 425-439.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_E1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_E1","text":"prob_dde_DDETST_E1\n\nDelay differential equation model of a food-limited population, given by\n\nu(t) = r u(t) (1 - u(t - 1) - c u(t - 1))\n\nfor t in 0 40 with history function ϕ(t) = 2 + t for t  0, where r = π  sqrt3 + 120 and c = sqrt3  (2π) - 1  25.\n\nReferences\n\nKuang, Y. and Feldstein, A. (1991). Boundedness of solutions of a nonlinear nonautonomous neutral delay equation, J. Math. Anal. Appl. (156), pp. 293-304.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_E2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_E2","text":"prob_dde_DDETST_E2\n\nDelay differential equation model of a logistic Gauss-type predator-prey system, given by\n\nbeginalign*\nu_1(t) = u_1(t) (1 - u_1(t - τ) - ρ u_1(t - τ)) - fracu_2(t) u_1(t)^2u_1(t)^2 + 1 \nu_2(t) = u_2(t) left(fracu_1(t)^2u_1(t)^2 + 1 - αright)\nendalign*\n\nfor t in 0 2 with history function\n\nbeginalign*\nϕ_1(t) = 033 - t  10 \nϕ_2(t) = 222 + t  10\nendalign*\n\nfor t  0, where α = 01, ρ = 29, and τ = 042.\n\nReferences\n\nKuang, Y. (1991). On neutral delay logistics Gauss-type predator-prey systems, Dyn. Stab. Systems (6), pp. 173-189.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_F1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_F1","text":"prob_dde_DDETST_F1\n\nDelay differential equation\n\nu(t) = 2 cos(2t) u(t  2)^2 cos t + log(u(t  2)) - log(2 cos t) - sin t\n\nfor t in 0 1 with history function ϕ(0) = 1 and ϕ(0) = 2.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = exp(sin(2t))\n\nReferences\n\nJackiewicz, Z. (1981). One step methods for the numerical solution of Volterra functional differential equations of neutral type, Applicable Anal. (12), pp. 1-11.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_F2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_F2","text":"prob_dde_DDETST_F2\n\nDelay differential equation\n\nu(t) = u(2t - 05)\n\nfor t in 025 0499 with history function ϕ(t) = exp(-t^2) and ϕ(t) = -2t exp(-t^2) for t  025.\n\nSolution\n\nThe analytical solution for t in 025 0499 is\n\nu(t) = u_i(t) = exp(-4^i t^2 + B_i t + C_i)  2^i + K_i\n\nif t in x_i x_i + 1, where\n\nbeginalign*\nx_i = (1 - 2^-i)  2 \nB_i = 2 (4^i-1 + B_i-1) \nC_i = - 4^i-2 - B_i-1  2 + C_i-1 \nK_i = - exp(-4^i x_i^2 + B_i x_i + C_i)  2^i + u_i-1(x_i)\nendalign*\n\nand B_0 = C_0 = K_0 = 0.\n\nReferences\n\nNeves, K. W. and Thompson, S. (1992). Solution of systems of functional differential equations with state dependent delays, Technical Report TR-92-009, Computer Science, Radford University.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_F3","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_F3","text":"prob_dde_DDETST_F3\n\nDelay differential equation\n\nu(t) = exp(-u(t)) + L_3 leftsin(u(α(t))) - sinleft(frac13 + α(t)right)right\n\nfor t in 0 10 with history function ϕ(0) = log 3 and ϕ(0) = 1  3, where α(t) = 05 t (1 - cos(2πt)) and L_3 = 02.\n\nSolution\n\nThe analytical solution for t in 0 10 is\n\nu(t) = log(t + 3)\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_F4","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_F4","text":"prob_dde_DDETST_F4\n\nSame delay differential equation as prob_dde_DDETST_F3 with L_3 = 04.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_F5","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_F5","text":"prob_dde_DDETST_F5\n\nSame delay differential equation as prob_dde_DDETST_F3 with L_3 = 06.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_G1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_G1","text":"prob_dde_DDETST_G1\n\nDelay differential equation\n\nu(t) = - u(t - u(t)^2  4)\n\nfor t in 0 1 with history function ϕ(t) = 1 - t for t  0 and ϕ(t) = -1 for t  0.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = t + 1\n\nReferences\n\nEl'sgol'ts, L. E. and Norkin, S. B. (1973). Introduction to the Theory and Application of Differential Equations with Deviating Arguments, Academic Press, New York, p. 44.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_G2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_G2","text":"prob_dde_DDETST_G2\n\nDelay differential equation\n\nu(t) = - u(u(t) - 2)\n\nfor t in 0 1 with history function ϕ(t) = 1 - t for t  0 and ϕ(t) = -1 for t  0.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = t + 1\n\nEl'sgol'ts, L. E. and Norkin, S. B. (1973). Introduction to the Theory and Application of Differential Equations with Deviating Arguments, Academic Press, New York, pp. 44-45.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_H1","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_H1","text":"prob_dde_DDETST_H1\n\nDelay differential equation\n\nu(t) = - frac4 t u(t)^24 + log(cos(2t))^2 + tan(2t) + 05 arctanleft(uleft(fract u(t)^21 + u(t)^2right)right)\n\nfor t in 0 0225 π with history function ϕ(0) = 0 and ϕ(0) = 0.\n\nSolution\n\nThe analytical solution for t in 0 0225 π is\n\nu(t) = - log(cos(2t))  2\n\nReferences\n\nCastleton, R. N. and Grimm, L. J. (1973). A first order method for differential equations of neutral type, Math. Comput. (27), pp. 571-577.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_H2","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_H2","text":"prob_dde_DDETST_H2\n\nDelay differential equation\n\nu(t) = cos(t) (1 + u(t u(t)^2)) + L_3 u(t) u(t u(t)^2) + (1 - L_3) sin(t) cos(t sin(t)^2) - sin(t + t sin(t)^2)\n\nfor t in 0 π with history function ϕ(0) = 0 and ϕ(0) = 1, where L_3 = 01.\n\nSolution\n\nThe analytical solution for t in 0 π is\n\nu(t) = sin(t)\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_H3","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_H3","text":"prob_dde_DDETST_H3\n\nSame delay differential equation as prob_dde_DDETST_H2 with L_3 = 03.\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_DDETST_H4","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_DDETST_H4","text":"prob_dde_DDETST_H4\n\nSame delay differential equation as prob_dde_DDETST_H2 with L_3 = 05.\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_RADAR5_oregonator","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_RADAR5_oregonator","text":"prob_dde_RADAR5_oregonator\n\nDelay differential equation model from chemical kinetics, given by\n\nbeginalign*\n  u_1(t) = - k_1 A u_2(t) - k_2 u_1(t) u_2(t - τ) + k_3 B u_1(t) - 2 k_4 u_1(t)^2 \n  u_2(t) = - k_1 A u_2(t) - k_2 u_1(t) u_2(t - τ) + f k_3 B u_1(t)\nendalign*\n\nfor t in 0 1005 with history function\n\nbeginalign*\n  ϕ_1(t) = 10^-10 \n  ϕ_2(t) = 10^-5\nendalign*\n\nfor t  0, where k_1 = 134, k_2 = 1610^9, k_3 = 8000, k_4 = 410^7, k_5 = 1, f = 1, A = 006, B = 006, and τ = 015.\n\nReferences\n\nEpstein, I. and Luo, Y. (1991). Differential delay equations in chemical kinetics. Nonlinear models, Journal of Chemical Physics (95), pp. 244-254.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_RADAR5_robertson","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_RADAR5_robertson","text":"prob_dde_RADAR5_robertson\n\nDelay differential equation model of a chemical reaction with steady state solution, given by\n\nbeginalign*\n  u_1(t) = - a u_1(t) + b u_2(t - τ) u_3(t) \n  u_2(t) =   a u_1(t) - b u_2(t - τ) u_3(t) - c u_2(t)^2 \n  u_3(t) = c u_2(t)^2\nendalign*\n\nfor t in 0 10^10 with history function ϕ_1(0) = 1, ϕ_2(t) = 0 for t in -τ 0, and ϕ_3(0) = 0, where a = 004, b = 10_000, c = 310^7, and τ = 001.\n\nReferences\n\nGuglielmi, N. and Hairer, E. (2001). Implementing Radau IIA methods for stiff delay differential equations, Computing (67), pp. 1-12.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_RADAR5_waltman","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_RADAR5_waltman","text":"prob_dde_RADAR5_waltman\n\nDelay differential equation model of antibody production, given by\n\nbeginalign*\n  u_1(t) = - r u_1(t) u_2(t) - s u_1(t) u_4(t) \n  u_2(t) = - r u_1(t) u_2(t) + α r u_1(u_5(t)) u_2(u_5(t))  t  t_0 \n  u_3(t) = r u_1(t) u_2(t) \n  u_4(t) = - s u_1(t) u_4(t) - γ u_4(t) + β r u_1(u_6(t)) u_2(u_6(t))  t  t_1 \n  u_5(t) = t  t_0 fracu_1(t) u_2(t) + u_3(t)u_1(u_5(t)) u_2(u_5(t)) + u_3(u_5(t)) \n  u_6(t) = t  t_1 frac10^-12 + u_2(t) + u_3(t)10^-12 + u_2(u_6(t)) + u_3(u_6(t))\nendalign*\n\nfor t in 0 300 with history function\n\nbeginalign*\n  ϕ_1(t) = ϕ_0 \n  ϕ_2(t) = 10^-15 \n  ϕ_3(t) = 0 \n  ϕ_4(t) = 0 \n  ϕ_5(t) = 0 \n  ϕ_6(t) = 0\nendalign*\n\nfor t  0, where α = 18, β = 20, γ = 0002, r = 510^4, s = 10^5, t_0 = 32, t_1 = 119, and ϕ_0 = 07510^-4.\n\nReferences\n\nWaltman, P. (1978). A threshold model of antigen-stimulated antibody production, Theoretical Immunology (8), pp. 437-453.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEProblemLibrary.prob_dde_qs","page":"DDE Problems","title":"DDEProblemLibrary.prob_dde_qs","text":"prob_dde_qs\n\nDelay differential equation model of Quorum Sensing (QS) of Pseudomonas putida IsoF in continuous cultures.\n\nReferences\n\nBuddrus-Schiemann et al. (2014). Analysis of N-Acylhomoserine Lactone Dynamics in Continuous Cultures of Pseudomonas Putida IsoF By Use of ELISA and UHPLC/qTOF-MS-derived Measurements and Mathematical Models, Analytical and Bioanalytical Chemistry.\n\n\n\n\n\n","category":"constant"},{"location":"examples/spiking_neural_systems/#Spiking-Neural-Systems","page":"Spiking Neural Systems","title":"Spiking Neural Systems","text":"This is an introduction to spiking neural systems with Julia's DifferentialEquations package. We will cover three different models: leaky integrate-and-fire, Izhikevich, and Hodgkin-Huxley. Finally, we will also learn about two mechanisms that simulate synaptic inputs like real neurons receive them. The alpha synapse and the Tsodyks-Markram synapse. Let's get started with the leaky integrate-and-fire (LIF) model.","category":"section"},{"location":"examples/spiking_neural_systems/#The-Leaky-Integrate-and-Fire-Model","page":"Spiking Neural Systems","title":"The Leaky Integrate-and-Fire Model","text":"The LIF model is an extension of the integrate-and-fire (IF) model. While the IF model simply integrates input until it fires, the LIF model integrates input but also decays towards an equilibrium potential. This means that inputs that arrive in quick succession have a much higher chance to make the cell spike as opposed to inputs that are further apart in time. The LIF is a more realistic neuron model than the IF, because it is known from real neurons that the timing of inputs is extremely relevant for their spiking.\n\nThe LIF model has five parameters, gL, EL, C, Vth, I and we define it in the lif(u, p, t) function.\n\nimport DifferentialEquations as DE\nimport ComponentArrays\nimport Plots\nPlots.gr()\n\nfunction lif(u, p, t)\n    gL, EL, C, Vth, I = p\n    (-gL * (u - EL) + I) / C\nend\n\nOur system is described by one differential equation: (-gL*(u-EL)+I)/C, where u is the voltage, I is the input, gL is the leak conductance, EL is the equilibrium potential of the leak conductance and C is the membrane capacitance. Generally, any change of the voltage is slowed down (filtered) by the membrane capacitance. That's why we divide the whole equation by C. Without any external input, the voltage always converges towards EL. If u is larger than EL, u decreases until it is at EL. If u is smaller than EL, u increases until it is at EL. The only other thing that can change the voltage is the external input I.\n\nOur lif function requires a certain parameter structure because it will need to be compatible with the DifferentialEquations interface. The input signature is lif(u, p, t) where u is the voltage, p is the collection of the parameters that describe the equation, and t is time. You might wonder why time does not show up in our equation, although we need to calculate the change in voltage with respect to time. The ODE solver will take care of time for us. One of the advantages of the ODE solver as opposed to calculating the change of u in a for loop is that many ODE solver algorithms can dynamically adjust the time step in a way that is efficient and accurate.\n\nOne crucial thing is still missing, however. This is supposed to be a model of neural spiking, right? So, we need a mechanism that recognizes the spike and hyperpolarizes u in response. For this purpose, we will use callbacks. They can make discontinuous changes to the model when certain conditions are met.\n\nfunction thr(u, t, integrator)\n    u - integrator.p.Vth\nend\n\nfunction reset!(integrator)\n    integrator.u = integrator.p.EL\nend\n\nthreshold = DE.ContinuousCallback(thr, reset!, nothing)\ncurrent_step = DE.PresetTimeCallback([2, 15], integrator -> integrator.p.I += 150.0)\ncb = DE.CallbackSet(current_step, threshold)\n\nOur condition is thr(u,t,integrator) and the condition kicks in when u > integrator.p.Vth. Our effect of the condition is reset!(integrator). It sets u back to the equilibrium potential p.EL. We then wrap both the condition and the effect into a ContinuousCallback called threshold. There is one more callback called PresetTimeCallback that is particularly useful. This one increases the input p.I at t=2 and t=15 by 150.0. Both callbacks are then combined into a CallbackSet. We are almost done to simulate our system, we just need to put numbers on our initial voltage and parameters.\n\nu0 = -75\ntspan = (0.0, 40.0)\np = ComponentArrays.ComponentArray(gL = 5.0, EL = -75.0, C = 50.0, Vth = -55.0, I = 0)\n\nprob = DE.ODEProblem(lif, u0, tspan, p, callback = cb)\n\nOur initial voltage is u0 = - 75, which will be the same as our equilibrium potential, so we start at a stable point. Then we define the timespan we want to simulate. The timescale of the LIF as it is defined conforms roughly to milliseconds. Then we define our parameters as p = ComponentArray(gL = 5.0, EL = -75.0, C = 50.0, Vth = -55.0, I = 0). Finally, we wrap everything into a call to ODEProblem. Can't forget the CallbackSet. With that, our model is defined. Now we just need to solve it with a quick call to solve.\n\nsol = DE.solve(prob)\n\nFirst of all the solve output tells us if solving the system generally worked. In this case, we know it worked because the return code (retcode) says Success. Then we get the numbers for the timestep and the solution to u. The raw numbers are not super interesting. So, let's plot our solution.\n\nPlots.plot(sol)\n\nWe see that the model is resting at -75 while there is no input. At t=2 the input increases by 150 and the model starts to spike. Spiking does not start immediately because the input first has to charge the membrane capacitance. Notice how once spiking starts, it rapidly becomes extremely regular. Increasing the input again at t=15 increases firing as we would expect, but it is still extremely regular. This is one of the features of the LIF. The firing frequency is regular for constant input and a linear function of the input strength. There are ways to make LIF models less regular. For example, we could use certain noise types at the input. We could also simulate a large number of LIF models and connect them synaptically. Instead of going into those topics, we will move on to the Izhikevich model, which is known for its ability to generate a large variety of spiking dynamics during constant inputs.","category":"section"},{"location":"examples/spiking_neural_systems/#The-Izhikevich-Model","page":"Spiking Neural Systems","title":"The Izhikevich Model","text":"The Izhikevich model is a two-dimensional model of neuronal spiking. It was derived from a bifurcation analysis of a cortical neuron. Because it is two-dimensional, it can generate much more complex spike dynamics than the LIF model. The kind of dynamics depends on the four parameters and the input a, b, c, d, I = p. All the concepts are the same as above, except for some minor changes to our function definitions to accommodate for the second dimension.\n\n#Izhikevichch Model\nimport DifferentialEquations as DE\nimport Plots\n\nfunction izh!(du, u, p, t)\n    a, b, c, d, I = p\n\n    du[1] = 0.04 * u[1]^2 + 5 * u[1] + 140 - u[2] + I\n    du[2] = a * (b * u[1] - u[2])\nend\n\nThis is our Izhikevich model. There are two important changes here. First of all, note the additional input parameter du. This is a sequence of differences. du[1] corresponds to the voltage (the first dimension of the system) and du[2] corresponds to the second dimension. This second dimension is called u in the original Izhikevich work, and it makes the notation a little annoying. In this tutorial, we will generally stick to Julia and DifferentialEquations conventions as opposed to conventions of the specific models and du is commonly used. We will never define du ourselves outside the function, but the ODE solver will use it internally. The other change here is the ! after our function name. This signifies that du will be preallocated before integration and then updated in-place, which saves a lot of allocation time. Now we just need our callbacks to take care of spikes and increase the input.\n\nfunction thr(u, t, integrator)\n    integrator.u[1] >= 30\nend\n\nfunction reset!(integrator)\n    integrator.u[1] = integrator.p[3]\n    integrator.u[2] += integrator.p[4]\nend\n\nthreshold = DE.DiscreteCallback(thr, reset!)\ncurrent_step = DE.PresetTimeCallback(50, integrator -> integrator.p[5] += 10)\ncb = DE.CallbackSet(current_step, threshold)\n\nOne key feature of the Izhikevich model is that each spike increases our second dimension u[2] by a preset amount p[4]. Between spikes u[2] decays to a value that depends on p[1] and p[2] and the equilibrium potential p[3]. Otherwise, the code is not too different from the LIF model. We will again need to define our parameters, and we are ready to simulate.\n\np = [0.02, 0.2, -50, 2, 0]\nu0 = [-65, p[2] * -65]\ntspan = (0.0, 300)\n\nprob = DE.ODEProblem(izh!, u0, tspan, p, callback = cb)\n\nsol = DE.solve(prob);\nPlots.plot(sol, vars = 1)\n\nThis spiking type is called chattering. It fires with intermittent periods of silence. Note that the input starts at t=50 and remain constant for the duration of the simulation. One of the mechanisms that sustains this type of firing is the spike induced hyperpolarization coming from our second dimension, so let's look at this variable.\n\nPlots.plot(sol, vars = 2)\n\nOur second dimension u[2] increases with every spike. When it becomes too large, the system cannot generate another spike until u[2] has decayed to a value small enough that spiking can resume. This process repeats. In this model, spiking is no longer regular like it was in the LIF. Here we have two frequencies, the frequency during the spiking state and the frequency between spiking states. The LIF model was dominated by one single frequency that was a function of the input strength. Let's see if we can generate another spiking type by changing the parameters.\n\np = [0.02, 0.2, -65, 8, 0]\nu0 = [-65, p[2] * -65]\ntspan = (0.0, 300)\n\nprob = DE.ODEProblem(izh!, u0, tspan, p, callback = cb)\nsol = DE.solve(prob);\nPlots.plot(sol, vars = 1)\n\nThis type is called regularly spiking, and we created it just by lowering p[3] and increasing p[4]. Note that the type is called regularly spiking, but it is not instantaneously regular. The instantaneous frequency is higher in the beginning. This is called spike frequency adaptation and is a common property of real neurons. There are many more spike types that can be generated. Check out the original Izhikevich work and create your own favorite neuron!","category":"section"},{"location":"examples/spiking_neural_systems/#Hodgkin-Huxley-Model","page":"Spiking Neural Systems","title":"Hodgkin-Huxley Model","text":"The Hodgkin-Huxley (HH) model is our first biophysically realistic model. This means that all parameters and mechanisms of the model represent biological mechanisms. Specifically, the HH model simulates the ionic currents that depolarize and hyperpolarize a neuron during an action potential. This makes the HH model four-dimensional. Let's see how it looks.\n\nimport DifferentialEquations as DE\nimport Plots\n\n# Potassium ion-channel rate functions\nalpha_n(v) = (0.02 * (v - 25.0)) / (1.0 - exp((-1.0 * (v - 25.0)) / 9.0))\nbeta_n(v) = (-0.002 * (v - 25.0)) / (1.0 - exp((v - 25.0) / 9.0))\n\n# Sodium ion-channel rate functions\nalpha_m(v) = (0.182 * (v + 35.0)) / (1.0 - exp((-1.0 * (v + 35.0)) / 9.0))\nbeta_m(v) = (-0.124 * (v + 35.0)) / (1.0 - exp((v + 35.0) / 9.0))\n\nalpha_h(v) = 0.25 * exp((-1.0 * (v + 90.0)) / 12.0)\nbeta_h(v) = (0.25 * exp((v + 62.0) / 6.0)) / exp((v + 90.0) / 12.0)\n\nfunction HH!(du, u, p, t)\n    gK, gNa, gL, EK, ENa, EL, C, I = p\n    v, n, m, h = u\n\n    du[1] = (-(gK * n^4 * (v - EK)) - (gNa * m^3 * h * (v - ENa)) -\n             (gL * (v - EL)) + I) / C\n    du[2] = (alpha_n(v) * (1.0 - n)) - (beta_n(v) * n)\n    du[3] = (alpha_m(v) * (1.0 - m)) - (beta_m(v) * m)\n    du[4] = (alpha_h(v) * (1.0 - h)) - (beta_h(v) * h)\nend\n\nWe have three different types of ionic conductances. Potassium, sodium and the leak. The potassium and sodium conductance are voltage gated. They increase or decrease depending on the voltage. In ion channel terms, open channels can transition to the closed state and closed channels can transition to the open state. It's probably easiest to start with the potassium current described by gK * (n^4.0) * (EK - v). Here gK is the total possible conductance that we could reach if all potassium channels were open. If all channels were open, n would equal 1 which is usually not the case. The transition from open state to closed state is modeled in alpha_n(v) while the transition from closed to open is in beta_n(v). Because potassium conductance is voltage gated, these transitions depend on v. The numbers in alpha_n; beta_n were calculated by Hodgkin and Huxley based on their extensive experiments on the squid giant axon. They also determined, that n needs to be taken to the power of 4 to correctly model the amount of open channels.\n\nThe sodium current is not very different, but it has two gating variables, m, h instead of one. The leak conductance gL has no gating variables because it is not voltage gated. Let's move on to the parameters. If you want all the details on the HH model, you can find a great description here.\n\ncurrent_step = DE.PresetTimeCallback(100, integrator -> integrator.p[8] += 1)\n\n# n, m & h steady-states\nn_inf(v) = alpha_n(v) / (alpha_n(v) + beta_n(v))\nm_inf(v) = alpha_m(v) / (alpha_m(v) + beta_m(v))\nh_inf(v) = alpha_h(v) / (alpha_h(v) + beta_h(v))\n\np = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0]\nu0 = [-60, n_inf(-60), m_inf(-60), h_inf(-60)]\ntspan = (0.0, 1000)\n\nprob = DE.ODEProblem(HH!, u0, tspan, p, callback = current_step)\n\nFor the HH model, we need only one callback. The PresetTimeCallback that starts our input current. We don't need to reset the voltage when it reaches threshold because the HH model has its own repolarization mechanism. That is the potassium current, which activates at large voltages and makes the voltage more negative. The three functions n_inf; m_inf; h_inf help us find good initial values for the gating variables. Those functions tell us that the steady-state gating values should be for the initial voltage. The parameters were chosen in a way that the properties of the model roughly resemble that of a cortical pyramidal cell instead of the giant axon Hodgkin and Huxley were originally working on.\n\nsol = DE.solve(prob);\nPlots.plot(sol, vars = 1)\n\nThat's some good regular voltage spiking. One of the cool things about a biophysically realistic model is that the gating variables tell us something about the mechanisms behind the action potential. You might have seen something like the following plot in a biology textbook.\n\nPlots.plot(sol, vars = [2, 3, 4], tspan = (105.0, 130.0))\n\nSo far, we have only given our neurons simple step inputs by simply changing the number I. Actual neurons receive their inputs mostly from chemical synapses. They produce conductance changes with very complex structures. In the next chapter, we will incorporate a synapse into our HH model.","category":"section"},{"location":"examples/spiking_neural_systems/#Alpha-Synapse","page":"Spiking Neural Systems","title":"Alpha Synapse","text":"One of the most simple synaptic mechanisms used in computational neuroscience is the alpha synapse. When this mechanism is triggered, it causes an instantaneous rise in conductance followed by an exponential decay. Let's incorporate that into our HH model.\n\nfunction gSyn(max_gsyn, tau, tf, t)\n    if t - tf >= 0\n        return max_gsyn * exp(-(t - tf) / tau)\n    else\n        return 0.0\n    end\nend\nfunction HH!(du, u, p, t)\n    gK, gNa, gL, EK, ENa, EL, C, I, max_gSyn, ESyn, tau, tf = p\n    v, n, m, h = u\n\n    ISyn = gSyn(max_gSyn, tau, tf, t) * (v - ESyn)\n\n    du[1] = (-(gK * (n^4.0) * (v - EK)) - (gNa * (m^3.0) * h * (v - ENa)) -\n             (gL * (v - EL)) + I - ISyn) / C\n    du[2] = (alpha_n(v) * (1.0 - n)) - (beta_n(v) * n)\n    du[3] = (alpha_m(v) * (1.0 - m)) - (beta_m(v) * m)\n    du[4] = (alpha_h(v) * (1.0 - h)) - (beta_h(v) * h)\nend\n\ngSyn models the step to the maximum conductance and the following exponential decay with time constant tau. Of course, we only want to integrate the conductance at and after time tf, the onset of the synaptic response. Before tf, gSyn returns zero. To convert the conductance to a current, we multiply by the difference between the current voltage and the synapses' equilibrium voltage: ISyn = gSyn(max_gSyn, tau, tf, t) * (v - ESyn). Later we will set the parameter ESyn to 0, making this synapse an excitatory synapse. Excitatory synapses have equilibrium potentials far above the resting potential. Let's see what our synapse does to the voltage of the cell.\n\np = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0, 0.008, 0, 20, 100]\ntspan = (0.0, 200)\nprob = DE.ODEProblem(HH!, u0, tspan, p)\nsol = DE.solve(prob);\nPlots.plot(sol, vars = 1)\n\nWhat you see here is called an excitatory postsynaptic potential (EPSP). It is the voltage response to a synaptic current. While our synaptic conductance rises instantly, the voltage response rises at a slower time course that is given by the membrane capacitance C. This particular voltage response is not strong enough to evoke spiking, so we say it is subthreshold. To get a suprathreshold response that evokes spiking, we simply increase the parameter max_gSyn to increase the maximum conductance.\n\np = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0, 0.01, 0, 20, 100]\ntspan = (0.0, 200)\nprob = DE.ODEProblem(HH!, u0, tspan, p)\nsol = DE.solve(prob);\nPlots.plot!(sol, vars = 1)\n\nThis plot shows the subthreshold EPSP from above as well as the suprathreshold EPSP. Alpha synapses are nice because of their simplicity. Real synapses, however, are extremely complex structures. One of the most important features of real synapses is that their maximum conductance is not the same on every event. The number and frequency of synaptic events changes the size of the maximum conductance in a dynamic way. While we usually avoid anatomical and biophysical details of real synapses, there is a widely used phenomenological way to capture those dynamics, called the Tsodyks-Markram synapse.","category":"section"},{"location":"examples/spiking_neural_systems/#Tsodyks-Markram-Synapse","page":"Spiking Neural Systems","title":"Tsodyks-Markram Synapse","text":"The Tsodyks-Markram synapse (TMS) is a dynamic system that models the changes of maximum conductance that occur between EPSPs at different frequencies. The single response is similar to the alpha synapse in that it rises instantaneously and decays exponentially. The maximum conductance it reaches depends on the event history. To simulate the TMS we need to incorporate three more dimensions, u, R, gsyn into our system. u decays towards 0, R decays towards 1 and gsyn decays towards 0 as it did with the alpha synapse. The crucial part of the TMS is in epsp!, where we handle the discontinuities when a synaptic event occurs. Instead of just setting gsyn to the maximum conductance gmax, we increment gsyn by a fraction of gmax that depends on the other two dynamic parameters. The frequency dependence comes from the size of the time constants tau_u and tau_R. Enough talk, let's simulate it.\n\nfunction HH!(du, u, p, t)\n    gK, gNa, gL, EK, ENa, EL, C, I, tau, tau_u, tau_R, u0, gmax, Esyn = p\n    v, n, m, h, u, R, gsyn = u\n\n    du[1] = ((gK * (n^4.0) * (EK - v)) + (gNa * (m^3.0) * h * (ENa - v)) + (gL * (EL - v)) +\n             I + gsyn * (Esyn - v)) / C\n    du[2] = (alpha_n(v) * (1.0 - n)) - (beta_n(v) * n)\n    du[3] = (alpha_m(v) * (1.0 - m)) - (beta_m(v) * m)\n    du[4] = (alpha_h(v) * (1.0 - h)) - (beta_h(v) * h)\n\n    # Synaptic variables\n    du[5] = -(u / tau_u)\n    du[6] = (1 - R) / tau_R\n    du[7] = -(gsyn / tau)\nend\n\nfunction epsp!(integrator)\n    integrator.u[5] += integrator.p[12] * (1 - integrator.u[5])\n    integrator.u[7] += integrator.p[13] * integrator.u[5] * integrator.u[6]\n    integrator.u[6] -= integrator.u[5] * integrator.u[6]\nend\n\nepsp_ts = DE.PresetTimeCallback(100:100:500, epsp!)\n\np = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0, 30, 1000, 50, 0.5, 0.005, 0]\nu0 = [-60, n_inf(-60), m_inf(-60), h_inf(-60), 0.0, 1.0, 0.0]\ntspan = (0.0, 700)\nprob = DE.ODEProblem(HH!, u0, tspan, p, callback = epsp_ts)\nsol = DE.solve(prob);\nPlots.plot(sol, vars = 1)\n\nPlots.plot(sol, vars = 7)\n\nBoth the voltage response and the conductances show what is called short-term facilitation. An increase in peak conductance over multiple synaptic events. Here the first event has a conductance of around 0.0025 and the last one of 0.004. We can plot the other two variables to see what underlies those dynamics\n\nPlots.plot(sol, vars = [5, 6])\n\nBecause of the time courses at play here, this facilitation is frequency-dependent. If we increase the period between these events, facilitation does not occur.\n\nepsp_ts = DE.PresetTimeCallback(100:1000:5100, epsp!)\n\np = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0, 30, 500, 50, 0.5, 0.005, 0]\nu0 = [-60, n_inf(-60), m_inf(-60), h_inf(-60), 0.0, 1.0, 0.0]\ntspan = (0.0, 5300)\nprob = DE.ODEProblem(HH!, u0, tspan, p, callback = epsp_ts)\nsol = DE.solve(prob);\nPlots.plot(sol, vars = 7)\n\nPlots.plot(sol, vars = [5, 6])\n\nWe can also change these time constants such that the dynamics show short-term depression instead of facilitation.\n\nepsp_ts = DE.PresetTimeCallback(100:100:500, epsp!)\n\np = [35.0, 40.0, 0.3, -77.0, 55.0, -65.0, 1, 0, 30, 100, 1000, 0.5, 0.005, 0]\nu0 = [-60, n_inf(-60), m_inf(-60), h_inf(-60), 0.0, 1.0, 0.0]\ntspan = (0.0, 700)\nprob = DE.ODEProblem(HH!, u0, tspan, p, callback = epsp_ts)\nsol = DE.solve(prob);\nPlots.plot(sol, vars = 7)\n\nPlots.plot(sol, vars = [5, 6])\n\nJust changing those two time constants has changed the dynamics to short-term depression. This is still frequency dependent. Changing these parameters can generate a variety of different short-term dynamics.","category":"section"},{"location":"examples/spiking_neural_systems/#Summary","page":"Spiking Neural Systems","title":"Summary","text":"That's it for now. Thanks for making it this far. If you want to learn more about neuronal dynamics, this is a great resource.","category":"section"},{"location":"tutorials/faster_ode_example/#speed","page":"Code Optimization for Differential Equations","title":"Code Optimization for Differential Equations","text":"note: Note\nSee this FAQ for information on common pitfalls and how to improve performance.","category":"section"},{"location":"tutorials/faster_ode_example/#Code-Optimization-in-Julia","page":"Code Optimization for Differential Equations","title":"Code Optimization in Julia","text":"Before starting this tutorial, we recommend the reader to check out one of the many tutorials for optimization Julia code. The following is an incomplete list:\n\nThe Julia Performance Tips\nMIT 18.337 Course Notes on Optimizing Serial Code\nWhat scientists must know about hardware to write fast code\n\nUser-side optimizations are important because, for sufficiently difficult problems, most time will be spent inside your f function, the function you are trying to solve. “Efficient” integrators are those that reduce the required number of f calls to hit the error tolerance. The main ideas for optimizing your DiffEq code, or any Julia function, are the following:\n\nMake it non-allocating\nUse StaticArrays for small arrays\nUse broadcast fusion\nMake it type-stable\nReduce redundant calculations\nMake use of BLAS calls\nOptimize algorithm choice\n\nWe'll discuss these strategies in the context of differential equations. Let's start with small systems.","category":"section"},{"location":"tutorials/faster_ode_example/#Example-Accelerating-a-Non-Stiff-Equation:-The-Lorenz-Equation","page":"Code Optimization for Differential Equations","title":"Example Accelerating a Non-Stiff Equation: The Lorenz Equation","text":"Let's take the classic Lorenz system. Let's start by naively writing the system in its out-of-place form:\n\nfunction lorenz(u, p, t)\n    dx = 10.0 * (u[2] - u[1])\n    dy = u[1] * (28.0 - u[3]) - u[2]\n    dz = u[1] * u[2] - (8 / 3) * u[3]\n    [dx, dy, dz]\nend\n\nHere, lorenz returns an object, [dx,dy,dz], which is created within the body of lorenz.\n\nThis is a common code pattern from high-level languages like MATLAB, SciPy, or R's deSolve. However, the issue with this form is that it allocates a vector, [dx,dy,dz], at each step. Let's benchmark the solution process with this choice of function:\n\nimport DifferentialEquations as DE, BenchmarkTools as BT\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 100.0)\nprob = DE.ODEProblem(lorenz, u0, tspan)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nThe BenchmarkTools.jl package's BT.@benchmark runs the code multiple times to get an accurate measurement. The minimum time is the time it takes when your OS and other background processes aren't getting in the way. Notice that in this case it takes about 5ms to solve and allocates around 11.11 MiB. However, if we were to use this inside of a real user code, we'd see a lot of time spent doing garbage collection (GC) to clean up all the arrays we made. Even if we turn off saving, we have these allocations.\n\nBT.@btime DE.solve(prob, DE.Tsit5(); save_everystep = false);\nnothing # hide\n\nThe problem, of course, is that arrays are created every time our derivative function is called. This function is called multiple times per step and is thus the main source of memory usage. To fix this, we can use the in-place form to ***make our code non-allocating***:\n\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\n    nothing\nend\n\nHere, instead of creating an array each time, we utilized the cache array du. When the in-place form is used, DifferentialEquations.jl takes a different internal route that minimizes the internal allocations as well.\n\nnote: Note\nNotice that nothing is returned. When in in-place form, the ODE solver ignores the return. Instead, make sure that the original du array is mutated instead of constructing a new array\n\nWhen we benchmark this function, we will see quite a difference.\n\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 100.0)\nprob = DE.ODEProblem(lorenz!, u0, tspan)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nBT.@btime DE.solve(prob, DE.Tsit5(); save_everystep = false);\nnothing # hide\n\nThere is a 16x time difference just from that change! Notice there are still some allocations and this is due to the construction of the integration cache. But this doesn't scale with the problem size:\n\ntspan = (0.0, 500.0) # 5x longer than before\nprob = DE.ODEProblem(lorenz!, u0, tspan)\nBT.@btime DE.solve(prob, DE.Tsit5(); save_everystep = false);\nnothing # hide\n\nSince that's all setup allocations, the user-side optimization is complete.","category":"section"},{"location":"tutorials/faster_ode_example/#Further-Optimizations-of-Small-Non-Stiff-ODEs-with-StaticArrays","page":"Code Optimization for Differential Equations","title":"Further Optimizations of Small Non-Stiff ODEs with StaticArrays","text":"Allocations are only expensive if they are “heap allocations”. For a more in-depth definition of heap allocations, there are many sources online. But a good working definition is that heap allocations are variable-sized slabs of memory which have to be pointed to, and this pointer indirection costs time. Additionally, the heap has to be managed, and the garbage controllers has to actively keep track of what's on the heap.\n\nHowever, there's an alternative to heap allocations, known as stack allocations. The stack is statically-sized (known at compile time) and thus its accesses are quick. Additionally, the exact block of memory is known in advance by the compiler, and thus re-using the memory is cheap. This means that allocating on the stack has essentially no cost!\n\nArrays have to be heap allocated because their size (and thus the amount of memory they take up) is determined at runtime. But there are structures in Julia which are stack-allocated. structs for example are stack-allocated “value-type”s. Tuples are a stack-allocated collection. The most useful data structure for DiffEq though is the StaticArray from the package StaticArrays.jl. These arrays have their length determined at compile-time. They are created using macros attached to normal array expressions, for example:\n\nimport StaticArrays\nA = StaticArrays.SA[2.0, 3.0, 5.0]\ntypeof(A) # StaticArrays.SVector{3, Float64} (alias for StaticArrays.SArray{Tuple{3}, Float64, 1, 3})\n\nNotice that the 3 after StaticArrays.SVector gives the size of the StaticArrays.SVector. It cannot be changed. Additionally, StaticArrays.SVectors are immutable, so we have to create a new StaticArrays.SVector to change values. But remember, we don't have to worry about allocations because this data structure is stack-allocated. SArrays have numerous extra optimizations as well: they have fast matrix multiplication, fast QR factorizations, etc. which directly make use of the information about the size of the array. Thus, when possible, they should be used.\n\nUnfortunately, static arrays can only be used for sufficiently small arrays. After a certain size, they are forced to heap allocate after some instructions and their compile time balloons. Thus, static arrays shouldn't be used if your system has more than ~20 variables. Additionally, only the native Julia algorithms can fully utilize static arrays.\n\nLet's ***optimize lorenz using static arrays***. Note that in this case, we want to use the out-of-place allocating form, but this time we want to output a static array:\n\nfunction lorenz_static(u, p, t)\n    dx = 10.0 * (u[2] - u[1])\n    dy = u[1] * (28.0 - u[3]) - u[2]\n    dz = u[1] * u[2] - (8 / 3) * u[3]\n    StaticArrays.SA[dx, dy, dz]\nend\n\nTo make the solver internally use static arrays, we simply give it a static array as the initial condition:\n\nu0 = StaticArrays.SA[1.0, 0.0, 0.0]\ntspan = (0.0, 100.0)\nprob = DE.ODEProblem(lorenz_static, u0, tspan)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nBT.@btime DE.solve(prob, DE.Tsit5(); save_everystep = false);\nnothing # hide\n\nAnd that's pretty much all there is to it. With static arrays, you don't have to worry about allocating, so use operations like * and don't worry about fusing operations (discussed in the next section). Do “the vectorized code” of R/MATLAB/Python and your code in this case will be fast, or directly use the numbers/values.","category":"section"},{"location":"tutorials/faster_ode_example/#Example-Accelerating-a-Stiff-Equation:-the-Robertson-Equation","page":"Code Optimization for Differential Equations","title":"Example Accelerating a Stiff Equation: the Robertson Equation","text":"For these next examples, let's solve the Robertson equations (also known as ROBER):\n\nbeginaligned\nfracdy_1dt = -004y₁ + 10^4 y_2 y_3 \nfracdy_2dt = 004 y_1 - 10^4 y_2 y_3 - 310^7 y_2^2 \nfracdy_3dt = 310^7 y_2^2 \nendaligned\n\nGiven that these equations are stiff, non-stiff ODE solvers like DE.Tsit5 or DE.Vern9 will fail to solve these equations. The automatic algorithm will detect this and automatically switch to something more robust to handle these issues. For example:\n\nimport DifferentialEquations as DE\nimport Plots\nfunction rober!(du, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du[1] = -k₁ * y₁ + k₃ * y₂ * y₃\n    du[2] = k₁ * y₁ - k₂ * y₂^2 - k₃ * y₂ * y₃\n    du[3] = k₂ * y₂^2\n    nothing\nend\nprob = DE.ODEProblem(rober!, [1.0, 0.0, 0.0], (0.0, 1e5), [0.04, 3e7, 1e4])\nsol = DE.solve(prob)\nPlots.plot(sol, tspan = (1e-2, 1e5), xscale = :log10)\n\nimport BenchmarkTools as BT\nBT.@btime DE.solve(prob);\nnothing # hide","category":"section"},{"location":"tutorials/faster_ode_example/#Choosing-a-Good-Solver","page":"Code Optimization for Differential Equations","title":"Choosing a Good Solver","text":"Choosing a good solver is required for getting top-notch speed. General recommendations can be found on the solver page (for example, the ODE Solver Recommendations). The current recommendations can be simplified to a Rosenbrock method (DE.Rosenbrock23 or DE.Rodas5) for smaller (<50 ODEs) problems, ESDIRK methods for slightly larger (DE.TRBDF2 or DE.KenCarp4 for <2000 ODEs), and DE.QNDF for even larger problems. lsoda from LSODA.jl is sometimes worth a try for the medium-sized category.\n\nMore details on the solver to choose can be found by benchmarking. See the SciMLBenchmarks to compare many solvers on many problems.\n\nFrom this, we try the recommendation of DE.Rosenbrock23() for stiff ODEs at default tolerances:\n\nBT.@btime DE.solve(prob, DE.Rosenbrock23());\nnothing # hide","category":"section"},{"location":"tutorials/faster_ode_example/#Declaring-Jacobian-Functions","page":"Code Optimization for Differential Equations","title":"Declaring Jacobian Functions","text":"In order to reduce the Jacobian construction cost, one can describe a Jacobian function by using the jac argument for the DE.ODEFunction. First we have to derive the Jacobian fracdf_idu_j which is J[i,j]. From this, we get:\n\nfunction rober_jac!(J, u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    J[1, 1] = k₁ * -1\n    J[2, 1] = k₁\n    J[3, 1] = 0\n    J[1, 2] = y₃ * k₃\n    J[2, 2] = y₂ * k₂ * -2 + y₃ * k₃ * -1\n    J[3, 2] = y₂ * 2 * k₂\n    J[1, 3] = k₃ * y₂\n    J[2, 3] = k₃ * y₂ * -1\n    J[3, 3] = 0\n    nothing\nend\nf! = DE.ODEFunction(rober!, jac = rober_jac!)\nprob_jac = DE.ODEProblem(f!, [1.0, 0.0, 0.0], (0.0, 1e5), (0.04, 3e7, 1e4))\n\nBT.@btime DE.solve(prob_jac, DE.Rosenbrock23());\nnothing # hide","category":"section"},{"location":"tutorials/faster_ode_example/#Automatic-Derivation-of-Jacobian-Functions","page":"Code Optimization for Differential Equations","title":"Automatic Derivation of Jacobian Functions","text":"But that was hard! If you want to take the symbolic Jacobian of numerical code, we can make use of ModelingToolkit.jl to symbolic-ify the numerical code and do the symbolic calculation and return the Julia code for this.\n\nimport ModelingToolkit as MTK\nde = MTK.complete(MTK.modelingtoolkitize(prob))\n\nWe can tell it to compute the Jacobian if we want to see the code:\n\nMTK.generate_jacobian(de)[2] # Second is in-place\n\nNow let's use that to give the analytical solution Jacobian:\n\nprob_jac2 = DE.ODEProblem(de, [], (0.0, 1e5); jac = true)\n\nBT.@btime DE.solve(prob_jac2);\nnothing # hide\n\nSee the ModelingToolkit.jl documentation for more details.","category":"section"},{"location":"tutorials/faster_ode_example/#Accelerating-Small-ODE-Solves-with-Static-Arrays","page":"Code Optimization for Differential Equations","title":"Accelerating Small ODE Solves with Static Arrays","text":"If the ODE is sufficiently small (<20 ODEs or so), using StaticArrays.jl for the state variables can greatly enhance the performance. This is done by making u0 a StaticArray and writing an out-of-place non-mutating dispatch for static arrays, for the ROBER problem, this looks like:\n\nimport StaticArrays\nfunction rober_static(u, p, t)\n    y₁, y₂, y₃ = u\n    k₁, k₂, k₃ = p\n    du1 = -k₁ * y₁ + k₃ * y₂ * y₃\n    du2 = k₁ * y₁ - k₂ * y₂^2 - k₃ * y₂ * y₃\n    du3 = k₂ * y₂^2\n    StaticArrays.SA[du1, du2, du3]\nend\nprob = DE.ODEProblem(rober_static, StaticArrays.SA[1.0, 0.0, 0.0], (0.0, 1e5), StaticArrays.SA[0.04, 3e7, 1e4])\nsol = DE.solve(prob, DE.Rosenbrock23())\n\nIf we benchmark this, we see a really fast solution with really low allocation counts:\n\nBT.@btime sol = DE.solve(prob, DE.Rosenbrock23());\nnothing # hide\n\nThis version is thus very amenable to multithreading and other forms of parallelism.","category":"section"},{"location":"tutorials/faster_ode_example/#Example-Accelerating-Linear-Algebra-PDE-Semi-Discretization","page":"Code Optimization for Differential Equations","title":"Example Accelerating Linear Algebra PDE Semi-Discretization","text":"In this tutorial, we will optimize the right-hand side definition of a PDE semi-discretization.\n\nnote: Note\nWe highly recommend looking at the Solving Large Stiff Equations tutorial for details on customizing DifferentialEquations.jl for more efficient large-scale stiff ODE solving. This section will only focus on the user-side code.\n\nLet's optimize the solution of a Reaction-Diffusion PDE's discretization. In its discretized form, this is the ODE:\n\nbeginalign*\ndu = D_1 (A_y u + u A_x) + fracau^2v + baru - alpha u\ndv = D_2 (A_y v + v A_x) + a u^2 + beta v\nendalign*\n\nwhere u, v, and A are matrices. Here, we will use the simplified version where A is the tridiagonal stencil 1-21, i.e. it's the 2D discretization of the Laplacian. The native code would be something along the lines of:\n\nimport DifferentialEquations as DE, LinearAlgebra as LA, BenchmarkTools as BT\n# Generate the constants\np = (1.0, 1.0, 1.0, 10.0, 0.001, 100.0) # a,α,ubar,β,D1,D2\nN = 100\nAx = Array(LA.Tridiagonal([1.0 for i in 1:(N - 1)], [-2.0 for i in 1:N],\n    [1.0 for i in 1:(N - 1)]))\nAy = copy(Ax)\nAx[2, 1] = 2.0\nAx[end - 1, end] = 2.0\nAy[1, 2] = 2.0\nAy[end, end - 1] = 2.0\n\nfunction basic_version!(dr, r, p, t)\n    a, α, ubar, β, D1, D2 = p\n    u = r[:, :, 1]\n    v = r[:, :, 2]\n    Du = D1 * (Ay * u + u * Ax)\n    Dv = D2 * (Ay * v + v * Ax)\n    dr[:, :, 1] = Du .+ a .* u .* u ./ v .+ ubar .- α * u\n    dr[:, :, 2] = Dv .+ a .* u .* u .- β * v\nend\n\na, α, ubar, β, D1, D2 = p\nuss = (ubar + β) / α\nvss = (a / β) * uss^2\nr0 = zeros(100, 100, 2)\nr0[:, :, 1] .= uss .+ 0.1 .* rand.()\nr0[:, :, 2] .= vss\n\nprob = DE.ODEProblem(basic_version!, r0, (0.0, 0.1), p)\n\nIn this version, we have encoded our initial condition to be a 3-dimensional array, with u[:,:,1] being the A part and u[:,:,2] being the B part.\n\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nWhile this version isn't very efficient,","category":"section"},{"location":"tutorials/faster_ode_example/#We-recommend-writing-the-“high-level”-code-first,-and-iteratively-optimizing-it!","page":"Code Optimization for Differential Equations","title":"We recommend writing the “high-level” code first, and iteratively optimizing it!","text":"The first thing that we can do is get rid of the slicing allocations. The operation r[:,:,1] creates a temporary array instead of a “view”, i.e. a pointer to the already existing memory. To make it a view, add @view. Note that we have to be careful with views because they point to the same memory, and thus changing a view changes the original values:\n\nA = rand(4)\n@show A\nB = @view A[1:3]\nB[2] = 2\n@show A\n\nNotice that changing B changed A. This is something to be careful of, but at the same time we want to use this since we want to modify the output dr. Additionally, the last statement is a purely element-wise operation, and thus we can make use of broadcast fusion there. Let's rewrite basic_version! to ***avoid slicing allocations*** and to ***use broadcast fusion***:\n\nfunction gm2!(dr, r, p, t)\n    a, α, ubar, β, D1, D2 = p\n    u = @view r[:, :, 1]\n    v = @view r[:, :, 2]\n    du = @view dr[:, :, 1]\n    dv = @view dr[:, :, 2]\n    Du = D1 * (Ay * u + u * Ax)\n    Dv = D2 * (Ay * v + v * Ax)\n    @. du = Du + a .* u .* u ./ v + ubar - α * u\n    @. dv = Dv + a .* u .* u - β * v\nend\nprob = DE.ODEProblem(gm2!, r0, (0.0, 0.1), p)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nNow, most of the allocations are taking place in Du = D1*(Ay*u + u*Ax) since those operations are vectorized and not mutating. We should instead replace the matrix multiplications with LA.mul!. When doing so, we will need to have cache variables to write into. This looks like:\n\nAyu = zeros(N, N)\nuAx = zeros(N, N)\nDu = zeros(N, N)\nAyv = zeros(N, N)\nvAx = zeros(N, N)\nDv = zeros(N, N)\nfunction gm3!(dr, r, p, t)\n    a, α, ubar, β, D1, D2 = p\n    u = @view r[:, :, 1]\n    v = @view r[:, :, 2]\n    du = @view dr[:, :, 1]\n    dv = @view dr[:, :, 2]\n    LA.mul!(Ayu, Ay, u)\n    LA.mul!(uAx, u, Ax)\n    LA.mul!(Ayv, Ay, v)\n    LA.mul!(vAx, v, Ax)\n    @. Du = D1 * (Ayu + uAx)\n    @. Dv = D2 * (Ayv + vAx)\n    @. du = Du + a * u * u ./ v + ubar - α * u\n    @. dv = Dv + a * u * u - β * v\nend\nprob = DE.ODEProblem(gm3!, r0, (0.0, 0.1), p)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nBut our temporary variables are global variables. We need to either declare the caches as const or localize them. We can localize them by adding them to the parameters, p. It's easier for the compiler to reason about local variables than global variables. ***Localizing variables helps to ensure type stability***.\n\np = (1.0, 1.0, 1.0, 10.0, 0.001, 100.0, Ayu, uAx, Du, Ayv, vAx, Dv) # a,α,ubar,β,D1,D2\nfunction gm4!(dr, r, p, t)\n    a, α, ubar, β, D1, D2, Ayu, uAx, Du, Ayv, vAx, Dv = p\n    u = @view r[:, :, 1]\n    v = @view r[:, :, 2]\n    du = @view dr[:, :, 1]\n    dv = @view dr[:, :, 2]\n    LA.mul!(Ayu, Ay, u)\n    LA.mul!(uAx, u, Ax)\n    LA.mul!(Ayv, Ay, v)\n    LA.mul!(vAx, v, Ax)\n    @. Du = D1 * (Ayu + uAx)\n    @. Dv = D2 * (Ayv + vAx)\n    @. du = Du + a * u * u ./ v + ubar - α * u\n    @. dv = Dv + a * u * u - β * v\nend\nprob = DE.ODEProblem(gm4!, r0, (0.0, 0.1), p)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nWe could then use the BLAS gemmv to optimize the matrix multiplications some more, but instead let's devectorize the stencil.\n\np = (1.0, 1.0, 1.0, 10.0, 0.001, 100.0, N)\nfunction fast_gm!(du, u, p, t)\n    a, α, ubar, β, D1, D2, N = p\n\n    @inbounds for j in 2:(N - 1), i in 2:(N - 1)\n\n        du[i, j, 1] = D1 *\n                      (u[i - 1, j, 1] + u[i + 1, j, 1] + u[i, j + 1, 1] + u[i, j - 1, 1] -\n                       4u[i, j, 1]) +\n                      a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n    end\n\n    @inbounds for j in 2:(N - 1), i in 2:(N - 1)\n\n        du[i, j, 2] = D2 *\n                      (u[i - 1, j, 2] + u[i + 1, j, 2] + u[i, j + 1, 2] + u[i, j - 1, 2] -\n                       4u[i, j, 2]) +\n                      a * u[i, j, 1]^2 - β * u[i, j, 2]\n    end\n\n    @inbounds for j in 2:(N - 1)\n        i = 1\n        du[1, j, 1] = D1 *\n                      (2u[i + 1, j, 1] + u[i, j + 1, 1] + u[i, j - 1, 1] - 4u[i, j, 1]) +\n                      a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n    end\n    @inbounds for j in 2:(N - 1)\n        i = 1\n        du[1, j, 2] = D2 *\n                      (2u[i + 1, j, 2] + u[i, j + 1, 2] + u[i, j - 1, 2] - 4u[i, j, 2]) +\n                      a * u[i, j, 1]^2 - β * u[i, j, 2]\n    end\n    @inbounds for j in 2:(N - 1)\n        i = N\n        du[end, j, 1] = D1 *\n                        (2u[i - 1, j, 1] + u[i, j + 1, 1] + u[i, j - 1, 1] - 4u[i, j, 1]) +\n                        a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n    end\n    @inbounds for j in 2:(N - 1)\n        i = N\n        du[end, j, 2] = D2 *\n                        (2u[i - 1, j, 2] + u[i, j + 1, 2] + u[i, j - 1, 2] - 4u[i, j, 2]) +\n                        a * u[i, j, 1]^2 - β * u[i, j, 2]\n    end\n\n    @inbounds for i in 2:(N - 1)\n        j = 1\n        du[i, 1, 1] = D1 *\n                      (u[i - 1, j, 1] + u[i + 1, j, 1] + 2u[i, j + 1, 1] - 4u[i, j, 1]) +\n                      a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n    end\n    @inbounds for i in 2:(N - 1)\n        j = 1\n        du[i, 1, 2] = D2 *\n                      (u[i - 1, j, 2] + u[i + 1, j, 2] + 2u[i, j + 1, 2] - 4u[i, j, 2]) +\n                      a * u[i, j, 1]^2 - β * u[i, j, 2]\n    end\n    @inbounds for i in 2:(N - 1)\n        j = N\n        du[i, end, 1] = D1 *\n                        (u[i - 1, j, 1] + u[i + 1, j, 1] + 2u[i, j - 1, 1] - 4u[i, j, 1]) +\n                        a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n    end\n    @inbounds for i in 2:(N - 1)\n        j = N\n        du[i, end, 2] = D2 *\n                        (u[i - 1, j, 2] + u[i + 1, j, 2] + 2u[i, j - 1, 2] - 4u[i, j, 2]) +\n                        a * u[i, j, 1]^2 - β * u[i, j, 2]\n    end\n\n    @inbounds begin\n        i = 1\n        j = 1\n        du[1, 1, 1] = D1 * (2u[i + 1, j, 1] + 2u[i, j + 1, 1] - 4u[i, j, 1]) +\n                      a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n        du[1, 1, 2] = D2 * (2u[i + 1, j, 2] + 2u[i, j + 1, 2] - 4u[i, j, 2]) +\n                      a * u[i, j, 1]^2 - β * u[i, j, 2]\n\n        i = 1\n        j = N\n        du[1, N, 1] = D1 * (2u[i + 1, j, 1] + 2u[i, j - 1, 1] - 4u[i, j, 1]) +\n                      a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n        du[1, N, 2] = D2 * (2u[i + 1, j, 2] + 2u[i, j - 1, 2] - 4u[i, j, 2]) +\n                      a * u[i, j, 1]^2 - β * u[i, j, 2]\n\n        i = N\n        j = 1\n        du[N, 1, 1] = D1 * (2u[i - 1, j, 1] + 2u[i, j + 1, 1] - 4u[i, j, 1]) +\n                      a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n        du[N, 1, 2] = D2 * (2u[i - 1, j, 2] + 2u[i, j + 1, 2] - 4u[i, j, 2]) +\n                      a * u[i, j, 1]^2 - β * u[i, j, 2]\n\n        i = N\n        j = N\n        du[end, end, 1] = D1 * (2u[i - 1, j, 1] + 2u[i, j - 1, 1] - 4u[i, j, 1]) +\n                          a * u[i, j, 1]^2 / u[i, j, 2] + ubar - α * u[i, j, 1]\n        du[end, end, 2] = D2 * (2u[i - 1, j, 2] + 2u[i, j - 1, 2] - 4u[i, j, 2]) +\n                          a * u[i, j, 1]^2 - β * u[i, j, 2]\n    end\nend\nprob = DE.ODEProblem(fast_gm!, r0, (0.0, 0.1), p)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nNotice that in this case fusing the loops and avoiding the linear operators is a major improvement of about 10x! That's an order of magnitude faster than our original MATLAB/SciPy/R vectorized style code!\n\nSince this is tedious to do by hand, we note that ModelingToolkit.jl's symbolic code generation can do this automatically from the basic version:\n\nimport ModelingToolkit as MTK\nfunction basic_version!(dr, r, p, t)\n    a, α, ubar, β, D1, D2 = p\n    u = r[:, :, 1]\n    v = r[:, :, 2]\n    Du = D1 * (Ay * u + u * Ax)\n    Dv = D2 * (Ay * v + v * Ax)\n    dr[:, :, 1] = Du .+ a .* u .* u ./ v .+ ubar .- α * u\n    dr[:, :, 2] = Dv .+ a .* u .* u .- β * v\nend\n\na, α, ubar, β, D1, D2 = p\nuss = (ubar + β) / α\nvss = (a / β) * uss^2\nr0 = zeros(100, 100, 2)\nr0[:, :, 1] .= uss .+ 0.1 .* rand.()\nr0[:, :, 2] .= vss\n\nprob = DE.ODEProblem(basic_version!, r0, (0.0, 0.1), p)\nde = MTK.complete(MTK.modelingtoolkitize(prob))\n\n# Note jac=true,sparse=true makes it automatically build sparse Jacobian code\n# as well!\n\nfastprob = DE.ODEProblem(de, [], (0.0, 0.1); jac = true, sparse = true)\n\nLastly, we can do other things like multithread the main loops. LoopVectorization.jl provides the @turbo macro for doing a lot of SIMD enhancements, and @tturbo is the multithreaded version.","category":"section"},{"location":"tutorials/faster_ode_example/#Optimizing-Algorithm-Choices","page":"Code Optimization for Differential Equations","title":"Optimizing Algorithm Choices","text":"The last thing to do is then ***optimize our algorithm choice***. We have been using DE.Tsit5() as our test algorithm, but in reality this problem is a stiff PDE discretization and thus one recommendation is to use Sundials.CVODE_BDF(). However, instead of using the default dense Jacobian, we should make use of the sparse Jacobian afforded by the problem. The Jacobian is the matrix fracdf_idr_j, where r is read by the linear index (i.e. down columns). But since the u variables depend on the v, the band size here is large, and thus this will not do well with a Banded Jacobian solver. Instead, we utilize sparse Jacobian algorithms. Sundials.CVODE_BDF allows us to use a sparse Newton-Krylov solver by setting linear_solver = :GMRES.\n\nnote: Note\nThe Solving Large Stiff Equations tutorial goes through these details. This is simply to give a taste of how much optimization opportunity is left on the table!\n\nLet's see how our fast right-hand side scales as we increase the integration time.\n\nprob = DE.ODEProblem(fast_gm!, r0, (0.0, 10.0), p)\nBT.@btime DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nimport Sundials\nBT.@btime DE.solve(prob, Sundials.CVODE_BDF(; linear_solver = :GMRES));\nnothing # hide\n\nprob = DE.ODEProblem(fast_gm!, r0, (0.0, 100.0), p)\n# Will go out of memory if we don't turn off `save_everystep`!\nBT.@btime DE.solve(prob, DE.Tsit5(); save_everystep = false);\nnothing # hide\n\nBT.@btime DE.solve(prob, Sundials.CVODE_BDF(; linear_solver = :GMRES); save_everystep = false);\nnothing # hide\n\nprob = DE.ODEProblem(fast_gm!, r0, (0.0, 500.0), p)\nBT.@btime DE.solve(prob, Sundials.CVODE_BDF(; linear_solver = :GMRES); save_everystep = false);\nnothing # hide\n\nNotice that we've eliminated almost all allocations, allowing the code to grow without hitting garbage collection and slowing down.\n\nWhy is Sundials.CVODE_BDF doing well? What's happening is that, because the problem is stiff, the number of steps required by the explicit Runge-Kutta method grows rapidly, whereas Sundials.CVODE_BDF is taking large steps. Additionally, the GMRES linear solver form is quite an efficient way to solve the implicit system in this case. This is problem-dependent, and in many cases using a Krylov method effectively requires a preconditioner, so you need to play around with testing other algorithms and linear solvers to find out what works best with your problem.\n\nNow continue to the Solving Large Stiff Equations tutorial for more details on optimizing the algorithm choice for such codes.","category":"section"},{"location":"basics/compatibility_chart/#Solver-Compatibility-Chart","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"This chart is for documenting the compatibility of the component solver packages to the common interface. An x means that the option is implemented or the add-on functionality will work with the given solver. A blank means that the option has not been implemented, or that a given add-on has not been tested with a given package. If there are any errors in this chart, please file an issue or submit a pull-request.\n\nOption OrdinaryDiffEq.jl Sundials.jl ODE.jl ODEInterface.jl LSODA.jl StochasticDiffEq.jl DelayDiffEq.jl DASKR.jl DASSL.jl\nNonlinear Dense (continuous) output x x    x x x \nTolerance control x x x x x x x x x\nAdvanced stepsize control x 0  x 0 x x 0 \nMass Matrices^ x 0  x 0 x x 0 \nAnalytical Jacobians^† x x  x  x x x \nGeneral Performance Overloads^† x 0  0 0 x x 0 \ninternalnorm x 0 x 0 0 x x 0 \nInitial dt x x x x  x x x \nsave_everystep x x x x x x x x \nsaveat x x x x x x x x \ntstops x x  0  x x x \nd_discontinuities x   0  x x  \nisoutofdomain x  x   x x  \nAllows reverse time direction x x x x x x x  \nUnitful numbers x 0  0 0  x 0 \nArbitrary dimension arrays x x x x x x x x x\nComplex numbers p     x p  \nArbitrary precision x 0 x 0 0 x x 0 x\nApproxFun types x 0  0 0  x 0 \nProgress monitoring x     x x  \nIntegrator interface x x  0  x x  \nResizability x 0  0 0 x x 0 \nCache iterator x 0  0 0 x x 0 \nCan choose linear solvers x s    x x s x\nCan choose nonlinear solvers x 0  0 0 x x 0 x\nCan use out of place natively x 0 x 0 0 x x 0 x\nCan use inplace natively x x  x x x x x \nCompatible with DiffEqDevTools x x x x x x x x \nCompatible with ParameterizedFunctions x x x x x x x x \nContinuous Callbacks x x  x  x x  x\nDiscrete Callbacks x x  x  x x  \nMonte Carlo Simulations x x x x x x x x \nParameter Estimation x n n n n x x n x\nParameter Sensitivity Analysis x x x x x  x  \nPlotting and solution handling x x x x x x x x x\n\nx: Full compatibility\np: Partial compatibility, only in nonstiff methods, unless the Jacobian is provided.\nn: General compatibility, but not compatible with routines which require being able to autodifferentiate through the entire solver.\n0: Not possible. This is generally due to underlying inflexibility in a wrapped library.\ns: Special, Sundials has its own linear solver choices.\n^: Only stiff (implicit) methods.\n†: For packages with compatibility, no warning is given when a specific algorithm does not need to use this feature.\n\nAll blank spaces are possible future additions.","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#OrdinaryDiffEqIMEXMultistep","page":"OrdinaryDiffEqIMEXMultistep","title":"OrdinaryDiffEqIMEXMultistep","text":"Standard low-order IMEX (Implicit-Explicit) multistep methods for problems that can be split into stiff and non-stiff components. These are widely used classical methods in partial differential equation applications, providing simple and reliable IMEX integration with moderate accuracy requirements.","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Key-Properties","page":"OrdinaryDiffEqIMEXMultistep","title":"Key Properties","text":"IMEX Multistep methods provide:\n\nStandard IMEX formulations commonly used in PDE applications\nLow-order accuracy (typically 2nd order) with good stability\nSimple implementation and well-understood behavior\nExplicit treatment of non-stiff terms with implicit handling of stiff components\nFixed timestep requirements due to multistep nature\nEfficient for large-scale problems where splitting is natural","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#When-to-Use-IMEX-Multistep-Methods","page":"OrdinaryDiffEqIMEXMultistep","title":"When to Use IMEX Multistep Methods","text":"These methods are recommended for:\n\nClassical PDE applications where standard IMEX methods are established\nReaction-diffusion systems with natural explicit/implicit splitting\nConvection-diffusion problems where convection is explicit and diffusion implicit\nLarge-scale spatial discretizations where simple, efficient methods are preferred\nApplications prioritizing robustness over high-order accuracy\nProblems with natural operator splitting methodology","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Mathematical-Background","page":"OrdinaryDiffEqIMEXMultistep","title":"Mathematical Background","text":"IMEX multistep methods treat the split system: du/dt = f₁(u,t) + f₂(u,t)\n\nusing:\n\nImplicit multistep schemes (like Crank-Nicolson) for stiff terms f₁\nExplicit multistep schemes (like Adams-Bashforth) for non-stiff terms f₂\n\nThis combination provides stability for stiff components while maintaining efficiency for non-stiff parts.","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Problem-Splitting-Requirements","page":"OrdinaryDiffEqIMEXMultistep","title":"Problem Splitting Requirements","text":"These methods require a SplitODEProblem where:\n\nFirst function f₁ contains stiff terms requiring implicit treatment\nSecond function f₂ contains non-stiff terms suitable for explicit treatment\nSplitting should align with the natural time scale separation\nLinear stiff terms work particularly well with these methods","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Solver-Selection-Guide","page":"OrdinaryDiffEqIMEXMultistep","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Available-Methods","page":"OrdinaryDiffEqIMEXMultistep","title":"Available Methods","text":"CNAB2: Recommended - Crank-Nicolson Adams-Bashforth 2nd order method\nCNLF2: Crank-Nicolson Leap-Frog 2nd order method","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Method-characteristics","page":"OrdinaryDiffEqIMEXMultistep","title":"Method characteristics","text":"CNAB2: Most commonly used, good stability and accuracy balance\nCNLF2: Alternative formulation, may have different stability properties","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Performance-Guidelines","page":"OrdinaryDiffEqIMEXMultistep","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#When-IMEX-Multistep-methods-excel","page":"OrdinaryDiffEqIMEXMultistep","title":"When IMEX Multistep methods excel","text":"PDE problems with established IMEX splitting practices\nLarge spatial discretizations where method efficiency matters more than high accuracy\nProblems with linear stiff terms that are efficiently handled implicitly\nApplications requiring consistent timesteps (no adaptive timestepping)\nWell-conditioned problems where simple methods suffice","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Splitting-strategy-considerations","page":"OrdinaryDiffEqIMEXMultistep","title":"Splitting strategy considerations","text":"Linear diffusion terms → implicit component (f₁)\nNonlinear convection/reaction → explicit component (f₂) if not too stiff\nSource terms → choose based on stiffness characteristics\nBoundary conditions → often naturally handled in implicit component","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Limitations-and-Considerations","page":"OrdinaryDiffEqIMEXMultistep","title":"Limitations and Considerations","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Method-limitations","page":"OrdinaryDiffEqIMEXMultistep","title":"Method limitations","text":"Fixed timestep required - no adaptive timestepping capabilities\nLow order only - maximum 2nd order accuracy\nStartup procedures needed for multistep methods\nLimited stability analysis compared to modern IMEX-RK methods","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#When-to-consider-alternatives","page":"OrdinaryDiffEqIMEXMultistep","title":"When to consider alternatives","text":"Higher accuracy needs: Use IMEX-RK or higher-order IMEX methods\nAdaptive timestepping: Use IMEX-RK or ARK methods\nComplex stability requirements: Use more sophisticated IMEX schemes\nVery stiff problems: Consider fully implicit methods","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Alternative-Approaches","page":"OrdinaryDiffEqIMEXMultistep","title":"Alternative Approaches","text":"Consider these alternatives:\n\nIMEX Runge-Kutta methods for adaptive timestepping and higher order\nIMEX BDF methods for better stability properties and higher accuracy\nFully implicit methods if splitting is not beneficial\nExponential integrators for linear stiff problems","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Classical-Applications","page":"OrdinaryDiffEqIMEXMultistep","title":"Classical Applications","text":"These methods are standard in:\n\nComputational fluid dynamics for incompressible Navier-Stokes equations\nAtmospheric modeling for advection-diffusion-reaction systems\nOcean modeling for transport equations with diffusion\nAstrophysical simulations for multiphysics problems\n\nimex_first_steps = evalfile(\"./common_imex_first_steps.jl\")\nimex_first_steps(\"OrdinaryDiffEqIMEXMultistep\", \"CNAB2\")","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#Full-list-of-solvers","page":"OrdinaryDiffEqIMEXMultistep","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#OrdinaryDiffEqIMEXMultistep.CNAB2","page":"OrdinaryDiffEqIMEXMultistep","title":"OrdinaryDiffEqIMEXMultistep.CNAB2","text":"CNAB2()\n\nIMEX Multistep method. Crank-Nicolson Adams Bashforth Order 2 (fixed time step)\n\nKeyword Arguments\n\nReferences\n\n@article{jorgenson2014unconditional,     title={Unconditional stability of a Crank-Nicolson Adams-Bashforth 2 numerical method},     author={JORGENSON, ANDREW D},     journal={A (A- C)},     volume={1},     number={2},     pages={1},     year={2014}}     @article{he2010numerical,     title={Numerical implementation of the Crank–Nicolson/Adams–Bashforth scheme for the time-dependent Navier–Stokes equations},     author={He, Yinnian and Li, Jian},     journal={International journal for numerical methods in fluids},     volume={62},     number={6},     pages={647–659},     year={2010},     publisher={Wiley Online Library}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/imex/IMEXMultistep/#OrdinaryDiffEqIMEXMultistep.CNLF2","page":"OrdinaryDiffEqIMEXMultistep","title":"OrdinaryDiffEqIMEXMultistep.CNLF2","text":"CNLF2()\n\nIMEX Multistep method. Crank-Nicholson Leapfrong 2.\n\nKeyword Arguments\n\nReferences\n\n@article{han2020second,     title={A second order, linear, unconditionally stable, Crank–Nicolson–Leapfrog scheme for phase field models of two-phase incompressible flows},     author={Han, Daozhi and Jiang, Nan},     journal={Applied Mathematics Letters},     volume={108},     pages={106521},     year={2020},     publisher={Elsevier}}     @article{jiang2015crank,     title={A Crank–Nicolson Leapfrog stabilization: Unconditional stability and two applications},     author={Jiang, Nan and Kubacki, Michaela and Layton, William and Moraiti, Marina and Tran, Hoang},     journal={Journal of Computational and Applied Mathematics},     volume={281},     pages={263–276},     year={2015},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"types/dae_types/#DAE-Problems","page":"DAE Problems","title":"DAE Problems","text":"","category":"section"},{"location":"types/dae_types/#Solution-Type","page":"DAE Problems","title":"Solution Type","text":"","category":"section"},{"location":"types/dae_types/#Alias-Specifier","page":"DAE Problems","title":"Alias Specifier","text":"","category":"section"},{"location":"types/dae_types/#Example-Problems","page":"DAE Problems","title":"Example Problems","text":"Examples problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_dae_resrob, you can do something like:\n\nimport DiffEqProblemLibrary.DAEProblemLibrary, Sundials\nprob = DAEProblemLibrary.prob_dae_resrob\nsol = solve(prob, Sundials.IDA())","category":"section"},{"location":"types/dae_types/#SciMLBase.DAEProblem","page":"DAE Problems","title":"SciMLBase.DAEProblem","text":"Defines an implicit ordinary differential equation (ODE) or differential-algebraic equation (DAE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/dae_types/\n\nMathematical Specification of an DAE Problem\n\nTo define a DAE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\n0 = f(duupt)\n\nf should be specified as f(du,u,p,t) (or in-place as f(resid,du,u,p,t)). Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nProblem Type\n\nConstructors\n\nDAEProblem(f::DAEFunction,du0,u0,tspan,p=NullParameters();kwargs...)\nDAEProblem{isinplace,specialize}(f,du0,u0,tspan,p=NullParameters();kwargs...) : Defines the DAE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the documentation for more details. The default is AutoSpecialize.\n\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The function in the ODE.\ndu0: The initial condition for the derivative.\nu0: The initial condition.\ntspan: The timespan for the problem.\ndifferential_vars: A logical array which declares which variables are the differential (non-algebraic) vars (i.e. du' is in the equations for this variable). Defaults to nothing. Some solvers may require this be set if an initial condition needs to be determined.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\nExample Problems\n\nExamples problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_dae_resrob, you can do something like:\n\n#] add DAEProblemLibrary\nusing DAEProblemLibrary\nprob = DAEProblemLibrary.prob_dae_resrob\nsol = solve(prob,IDA())\n\n\n\n\n\n","category":"type"},{"location":"types/dae_types/#SciMLBase.DAEFunction","page":"DAE Problems","title":"SciMLBase.DAEFunction","text":"struct DAEFunction{iip, specialize, F, Ta, Tt, TJ, JVP, VJP, JP, SP, TW, TWt, TPJ, O, TCV, SYS, ID} <: SciMLBase.AbstractDAEFunction{iip}\n\nA representation of an implicit DAE function f, defined by:\n\n0 = fleft(fracdudtuptright)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDAEFunction{iip,specialize}(f;\n                           analytic = __has_analytic(f) ? f.analytic : nothing,\n                           jac = __has_jac(f) ? f.jac : nothing,\n                           jvp = __has_jvp(f) ? f.jvp : nothing,\n                           vjp = __has_vjp(f) ? f.vjp : nothing,\n                           jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                           sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                           colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n                           sys = __has_sys(f) ? f.sys : nothing)\n\nNote that only the function f itself is required. This function should be given as f!(out,du,u,p,t) or out = f(du,u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\njac(J,du,u,p,gamma,t) or J=jac(du,u,p,gamma,t): returns the implicit DAE Jacobian defined as γ fracdGd(du) + fracdGdu\njvp(Jv,v,du,u,p,gamma,t) or Jv=jvp(v,du,u,p,gamma,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,du,u,p,gamma,t) or Jv=vjp(v,du,u,p,gamma,t): returns the adjoint derivativefracdfdu^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DAEFunction type directly match the names of the inputs.\n\nExamples\n\nDeclaring Explicit Jacobians for DAEs\n\nFor fully implicit ODEs (DAEProblems), a slightly different Jacobian function is necessary. For the DAE\n\nG(duupt) = textres\n\nThe Jacobian should be given in the form gamma*dG/d(du) + dG/du where gamma is given by the solver. This means that the signature is:\n\nf(J,du,u,p,gamma,t)\n\nFor example, for the equation\n\nfunction testjac(res,du,u,p,t)\n  res[1] = du[1] - 2.0 * u[1] + 1.2 * u[1]*u[2]\n  res[2] = du[2] -3 * u[2] - u[1]*u[2]\nend\n\nwe would define the Jacobian as:\n\nfunction testjac(J,du,u,p,gamma,t)\n  J[1,1] = gamma - 2.0 + 1.2 * u[2]\n  J[1,2] = 1.2 * u[1]\n  J[2,1] = - 1 * u[2]\n  J[2,2] = gamma - 3 - u[1]\n  nothing\nend\n\nSymbolically Generating the Functions\n\nSee the modelingtoolkitize function from ModelingToolkit.jl for automatically symbolically generating the Jacobian and more from the numerically-defined functions.\n\n\n\n\n\n","category":"type"},{"location":"types/dae_types/#SciMLBase.DAESolution","page":"DAE Problems","title":"SciMLBase.DAESolution","text":"struct DAESolution{T, N, uType, duType, uType2, DType, tType, P, A, ID, S, rateType, V} <: SciMLBase.AbstractDAESolution{T, N, uType}\n\nRepresentation of the solution to an differential-algebraic equation defined by an DAEProblem.\n\nDESolution Interface\n\nFor more information on interacting with DESolution types, check out the Solution Handling page of the DifferentialEquations.jl documentation.\n\nhttps://docs.sciml.ai/DiffEqDocs/stable/basics/solution/\n\nFields\n\nu: the representation of the DAE solution. Given as an array of solutions, where u[i] corresponds to the solution at time t[i]. It is recommended in most cases one does not access sol.u directly and instead use the array interface described in the Solution Handling page of the DifferentialEquations.jl documentation.\ndu: the representation of the derivatives of the DAE solution.\nt: the time points corresponding to the saved values of the DAE solution.\nprob: the original DAEProblem that was solved.\nalg: the algorithm type used by the solver.\nstats: statistics of the solver, such as the number of function evaluations required, number of Jacobians computed, and more.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully, whether it terminated early due to a user-defined callback, or whether it exited due to an error. For more details, see the return code documentation.\n\n\n\n\n\n","category":"type"},{"location":"types/dae_types/#SciMLBase.DAEAliasSpecifier","page":"DAE Problems","title":"SciMLBase.DAEAliasSpecifier","text":"DAEAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = nothing, alias_du0 = nothing, alias_tstops = nothing, alias = nothing)\n\nHolds information on what variables to alias when solving a DAE. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false.\nalias_du0::Union{Bool, Nothing}: alias the du0 array for DAEs. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias::Union{Bool, Nothing}: sets all fields of the DAEAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"types/dae_types/#DAEProblemLibrary.prob_dae_resrob","page":"DAE Problems","title":"DAEProblemLibrary.prob_dae_resrob","text":"The Robertson biochemical reactions in DAE form\n\nbeginalign*\nfracdy₁dt = -k₁y₁+k₃y₂y₃ \nfracdy₂dt =  k₁y₁-k₂y₂^2-k₃y₂y₃ \n1 = y₁ + y₂ + y₃\nendalign*\n\nwhere k₁=004, k₂=310^7, k₃=10^4. For details, see: Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129 Usually solved on 010^11\n\n\n\n\n\n","category":"constant"},{"location":"api/ordinarydiffeq/usage/#Usage","page":"Usage","title":"Usage","text":"OrdinaryDiffEq.jl is part of the SciML common interface, but can be used independently of DifferentialEquations.jl. The only requirement is that the user passes an OrdinaryDiffEq.jl algorithm to solve. For example, we can solve the ODE tutorial from the docs using the Tsit5() algorithm:\n\nusing OrdinaryDiffEq\nf(u, p, t) = 1.01 * u\nu0 = 1 / 2\ntspan = (0.0, 1.0)\nprob = ODEProblem(f, u0, tspan)\nsol = solve(prob, Tsit5(), reltol = 1e-8, abstol = 1e-8)\nusing Plots\nplot(sol, linewidth = 5, title = \"Solution to the linear ODE with a thick line\",\n    xaxis = \"Time (t)\", yaxis = \"u(t) (in μm)\", label = \"My Thick Line!\") # legend=false\nplot!(sol.t, t -> 0.5 * exp(1.01 * t), lw = 3, ls = :dash, label = \"True Solution!\")\n\nThat example uses the out-of-place syntax f(u,p,t), while the in-place syntax (more efficient for systems of equations) is shown in the Lorenz example:\n\nusing OrdinaryDiffEq\nfunction lorenz(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 100.0)\nprob = ODEProblem(lorenz, u0, tspan)\nsol = solve(prob, Tsit5())\nusing Plots;\nplot(sol, vars = (1, 2, 3));\n\nVery fast static array versions can be specifically compiled to the size of your model. For example:\n\nusing OrdinaryDiffEq, StaticArrays\nfunction lorenz(u, p, t)\n    SA[10.0 * (u[2] - u[1]), u[1] * (28.0 - u[3]) - u[2], u[1] * u[2] - (8 / 3) * u[3]]\nend\nu0 = SA[1.0; 0.0; 0.0]\ntspan = (0.0, 100.0)\nprob = ODEProblem(lorenz, u0, tspan)\nsol = solve(prob, Tsit5())\n\nFor “refined ODEs”, like dynamical equations and SecondOrderODEProblems, refer to the DiffEqDocs. For example, in DiffEqTutorials.jl we show how to solve equations of motion using symplectic methods:\n\nfunction HH_acceleration(dv, v, u, p, t)\n    x, y = u\n    dx, dy = dv\n    dv[1] = -x - 2 * x * y\n    dv[2] = y^2 - y - x^2\nend\ninitial_positions = [0.0, 0.1]\ninitial_velocities = [0.5, 0.0]\nprob = SecondOrderODEProblem(HH_acceleration, initial_velocities, initial_positions, tspan)\nsol2 = solve(prob, KahanLi8(), dt = 1 / 10);\n\nOther refined forms are IMEX and semi-linear ODEs (for exponential integrators).","category":"section"},{"location":"api/ordinarydiffeq/usage/#Available-Solvers","page":"Usage","title":"Available Solvers","text":"For the list of available solvers, please refer to the DifferentialEquations.jl ODE Solvers, Dynamical ODE Solvers, and the Split ODE Solvers pages.","category":"section"},{"location":"basics/solution/#solution","page":"Solution Handling","title":"Solution Handling","text":"The solution is an RecursiveArrayTools.AbstractDiffEqArray. See RecursiveArrayTools.jl for more information on the interface. The following is a more DiffEq-centric explanation of the interface.","category":"section"},{"location":"basics/solution/#Accessing-the-Values","page":"Solution Handling","title":"Accessing the Values","text":"The solution type has a lot of built-in functionality to help analysis. For example, it has an array interface for accessing the values. Internally, the solution type has two important fields:\n\nu which holds the Vector of values at each timestep\nt which holds the times of each timestep.\n\nDifferent solution types may add extra information as necessary, such as the derivative at each timestep du or the spatial discretization x, y, etc.","category":"section"},{"location":"basics/solution/#Array-Interface","page":"Solution Handling","title":"Array Interface","text":"note: Note\nIn 2023 the linear indexing sol[i] was deprecated. It previously had the behavior that sol[i] = sol.u[i]. However, this is incompatible with standard AbstractArray interfaces, Since if A = VectorOfArray([[1,2],[3,4]]) and A is supposed to act like [1 3; 2 4], then there is a difference A[1] = [1,2] for the VectorOfArray while A[1] = 1 for the matrix. This causes many issues if AbstractVectorOfArray <: AbstractArray. Thus we plan in 2026 to complete the deprecation and thus have a breaking update where sol[i] matches the linear indexing of an AbstractArray, and then making AbstractVectorOfArray <: AbstractArray. Until then, AbstractVectorOfArray due to this interface break but manually implements an AbstractArray-like interface for future compatibility.\n\nThe general operations are as follows. Use\n\nsol.u[j]\n\nto access the value at timestep j (if the timeseries was saved), and\n\nsol.t[j]\n\nto access the value of t at timestep j. For multidimensional systems, this will address first by component and lastly by time, and thus\n\nsol[i, j]\n\nwill be the ith component at timestep j. Hence, sol[j][i] == sol[i, j]. This is done because Julia is column-major, so the leading dimension should be contiguous in memory. If the independent variables had shape (for example, was a matrix), then i is the linear index. We can also access solutions with shape:\n\nsol[i, k, j]\n\ngives the [i,k] component of the system at timestep j. The colon operator is supported, meaning that\n\nsol[i, :]\n\ngives the timeseries for the ith component.","category":"section"},{"location":"basics/solution/#Using-the-AbstractArray-Interface","page":"Solution Handling","title":"Using the AbstractArray Interface","text":"The AbstractArray interface can be directly used. For example, for a vector system of variables sol[i,j] is a matrix with rows being the variables and columns being the timepoints. Operations like sol' will transpose the solution type. Functionality written for AbstractArrays can directly use this. For example, the Base cov function computes correlations amongst columns, and thus:\n\ncov(sol)\n\ncomputes the correlation of the system state in time, whereas\n\ncov(sol, 2)\n\ncomputes the correlation between the variables. Similarly, mean(sol,2) is the mean of the variable in time, and var(sol,2) is the variance. Other statistical functions and packages which work on AbstractArray types will work on the solution type.\n\nAt anytime, a true Array can be created using Array(sol).","category":"section"},{"location":"basics/solution/#Interpolations-and-Calculating-Derivatives","page":"Solution Handling","title":"Interpolations and Calculating Derivatives","text":"If the solver allows for dense output and dense=true was set for the solving (which is the default), then we can access the approximate value at a time t using the command\n\nsol(t)\n\nNote that the interpolating function allows for t to be a vector and uses this to speed up the interpolation calculations. If t is an AbstractVector, then the returned object is a RecursiveArrayTools.DiffEqArray, see RecursiveArrayTools.jl for more information. Note that the differential equation solution object itself is an AbstractDiffEqArray, and this means that the returned object will have the same indexing behavior as a solution type itself, thus see the indexing description above.\n\nThe full API for the interpolations is:\n\nsol(t, deriv = Val{0}; idxs = nothing, continuity = :left)\n\nThe optional argument deriv lets you choose the number n derivative to solve the interpolation for, defaulting with n=0. (Note the implementation of this is per solver, most of the derivatives have implemented, but you might find some that are not. Open an issue if there's a specific one needed that you find is missing). continuity describes whether to satisfy left or right continuity when a discontinuity is saved. The default is :left, i.e. grab the value before the callback's change, but can be changed to :right. idxs allows you to choose the indices the interpolation should solve for. For example,\n\nsol(t, idxs = 1:2:5)\n\nwill return a Vector of length 3 which is the interpolated values at t for components 1, 3, and 5. idxs=nothing, the default, means it will return every component. In addition, we can do\n\nsol(t, idxs = 1)\n\nand it will return a Number for the interpolation of the single value. Note that this interpolation only computes the values which are requested, and thus it's much faster on large systems to use this rather than computing the full interpolation and using only a few values.\n\nIn addition, there is an inplace form:\n\nsol(out, t, deriv = Val{0}; idxs = nothing, continuity = :left)\n\nwhich will write the output to out. This allows one to use pre-allocated vectors for the output to improve the speed even more.","category":"section"},{"location":"basics/solution/#Comprehensions","page":"Solution Handling","title":"Comprehensions","text":"One can use the extra components of the solution object as well as using zip. For example, say the solution type holds du, the derivative at each timestep. One can comprehend over the values using:\n\n[t + 3u - du for (t, u, du) in zip(sol.t, sol.u, sol.du)]\n\nNote that the solution object acts as a vector in time, and so its length is the number of saved timepoints.","category":"section"},{"location":"basics/solution/#Special-Fields","page":"Solution Handling","title":"Special Fields","text":"The solution interface also includes some special fields. The problem object prob and the algorithm used to solve the problem alg are included in the solution. Additionally, the field dense is a boolean which states whether the interpolation functionality is available. Further, the field destats contains the internal statistics for the solution process, such as the number of linear solves and convergence failures. Lastly, there is a mutable state tslocation which controls the plot recipe behavior. By default, tslocation=0. Its values have different meanings between partial and ordinary differential equations:\n\ntslocation=0  for non-spatial problems (ODEs) means that the plot recipe will plot the full solution. tslocation=i means that it will only plot the timepoint i.\ntslocation=0 for spatial problems (PDEs) means the plot recipe will plot the final timepoint. tslocation=i means that the plot recipe will plot the ith timepoint.\n\nWhat this means is that for ODEs, the plots will default to the full plot and PDEs will default to plotting the surface at the final timepoint. The iterator interface simply iterates the value of tslocation, and the animate function iterates the solution calling solve at each step.","category":"section"},{"location":"basics/solution/#Differential-Equation-Solver-Statistics-(destats)","page":"Solution Handling","title":"Differential Equation Solver Statistics (destats)","text":"","category":"section"},{"location":"basics/solution/#retcodes","page":"Solution Handling","title":"Return Codes (RetCodes)","text":"The solution types have a retcode field which returns an enum value signifying the error state of the solution. Return codes are now implemented as an enum using EnumX.jl rather than symbols.\n\nTo check if a solution was successful, use:\n\nSciMLBase.successful_retcode(sol)\n\nwarning: Warning\nPrevious iterations of the interface suggested using sol.retcode == :Success, however, that is now not advised because there are more than one return code that can be interpreted as successful. For example, Terminated is a successful run to a manual termination, and would be missed if only checking for Success. Therefore we highly recommend you use SciMLBase.successful_retcode(sol) instead.\n\nThe return codes include are accessed via the ReturnCode module, i.e. SciMLBase.ReturnCode.Success. The following are major return codes to know:\n\nDefault: The solver did not set retcodes.\nSuccess: The integration completed without erroring or the steady state solver from SteadyStateDiffEq found the steady state.\nTerminated: The integration is terminated with terminate!(integrator). Note that this may occur by using TerminateSteadyState from the callback library DiffEqCallbacks.\nMaxIters: The integration exited early because it reached its maximum number of iterations.\nDtLessThanMin: The timestep method chose a stepsize which is smaller than the allowed minimum timestep, and exited early.\nUnstable: The solver detected that the solution was unstable and exited early.\nInitialFailure: The DAE solver could not find consistent initial conditions.\nConvergenceFailure: The internal implicit solvers failed to converge.\nFailure: General uncategorized failures or errors.\n\nFor a complete list of return codes and their properties, see the SciMLBase ReturnCode documentation.","category":"section"},{"location":"basics/solution/#Problem-Specific-Features","page":"Solution Handling","title":"Problem-Specific Features","text":"Extra fields for solutions of specific problems are specified in the appropriate problem definition page.","category":"section"},{"location":"basics/solution/#Solution-Function-Stripping","page":"Solution Handling","title":"Solution Function Stripping","text":"By default solution objects store functions, making them difficult to serialize. Using the function strip_solution(sol), a copy of the solution that does not contain any functions is created.","category":"section"},{"location":"basics/solution/#SciMLBase.DEStats","page":"Solution Handling","title":"SciMLBase.DEStats","text":"mutable struct DEStats\n\nStatistics from the differential equation solver about the solution process.\n\nFields\n\nnf: Number of function evaluations. If the differential equation is a split function, such as a SplitFunction for implicit-explicit (IMEX) integration, then nf is the number of function evaluations for the first function (the implicit function)\nnf2: If the differential equation is a split function, such as a SplitFunction for implicit-explicit (IMEX) integration, then nf2 is the number of function evaluations for the second function, i.e. the function treated explicitly. Otherwise it is zero.\nnw: The number of W=I-gamma*J (or W=I/gamma-J) matrices constructed during the solving process.\nnsolve: The number of linear solves W\\b required for the integration.\nnjacs: Number of Jacobians calculated during the integration.\nnnonliniter: Total number of iterations for the nonlinear solvers.\nnnonlinconvfail: Number of nonlinear solver convergence failures.\nncondition: Number of calls to the condition function for callbacks.\nnaccept: Number of accepted steps.\nnreject: Number of rejected steps.\nmaxeig: Maximum eigenvalue over the solution. This is only computed if the method is an auto-switching algorithm.\n\n\n\n\n\n","category":"type"},{"location":"basics/solution/#SciMLBase.strip_solution","page":"Solution Handling","title":"SciMLBase.strip_solution","text":"strip_solution(sol)\n\nStrips a SciMLSolution object and its interpolation of their functions to better accommodate serialization.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/#OrdinaryDiffEq.jl","page":"OrdinaryDiffEq.jl: ODE solvers and utilities","title":"OrdinaryDiffEq.jl","text":"OrdinaryDiffEq.jl is a component package in the DifferentialEquations ecosystem. It holds the core ordinary differential equation solvers and utilities. While completely independent and usable on its own, users interested in using this functionality should check out DifferentialEquations.jl.","category":"section"},{"location":"api/ordinarydiffeq/#Installation","page":"OrdinaryDiffEq.jl: ODE solvers and utilities","title":"Installation","text":"Assuming that you already have Julia correctly installed, it suffices to import OrdinaryDiffEq.jl in the standard way:\n\nimport Pkg;\nPkg.add(\"OrdinaryDiffEq\");","category":"section"},{"location":"types/sde_types/#SDE-Problems","page":"SDE Problems","title":"SDE Problems","text":"","category":"section"},{"location":"types/sde_types/#Solution-Type","page":"SDE Problems","title":"Solution Type","text":"SDEProblem solutions return an RODESolution. For more information, see the RODE problem definition page for the RODESolution docstring.","category":"section"},{"location":"types/sde_types/#Alias-Specifier","page":"SDE Problems","title":"Alias Specifier","text":"","category":"section"},{"location":"types/sde_types/#Example-Problems","page":"SDE Problems","title":"Example Problems","text":"Examples problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_sde_linear, you can do something like:\n\n#] add DiffEqProblemLibrary\nimport DiffEqProblemLibrary.SDEProblemLibrary\nimport StochasticDiffEq as SDE\n# load problems\nSDEProblemLibrary.importsdeproblems()\nprob = SDEProblemLibrary.prob_sde_linear\nsol = SDE.solve(prob)","category":"section"},{"location":"types/sde_types/#SciMLBase.SDEProblem","page":"SDE Problems","title":"SciMLBase.SDEProblem","text":"Defines an stochastic differential equation (SDE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/sde_types/\n\nMathematical Specification of a SDE Problem\n\nTo define an SDE Problem, you simply need to give the forcing function f, the noise function g, and the initial condition u₀ which define an SDE:\n\ndu = f(upt)  dt + ᵢ gᵢ(upt)  dWⁱ\n\nf and g should be specified as f(u,p,t) and  g(u,p,t) respectively, and u₀ should be an AbstractArray whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. A vector of gs can also be defined to determine an SDE of higher Ito dimension.\n\nProblem Type\n\nWraps the data which defines an SDE problem\n\nu = f(upt)  dt + ᵢ gᵢ(upt)  dWⁱ\n\nwith initial condition u0.\n\nConstructors\n\nSDEProblem(f::SDEFunction,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing)\nSDEProblem{isinplace,specialize}(f,g,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing) : Defines the SDE with the specified functions. The default noise is WHITE_NOISE. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nParameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The drift function in the SDE.\ng: The noise function in the SDE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The optional parameters for the problem. Defaults to NullParameters.\nnoise: The noise process applied to the noise upon generation. Defaults to Gaussian white noise. For information on defining different noise processes, see the noise process documentation page.\nnoise_rate_prototype: A prototype type instance for the noise rates, that is the output g. It can be any type which overloads A_mul_B! with itself being the middle argument. Commonly, this is a matrix or sparse matrix. If this is not given, it defaults to nothing, which means the problem should be interpreted as having diagonal noise.\nkwargs: The keyword arguments passed onto the solves.\n\nExample Problems\n\nExamples problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_sde_linear, you can do something like:\n\n#] add SDEProblemLibrary\nusing SDEProblemLibrary\nprob = SDEProblemLibrary.prob_sde_linear\nsol = solve(prob)\n\n\n\n\n\n","category":"type"},{"location":"types/sde_types/#SciMLBase.SDEFunction","page":"SDE Problems","title":"SciMLBase.SDEFunction","text":"struct SDEFunction{iip, specialize, F, G, TMM, Ta, Tt, TJ, JVP, VJP, JP, SP, TW, TWt, TPJ, GG, O, TCV, SYS, ID} <: SciMLBase.AbstractSDEFunction{iip}\n\nA representation of an SDE function f, defined by:\n\nM  du = f(upt)  dt + g(upt)  dW\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nSDEFunction{iip,specialize}(f,g;\n                           mass_matrix = __has_mass_matrix(f) ? f.mass_matrix : I,\n                           analytic = __has_analytic(f) ? f.analytic : nothing,\n                           tgrad= __has_tgrad(f) ? f.tgrad : nothing,\n                           jac = __has_jac(f) ? f.jac : nothing,\n                           jvp = __has_jvp(f) ? f.jvp : nothing,\n                           vjp = __has_vjp(f) ? f.vjp : nothing,\n                           ggprime = nothing,\n                           jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                           sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                           paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n                           colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n                           sys = __has_sys(f) ? f.sys : nothing)\n\nNote that both the function f and g are required. This function should be given as f!(du,u,p,t) or du = f(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://docs.sciml.ai/DiffEqDocs/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracf(upt)t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivative fracdfdu v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivative fracdfdu^ v\nggprime(J,u,p,t) or J = ggprime(u,p,t): returns the Milstein derivative fracdg(upt)du g(upt)\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the ODEFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"types/sde_types/#SciMLBase.SDEAliasSpecifier","page":"SDE Problems","title":"SciMLBase.SDEAliasSpecifier","text":"SDEAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = nothing, alias_tstops = nothing, alias = nothing)\n\nHolds information on what variables to alias when solving an SDEProblem. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias_jumps::Union{Bool, Nothing}: alias jump process if wrapped in a JumpProcess.\nalias::Union{Bool, Nothing}: sets all fields of the SDEAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_linear","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_linear","text":"du_t = αudt + βudW_t\n\nwhere α=101, β=087, and initial condition u_0=12, with solution\n\nu(u_0ptW_t) = u_0 expleft(left(α - tfrac12β^2right) t + βW_tright)\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_2Dlinear","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_2Dlinear","text":"8 linear SDEs (as a 4×2 matrix):\n\ndu_t = αudt + βudW_t\n\nwhere α=101, β=087, and initial condition u_0=frac12 with solution\n\nu(u_0ptW_t) = u_0 expleft(left(α - tfrac12β^2right) t + βW_tright)\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_wave","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_wave","text":"du_t = -frac1100sin(u)cos^3(u) dt + frac110cos^2(u_t) dW_t\n\nand initial condition u_0=1 with solution\n\nu(u_0ptW_t) = arctanleft(fracW_t10 + tan(u_0)right)\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_lorenz","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_lorenz","text":"Lorenz Attractor with additive noise\n\nbeginalign*\ndx = σ(y-x)  dt + α  dW_t \ndy = (x(ρ-z) - y)  dt + α  dW_t \ndz = (xy - βz)  dt + α  dW_t\nendalign*\n\nwith σ=10, ρ=28, β=83, α=30 and initial condition u_0=111.\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_cubic","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_cubic","text":"du_t = frac14u(1-u^2)  dt + frac12(1-u^2)  dW_t\n\nand initial condition u_0=frac12, with solution\n\nu(u0ptW_t) = frac(1+u_0) exp(W_t) + u_0 - 1(1+u_0) exp(W_t) + 1 - u_0\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_additive","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_additive","text":"Additive noise problem\n\nu_t = left(fracβsqrt1+t - frac12(1+t)u_tright)  dt + fracαβsqrt1+t  dW_t\n\nand initial condition u_0=1 with α=01 and β=005, with solution\n\nu(u_0ptW_t)=fracu_0sqrt1+t + fracβ(t+αW_t)sqrt1+t\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_additivesystem","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_additivesystem","text":"A multiple dimension extension of additiveSDEExample\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_nltest","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_nltest","text":"Runge–Kutta methods for numerical solution of stochastic differential equations Tocino and Ardanuy\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.oval2ModelExample","page":"SDE Problems","title":"SDEProblemLibrary.oval2ModelExample","text":"oval2ModelExample(;largeFluctuations=false,useBigs=false,noiseLevel=1)\n\nA function which generates the Oval2 Epithelial-Mesenchymal Transition model from:\n\nRackauckas, C., & Nie, Q. (2017). Adaptive methods for stochastic differential equations via natural embeddings and rejection sampling with memory. Discrete and continuous dynamical systems. Series B, 22(7), 2731.\n\n19 SDEs which are only stiff during transitions between biological states.\n\n\n\n\n\n","category":"function"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_stiffquadstrat","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_stiffquadstrat","text":"The composite Euler method for stiff stochastic differential equations\n\nKevin Burrage, Tianhai Tian\n\nAnd\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nStiffness of Euler is determined by α+β²<1 Higher α or β is stiff, with α being deterministic stiffness and β being noise stiffness (and grows by square).\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_stiffquadito","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_stiffquadito","text":"The composite Euler method for stiff stochastic differential equations\n\nKevin Burrage, Tianhai Tian\n\nAnd\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nStiffness of Euler is determined by α+β²<1 Higher α or β is stiff, with α being deterministic stiffness and β being noise stiffness (and grows by square).\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.generate_stiff_stoch_heat","page":"SDE Problems","title":"SDEProblemLibrary.generate_stiff_stoch_heat","text":"Stochastic Heat Equation with scalar multiplicative noise\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nRaising D or k increases stiffness\n\n\n\n\n\n","category":"function"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_bistable","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_bistable","text":"Bistable chemical reaction network with a semi-stable lower state.\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_bruss","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_bruss","text":"Stochastic Brusselator\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#SDEProblemLibrary.prob_sde_oscilreact","page":"SDE Problems","title":"SDEProblemLibrary.prob_sde_oscilreact","text":"An oscillatory chemical reaction system\n\nChemical oscillator with hill function regulation. The system has 7 species: X, Y, Z (main oscillatory species), R (regulator), S (substrate), SP (single phosphorylated),  SP2 (double phosphorylated).\n\nParameters: p1=0.01, p2=3.0, p3=3.0, p4=4.5, p5=2.0, p6=15.0, p7=20.0, p8=0.005, p9=0.01, p10=0.05 Initial conditions: [X=200.0, Y=60.0, Z=120.0, R=100.0, S=50.0, SP=50.0, SP2=50.0]\n\n\n\n\n\n","category":"constant"},{"location":"types/nonautonomous_linear_ode/#nonauto_dynamical_prob","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Non-autonomous linear ODEs show up in a lot of scientific problems where the differential equation lives on a manifold, such as a Lie Group. In these situations, specialized solvers can be utilized to enforce physical bounds on the solution and enhance the solving.","category":"section"},{"location":"types/nonautonomous_linear_ode/#Mathematical-Specification-of-a-Non-autonomous-Linear-ODE","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Mathematical Specification of a Non-autonomous Linear ODE","text":"These algorithms require a Non-autonomous linear ODE of the form:\n\nu^prime = A(upt)u\n\nWhere A is an AbstractSciMLOperator (see SciMLOperators.jl for more information) that is multiplied against u. Many algorithms specialize on the form of A, such as A being a constant or A being only time-dependent (A(t)).","category":"section"},{"location":"types/nonautonomous_linear_ode/#Construction","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Construction","text":"Creating a non-autonomous linear ODE is the same as an ODEProblem, except f is represented by an AbstractSciMLOperator (note: this means that any standard ODE solver can also be applied to problems written in this form). As an example:\n\nfunction update_func(A, u, p, t)\n    A[1, 1] = cos(t)\n    A[2, 1] = sin(t)\n    A[1, 2] = -sin(t)\n    A[2, 2] = cos(t)\nend\n\nimport SciMLOperators\nA = SciMLOperators.MatrixOperator(ones(2, 2), update_func! = update_func)\nprob = ODEProblem(A, ones(2), (10, 50.0))\n\ndefines a quasi-linear ODE u^prime = A(t)u where the components of A are the given functions. Using that formulation, we can see that the general form is u^prime = A(upt)u, for example:\n\nfunction update_func(A, u, p, t)\n    A[1, 1] = 0\n    A[2, 1] = 1\n    A[1, 2] = -2 * (1 - cos(u[2]) - u[2] * sin(u[2]))\n    A[2, 2] = 0\nend\n\nhas a state-dependent linear operator. Note that many other AbstractSciMLOperators can be used, and MatrixOperator is just one version that represents A via a matrix (other choices are matrix-free). See the SciMLOperators.jl documentation for more information.\n\nNote that if A is a constant, then it is sufficient to supply A directly without an update_func.","category":"section"},{"location":"types/nonautonomous_linear_ode/#Note-About-Affine-Equations","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Note About Affine Equations","text":"Note that the affine equation\n\nu^prime = A(upt)u + g(upt)\n\ncan be written as a linear form by extending the size of the system by one to have a constant term of 1. This is done by extending A with a new row, containing only zeros, and giving this new state an initial value of 1. Then extend A to have a new column containing the values of g(u,p,t). In this way, these types of equations can be handled by these specialized integrators.","category":"section"},{"location":"features/diffeq_operator/#Matrix-Free-Linear-Operators-with-SciMLOperators.jl","page":"Matrix-Free Linear Operators with SciMLOperators.jl","title":"Matrix-Free Linear Operators with SciMLOperators.jl","text":"SciML has the SciMLOperators.jl library for defining linear operators. The ODE solvers will specialize on this property, for example using the linear operators automatically with Newton-Krylov methods for matrix-free Newton-Krylov optimizations, but also allows for methods that require knowing that part of the equation is linear, like exponential integrators. See the SciMLOperators.jl library for more information.","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#OrdinaryDiffEqStabilizedIRK","page":"OrdinaryDiffEqStabilizedIRK","title":"OrdinaryDiffEqStabilizedIRK","text":"Stabilized Implicit Runge-Kutta (IMEX) methods combine the benefits of stabilized explicit methods with implicit treatment of problematic eigenvalues. These IMEX schemes are designed for problems where the Jacobian has both large real eigenvalues (suitable for explicit stabilized methods) and large complex eigenvalues (requiring implicit treatment).","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Key-Properties","page":"OrdinaryDiffEqStabilizedIRK","title":"Key Properties","text":"Stabilized IRK methods provide:\n\nIMEX formulation treating different stiffness components appropriately\nLarge stability regions for real eigenvalues via explicit stabilized schemes\nImplicit treatment of complex eigenvalues for unconditional stability\nEfficient handling of mixed stiffness characteristics\nSplitting-based approach requiring SplitODEProblem formulation","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#When-to-Use-Stabilized-IRK-Methods","page":"OrdinaryDiffEqStabilizedIRK","title":"When to Use Stabilized IRK Methods","text":"These methods are recommended for:\n\nMixed stiffness problems with both real and complex eigenvalues\nParabolic PDEs with convection where diffusion and advection have different scales\nReaction-diffusion systems with stiff reactions and moderate diffusion\nProblems where pure explicit stabilized methods fail due to complex eigenvalues\nLarge-scale systems where full implicit methods are too expensive","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Mathematical-Background","page":"OrdinaryDiffEqStabilizedIRK","title":"Mathematical Background","text":"Standard stabilized explicit methods (like RKC, ROCK) achieve large stability regions along the negative real axis but struggle with complex eigenvalues. Stabilized IRK methods address this by:\n\nExplicit stabilized treatment for large real eigenvalues\nImplicit treatment for complex eigenvalues\nIMEX coupling to maintain overall stability and accuracy","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Problem-Splitting-Requirements","page":"OrdinaryDiffEqStabilizedIRK","title":"Problem Splitting Requirements","text":"These methods require a SplitODEProblem where:\n\nFirst component contains terms with large real eigenvalues (explicit treatment)\nSecond component contains terms with complex eigenvalues (implicit treatment)\nSplitting design is crucial for method performance","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Spectral-Radius-Estimation","page":"OrdinaryDiffEqStabilizedIRK","title":"Spectral Radius Estimation","text":"Users can supply an upper bound on the spectral radius:\n\neigen_est = (integrator) -> integrator.eigen_est = upper_bound\n\nThis bound applies to the explicit component of the split problem.","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqStabilizedIRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Available-methods","page":"OrdinaryDiffEqStabilizedIRK","title":"Available methods","text":"IRKC: Implicit Runge-Kutta-Chebyshev method for mixed stiffness problems","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Usage-considerations","page":"OrdinaryDiffEqStabilizedIRK","title":"Usage considerations","text":"Requires careful splitting of the problem components\nSpectral radius estimation needed for explicit component\nTest splitting strategies for optimal performance\nCompare with pure implicit or explicit stabilized alternatives","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Performance-Guidelines","page":"OrdinaryDiffEqStabilizedIRK","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#When-IMEX-stabilized-methods-excel","page":"OrdinaryDiffEqStabilizedIRK","title":"When IMEX stabilized methods excel","text":"Mixed eigenvalue distribution (both real and complex)\nModerate to large systems where splitting is natural\nProblems where neither pure explicit nor implicit methods are ideal","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Splitting-strategy-considerations","page":"OrdinaryDiffEqStabilizedIRK","title":"Splitting strategy considerations","text":"Identify dominant eigenvalue types in different terms\nReal-dominated terms → explicit component\nComplex-dominated terms → implicit component\nTest different splittings for best performance","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Alternative-Approaches","page":"OrdinaryDiffEqStabilizedIRK","title":"Alternative Approaches","text":"Consider these alternatives:\n\nPure implicit methods (BDF, SDIRK, Rosenbrock) for highly stiff problems\nExplicit stabilized methods (ROCK, RKC) if complex eigenvalues are small\nStandard IMEX methods for natural explicit/implicit splitting\n\nimex_first_steps = evalfile(\"./common_imex_first_steps.jl\")\nimex_first_steps(\"OrdinaryDiffEqStabilizedIRK\", \"IRKC\")","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#Full-list-of-solvers","page":"OrdinaryDiffEqStabilizedIRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/StabilizedIRK/#OrdinaryDiffEqStabilizedIRK.IRKC","page":"OrdinaryDiffEqStabilizedIRK","title":"OrdinaryDiffEqStabilizedIRK.IRKC","text":"IRKC(; eigen_est = nothing)\n\nStabilized Implicit Runge Kutta method. Implicit Runge-Kutta-Chebyshev method.\n\nKeyword Arguments\n\neigen_est: function of the form   (integrator) -> integrator.eigen_est = upper_bound,   where upper_bound is an estimated upper bound on the spectral radius of the Jacobian matrix.   If eigen_est is not provided, upper_bound will be estimated using the power iteration.\n\nReferences\n\nREF TBD\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#OrdinaryDiffEqNordsieck","page":"OrdinaryDiffEqNordsieck","title":"OrdinaryDiffEqNordsieck","text":"Nordsieck form multistep methods represent an alternative approach to traditional multistep algorithms. Instead of storing past solution values, these methods maintain a vector of scaled derivatives (similar to Taylor series coefficients) to advance the solution. This representation was pioneered in classic codes like LSODE, VODE, and CVODE.\n\nwarning: Research and Development\nThese methods are currently in research and development and not intended for general use.","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Key-Properties","page":"OrdinaryDiffEqNordsieck","title":"Key Properties","text":"Nordsieck methods provide:\n\nDerivative-based representation instead of solution history\nImproved restartability after discontinuities using derivative information\nVariable order and stepsize capabilities\nAlternative to history-based multistep methods\nResearch and experimental implementations","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#When-to-Use-Nordsieck-Methods","page":"OrdinaryDiffEqNordsieck","title":"When to Use Nordsieck Methods","text":"These methods are recommended for:\n\nResearch applications exploring alternative multistep representations\nProblems with discontinuities where restartability is important\nExperimental comparisons with traditional multistep methods\nDevelopment of discontinuity-aware algorithms","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Important-Limitations","page":"OrdinaryDiffEqNordsieck","title":"Important Limitations","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Experimental-Status","page":"OrdinaryDiffEqNordsieck","title":"Experimental Status","text":"Considered experimental and inferior to modern BDF implementations\nGenerally recommend FBDF instead for production use\nMaintained for research purposes and future development\nNumerical instabilities can arise from higher derivative representations","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Performance-Considerations","page":"OrdinaryDiffEqNordsieck","title":"Performance Considerations","text":"Less robust than fixed-leading coefficient BDF methods\nHigher computational overhead for derivative maintenance\nPotential stability issues with derivative representations","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Mathematical-Background","page":"OrdinaryDiffEqNordsieck","title":"Mathematical Background","text":"The Nordsieck form represents the solution using scaled derivatives: y_n = [y, h*y', h²*y''/2!, h³*y'''/3!, ...]\n\nThis representation allows reconstruction of the solution and its derivatives, enabling restarts after discontinuities without losing accuracy.","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Solver-Selection-Guide","page":"OrdinaryDiffEqNordsieck","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Nordsieck-implementations","page":"OrdinaryDiffEqNordsieck","title":"Nordsieck implementations","text":"AN5: Fifth-order Adams method with fixed leading coefficient\nJVODE: Variable order Adams/BDF method (experimental LSODE-style)\nJVODE_Adams: JVODE configured for Adams methods\nJVODE_BDF: JVODE configured for BDF methods","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Recommended-alternatives","page":"OrdinaryDiffEqNordsieck","title":"Recommended alternatives","text":"For most applications: Use QNDF or FBDF instead\nFor stiff problems: Prefer modern BDF implementations\nFor research: These methods are appropriate for experimental work","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Research-and-Development","page":"OrdinaryDiffEqNordsieck","title":"Research and Development","text":"These implementations serve as:\n\nExperimental testbed for Nordsieck form algorithms\nResearch platform for discontinuity-aware methods\nDevelopment basis for future improved BDF implementations\nEducational examples of alternative multistep representations","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Usage-Guidelines","page":"OrdinaryDiffEqNordsieck","title":"Usage Guidelines","text":"Not recommended for production applications\nUse FBDF or QNDF for reliable multistep integration\nConsider these methods only for research or experimental purposes\nExpect potentially lower performance compared to modern alternatives\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqNordsieck\", \"AN5\")","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#Full-list-of-solvers","page":"OrdinaryDiffEqNordsieck","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#OrdinaryDiffEqNordsieck.AN5","page":"OrdinaryDiffEqNordsieck","title":"OrdinaryDiffEqNordsieck.AN5","text":"AN5()\n\nAdaptive step size Adams explicit Method An adaptive 5th order fixed-leading coefficient Adams method in Nordsieck form.\n\nwarning: Experimental\nAN5 is experimental, the solver VCABM is generally preferred.\n\nKeyword Arguments\n\nReferences\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#OrdinaryDiffEqNordsieck.JVODE","page":"OrdinaryDiffEqNordsieck","title":"OrdinaryDiffEqNordsieck.JVODE","text":"warning: Experimental\nJVODE is experimental, the solver VCABM is generally preferred.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#OrdinaryDiffEqNordsieck.JVODE_Adams","page":"OrdinaryDiffEqNordsieck","title":"OrdinaryDiffEqNordsieck.JVODE_Adams","text":"warning: Experimental\nJVODE is experimental, the solver VCABM is generally preferred.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/implicit/Nordsieck/#OrdinaryDiffEqNordsieck.JVODE_BDF","page":"OrdinaryDiffEqNordsieck","title":"OrdinaryDiffEqNordsieck.JVODE_BDF","text":"warning: Experimental\nJVODE is experimental, the solver FBDF is generally preferred.\n\n\n\n\n\n","category":"function"},{"location":"examples/beeler_reuter/#An-Implicit/Explicit-CUDA-Accelerated-Solver-for-the-2D-Beeler-Reuter-Model","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","text":"","category":"section"},{"location":"examples/beeler_reuter/#Background","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"Background","text":"SciML is a suite of optimized Julia libraries to solve ordinary differential equations (ODE). SciML provides many explicit and implicit solvers suited for different types of ODE problems. It is possible to reduce a system of partial differential equations into an ODE problem by employing the method of lines (MOL). The essence of MOL is to discretize the spatial derivatives (by finite difference, finite volume or finite element methods) into algebraic equations and to keep the time derivatives as is. The resulting differential equations are left with only one independent variable (time) and can be solved with an ODE solver. Solving Systems of Stochastic PDEs and using GPUs in Julia is a brief introduction to MOL and using GPUs to accelerate PDE solving in JuliaDiffEq. Here we expand on this introduction by developing an implicit/explicit (IMEX) solver for a 2D cardiac electrophysiology model and show how to use CUDA libraries to run the explicit part of the model on a GPU.\n\nNote that this tutorial does not use the higher order IMEX methods built into DifferentialEquations.jl, but instead shows how to hand-split an equation when the explicit portion has an analytical solution (or approximate), which is common in many scenarios.\n\nThere are hundreds of ionic models that describe cardiac electrical activity in various degrees of detail. Most are based on the classic Hodgkin-Huxley model and define the time-evolution of different state variables in the form of nonlinear first-order ODEs. The state vector for these models includes the transmembrane potential, gating variables, and ionic concentrations. The coupling between cells is through the transmembrane potential only and is described as a reaction-diffusion equation, which is a parabolic PDE,\n\nfracpartial Vpartial t = nabla (D  nabla V) - frac I_textion C_m\n\nwhere V is the transmembrane potential, D is a diffusion tensor, I_textion is the sum of the transmembrane currents and is calculated from the ODEs, and C_m is the membrane capacitance, usually assumed to be constant. Here, we model a uniform and isotropic medium. Therefore, the model can be simplified to,\n\nfracpartial Vpartial t = D nabla^2 V - frac I_textion C_m\n\nwhere D is now a scalar. By nature, these models have to deal with different time scales and are therefore classified as stiff. Commonly, they are solved using the explicit Euler method, typically with a closed form for the integration of the gating variables (the Rush-Larsen method, see below). We can also solve these problems using implicit or semi-implicit PDE solvers (e.g., the Crank-Nicholson method combined with an iterative solver). Higher order explicit methods such as Runge-Kutta and linear multistep methods cannot overcome the stiffness and are not particularly helpful.\n\nIn this tutorial, we first develop a CPU-only IMEX solver and then show how to move the explicit part to a GPU.","category":"section"},{"location":"examples/beeler_reuter/#The-Beeler-Reuter-Model","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"The Beeler-Reuter Model","text":"We have chosen the Beeler-Reuter ventricular ionic model as our example. It is a classic model first described in 1977 and is used as a base for many other ionic models. It has eight state variables, which makes it complicated enough to be interesting without obscuring the main points of the exercise. The eight state variables are: the transmembrane potential (V), sodium-channel activation and inactivation gates (m and h, similar to the Hodgkin-Huxley model), with an additional slow inactivation gate (j), calcium-channel activation and deactivations gates (d and f), a time-dependent inward-rectifying potassium current gate (x_1), and intracellular calcium concentration (c). There are four currents: a sodium current (i_textNa), a calcium current (i_textCa), and two potassium currents, one time-dependent (i_x_1) and one background time-independent (i_K_1).","category":"section"},{"location":"examples/beeler_reuter/#CPU-Only-Beeler-Reuter-Solver","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"CPU-Only Beeler-Reuter Solver","text":"Let's start by developing a CPU only IMEX solver. The main idea is to use the DifferentialEquations framework to handle the implicit part of the equation and code the analytical approximation for the explicit part separately. If no analytical approximation was known for the explicit part, one could use methods from this list.\n\nFirst, we define the model constants:\n\nconst v0 = -84.624\nconst v1 = 10.0\nconst C_K1 = 1.0f0\nconst C_x1 = 1.0f0\nconst C_Na = 1.0f0\nconst C_s = 1.0f0\nconst D_Ca = 0.0f0\nconst D_Na = 0.0f0\nconst g_s = 0.09f0\nconst g_Na = 4.0f0\nconst g_NaC = 0.005f0\nconst ENa = 50.0f0 + D_Na\nconst γ = 0.5f0\nconst C_m = 1.0f0\n\nNote that the constants are defined as Float32 and not Float64. The reason is that most GPUs have many more single precision cores than double precision ones. To ensure uniformity between CPU and GPU, we also code most states variables as Float32 except for the transmembrane potential, which is solved by an implicit solver provided by the Sundial library and needs to be Float64.","category":"section"},{"location":"examples/beeler_reuter/#The-State-Structure","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"The State Structure","text":"Next, we define a struct to contain our state. BeelerReuterCpu is a functor, and we will define a deriv function as its associated function.\n\nmutable struct BeelerReuterCpu\n    t::Float64              # the last timestep time to calculate Δt\n    diff_coef::Float64      # the diffusion-coefficient (coupling strength)\n\n    C::Array{Float32, 2}    # intracellular calcium concentration\n    M::Array{Float32, 2}    # sodium current activation gate (m)\n    H::Array{Float32, 2}    # sodium current inactivation gate (h)\n    J::Array{Float32, 2}    # sodium current slow inactivation gate (j)\n    D::Array{Float32, 2}    # calcium current activation gate (d)\n    F::Array{Float32, 2}    # calcium current inactivation gate (f)\n    XI::Array{Float32, 2}   # inward-rectifying potassium current (iK1)\n\n    Δu::Array{Float64, 2}   # place-holder for the Laplacian\n\n    function BeelerReuterCpu(u0, diff_coef)\n        self = new()\n\n        ny, nx = size(u0)\n        self.t = 0.0\n        self.diff_coef = diff_coef\n\n        self.C = fill(0.0001f0, (ny, nx))\n        self.M = fill(0.01f0, (ny, nx))\n        self.H = fill(0.988f0, (ny, nx))\n        self.J = fill(0.975f0, (ny, nx))\n        self.D = fill(0.003f0, (ny, nx))\n        self.F = fill(0.994f0, (ny, nx))\n        self.XI = fill(0.0001f0, (ny, nx))\n\n        self.Δu = zeros(ny, nx)\n\n        return self\n    end\nend","category":"section"},{"location":"examples/beeler_reuter/#Laplacian","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"Laplacian","text":"The finite-difference Laplacian is calculated in-place by a 5-point stencil. The Neumann boundary condition is enforced.\n\nnote: Note\nFor more complex PDE discretizations, consider using MethodOfLines.jl which can automatically generate finite difference discretizations, or SciMLOperators.jl for defining matrix-free linear operators.\n\n# 5-point stencil\nfunction laplacian(Δu, u)\n    n1, n2 = size(u)\n\n    # internal nodes\n    for j in 2:(n2 - 1)\n        for i in 2:(n1 - 1)\n            @inbounds Δu[i, j] = u[i + 1, j] + u[i - 1, j] + u[i, j + 1] + u[i, j - 1] -\n                                 4 * u[i, j]\n        end\n    end\n\n    # left/right edges\n    for i in 2:(n1 - 1)\n        @inbounds Δu[i, 1] = u[i + 1, 1] + u[i - 1, 1] + 2 * u[i, 2] - 4 * u[i, 1]\n        @inbounds Δu[i, n2] = u[i + 1, n2] + u[i - 1, n2] + 2 * u[i, n2 - 1] - 4 * u[i, n2]\n    end\n\n    # top/bottom edges\n    for j in 2:(n2 - 1)\n        @inbounds Δu[1, j] = u[1, j + 1] + u[1, j - 1] + 2 * u[2, j] - 4 * u[1, j]\n        @inbounds Δu[n1, j] = u[n1, j + 1] + u[n1, j - 1] + 2 * u[n1 - 1, j] - 4 * u[n1, j]\n    end\n\n    # corners\n    @inbounds Δu[1, 1] = 2 * (u[2, 1] + u[1, 2]) - 4 * u[1, 1]\n    @inbounds Δu[n1, 1] = 2 * (u[n1 - 1, 1] + u[n1, 2]) - 4 * u[n1, 1]\n    @inbounds Δu[1, n2] = 2 * (u[2, n2] + u[1, n2 - 1]) - 4 * u[1, n2]\n    @inbounds Δu[n1, n2] = 2 * (u[n1 - 1, n2] + u[n1, n2 - 1]) - 4 * u[n1, n2]\nend","category":"section"},{"location":"examples/beeler_reuter/#The-Rush-Larsen-Method","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"The Rush-Larsen Method","text":"We use an explicit solver for all the state variables except for the transmembrane potential, which is solved with the help of an implicit solver. The explicit solver is a domain-specific exponential method, the Rush-Larsen method. This method utilizes an approximation on the model in order to transform the IMEX equation into a form suitable for an implicit ODE solver. This combination of implicit and explicit methods forms a specialized IMEX solver. For general IMEX integration, please see the IMEX solvers documentation. While we could have used the general model to solve the current problem, for this specific model, the transformation approach is more efficient and is of practical interest.\n\nThe Rush-Larsen method replaces the explicit Euler integration for the gating variables with direct integration. The starting point is the general ODE for the gating variables in Hodgkin-Huxley style ODEs,\n\nfracdgdt = (1 - g) alpha(V) - g beta(V)\n\nwhere g is a generic gating variable, ranging from 0 to 1, and α and β are reaction rates. This equation can be written as,\n\nfracdgdt = fracg_infty - gtau_g\n\nwhere g_ and τ_g are\n\ng_infty = fracalphaalpha + beta\n\nand,\n\ntau_g = frac1alpha + beta\n\nAssuming that g_ and τ_g are constant for the duration of a single time step (Δt), which is a reasonable assumption for most cardiac models, we can integrate directly to have,\n\ng(t + Δt) = g_ - left(g_ - g(Δt)right)e^-Δtτ_g\n\nThis is the Rush-Larsen technique. Note that as Δt  0, this equation morphs into the explicit Euler formula,\n\ng(t + Δt) = g(t) + Δt fracdgdt\n\nrush_larsen is a helper function that use the Rush-Larsen method to integrate the gating variables.\n\n@inline function rush_larsen(g, α, β, Δt)\n    inf = α / (α + β)\n    τ = 1.0f0 / (α + β)\n    return clamp(g + (g - inf) * expm1(-Δt / τ), 0.0f0, 1.0f0)\nend\n\nThe gating variables are updated as below. The details of how to calculate α and β are based on the Beeler-Reuter model and not of direct interest to this tutorial.\n\nfunction update_M_cpu(g, v, Δt)\n    # the condition is needed here to prevent NaN when v == 47.0\n    α = isapprox(v, 47.0f0) ? 10.0f0 : -(v + 47.0f0) / (exp(-0.1f0 * (v + 47.0f0)) - 1.0f0)\n    β = (40.0f0 * exp(-0.056f0 * (v + 72.0f0)))\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_H_cpu(g, v, Δt)\n    α = 0.126f0 * exp(-0.25f0 * (v + 77.0f0))\n    β = 1.7f0 / (exp(-0.082f0 * (v + 22.5f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_J_cpu(g, v, Δt)\n    α = (0.55f0 * exp(-0.25f0 * (v + 78.0f0))) / (exp(-0.2f0 * (v + 78.0f0)) + 1.0f0)\n    β = 0.3f0 / (exp(-0.1f0 * (v + 32.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_D_cpu(g, v, Δt)\n    α = γ * (0.095f0 * exp(-0.01f0 * (v - 5.0f0))) / (exp(-0.072f0 * (v - 5.0f0)) + 1.0f0)\n    β = γ * (0.07f0 * exp(-0.017f0 * (v + 44.0f0))) / (exp(0.05f0 * (v + 44.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_F_cpu(g, v, Δt)\n    α = γ * (0.012f0 * exp(-0.008f0 * (v + 28.0f0))) / (exp(0.15f0 * (v + 28.0f0)) + 1.0f0)\n    β = γ * (0.0065f0 * exp(-0.02f0 * (v + 30.0f0))) / (exp(-0.2f0 * (v + 30.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_XI_cpu(g, v, Δt)\n    α = (0.0005f0 * exp(0.083f0 * (v + 50.0f0))) / (exp(0.057f0 * (v + 50.0f0)) + 1.0f0)\n    β = (0.0013f0 * exp(-0.06f0 * (v + 20.0f0))) / (exp(-0.04f0 * (v + 20.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nThe intracellular calcium is not technically a gating variable, but we can use a similar explicit exponential integrator for it.\n\nfunction update_C_cpu(g, d, f, v, Δt)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * log(g)\n    kCa = C_s * g_s * d * f\n    iCa = kCa * (v - ECa)\n    inf = 1.0f-7 * (0.07f0 - g)\n    τ = 1.0f0 / 0.07f0\n    return g + (g - inf) * expm1(-Δt / τ)\nend","category":"section"},{"location":"examples/beeler_reuter/#Implicit-Solver","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"Implicit Solver","text":"Now, it is time to define the derivative function as an associated function of BeelerReuterCpu. We plan to use the CVODE_BDF solver as our implicit portion. Similar to other iterative methods, it calls the deriv function with the same t multiple times. For example, these are consecutive ts from a representative run:\n\n0.86830\n0.86830\n0.85485\n0.85485\n0.85485\n0.86359\n0.86359\n0.86359\n0.87233\n0.87233\n0.87233\n0.88598\n...\n\nHere, every time step is called three times. We distinguish between two types of calls to the deriv function. When t changes, the gating variables are updated by calling update_gates_cpu:\n\nfunction update_gates_cpu(u, XI, M, H, J, D, F, C, Δt)\n    let Δt = Float32(Δt)\n        n1, n2 = size(u)\n        for j in 1:n2\n            for i in 1:n1\n                v = Float32(u[i, j])\n\n                XI[i, j] = update_XI_cpu(XI[i, j], v, Δt)\n                M[i, j] = update_M_cpu(M[i, j], v, Δt)\n                H[i, j] = update_H_cpu(H[i, j], v, Δt)\n                J[i, j] = update_J_cpu(J[i, j], v, Δt)\n                D[i, j] = update_D_cpu(D[i, j], v, Δt)\n                F[i, j] = update_F_cpu(F[i, j], v, Δt)\n\n                C[i, j] = update_C_cpu(C[i, j], D[i, j], F[i, j], v, Δt)\n            end\n        end\n    end\nend\n\nOn the other hand, du is updated at each time step, since it is independent of Δt.\n\n# iK1 is the inward-rectifying potassium current\nfunction calc_iK1(v)\n    ea = exp(0.04f0 * (v + 85.0f0))\n    eb = exp(0.08f0 * (v + 53.0f0))\n    ec = exp(0.04f0 * (v + 53.0f0))\n    ed = exp(-0.04f0 * (v + 23.0f0))\n    return 0.35f0 * (4.0f0 * (ea - 1.0f0) / (eb + ec)\n            +\n            0.2f0 * (isapprox(v, -23.0f0) ? 25.0f0 : (v + 23.0f0) / (1.0f0 - ed)))\nend\n\n# ix1 is the time-independent background potassium current\nfunction calc_ix1(v, xi)\n    ea = exp(0.04f0 * (v + 77.0f0))\n    eb = exp(0.04f0 * (v + 35.0f0))\n    return xi * 0.8f0 * (ea - 1.0f0) / eb\nend\n\n# iNa is the sodium current (similar to the classic Hodgkin-Huxley model)\nfunction calc_iNa(v, m, h, j)\n    return C_Na * (g_Na * m^3 * h * j + g_NaC) * (v - ENa)\nend\n\n# iCa is the calcium current\nfunction calc_iCa(v, d, f, c)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * log(c)    # ECa is the calcium reversal potential\n    return C_s * g_s * d * f * (v - ECa)\nend\n\nfunction update_du_cpu(du, u, XI, M, H, J, D, F, C)\n    n1, n2 = size(u)\n\n    for j in 1:n2\n        for i in 1:n1\n            v = Float32(u[i, j])\n\n            # calculating individual currents\n            iK1 = calc_iK1(v)\n            ix1 = calc_ix1(v, XI[i, j])\n            iNa = calc_iNa(v, M[i, j], H[i, j], J[i, j])\n            iCa = calc_iCa(v, D[i, j], F[i, j], C[i, j])\n\n            # total current\n            I_sum = iK1 + ix1 + iNa + iCa\n\n            # the reaction part of the reaction-diffusion equation\n            du[i, j] = -I_sum / C_m\n        end\n    end\nend\n\nFinally, we put everything together is our deriv function, which is a call on BeelerReuterCpu.\n\nfunction (f::BeelerReuterCpu)(du, u, p, t)\n    Δt = t - f.t\n\n    if Δt != 0 || t == 0\n        update_gates_cpu(u, f.XI, f.M, f.H, f.J, f.D, f.F, f.C, Δt)\n        f.t = t\n    end\n\n    laplacian(f.Δu, u)\n\n    # calculate the reaction portion\n    update_du_cpu(du, u, f.XI, f.M, f.H, f.J, f.D, f.F, f.C)\n\n    # ...add the diffusion portion\n    du .+= f.diff_coef .* f.Δu\nend","category":"section"},{"location":"examples/beeler_reuter/#Results","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"Results","text":"Time to test! We need to define the starting transmembrane potential with the help of global constants v0 and v1, which represent the resting and activated potentials.\n\nconst N = 192;\nu0 = fill(v0, (N, N));\nu0[90:102, 90:102] .= v1;   # a small square in the middle of the domain\n\nThe initial condition is a small square in the middle of the domain.\n\nimport Plots\nPlots.heatmap(u0)\n\nNext, the problem is defined:\n\nimport DifferentialEquations as DE, Sundials\n\nderiv_cpu = BeelerReuterCpu(u0, 1.0);\nprob = DE.ODEProblem(deriv_cpu, u0, (0.0, 50.0));\n\nFor stiff reaction-diffusion equations, CVODE_BDF from Sundial library is an excellent solver.\n\n@time sol = DE.solve(prob, Sundials.CVODE_BDF(linear_solver = :GMRES), saveat = 100.0);\n\nPlots.heatmap(sol.u[end])","category":"section"},{"location":"examples/beeler_reuter/#CPU/GPU-Beeler-Reuter-Solver","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"CPU/GPU Beeler-Reuter Solver","text":"GPUs are great for embarrassingly parallel problems, but not so much for highly coupled models. We plan to keep the implicit part on CPU and run the decoupled explicit code on a GPU with the help of the CUDAnative library.","category":"section"},{"location":"examples/beeler_reuter/#GPUs-and-CUDA","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"GPUs and CUDA","text":"It this section, we present a brief summary of how GPUs (specifically NVIDIA GPUs) work and how to program them using the Julia CUDA interface. The readers who are familiar with these basic concepts may skip this section.\n\nLet's start by looking at the hardware of a typical high-end GPU, GTX 1080. It has four Graphics Processing Clusters (equivalent to a discrete CPU), each harboring five Streaming Multiprocessor (similar to a CPU core). Each SM has 128 single-precision CUDA cores. Therefore, GTX 1080 has a total of 4 x 5 x 128 = 2560 CUDA cores. The maximum  theoretical throughput for a GTX 1080 is reported as 8.87 TFLOPS. This figure is calculated for a boost clock frequency of 1.733 MHz as 2 x 2560 x 1.733 MHz = 8.87 TFLOPS. The factor 2 is included because two single floating-point operations, a multiplication and an addition, can be done in a clock cycle as part of a fused-multiply-addition FMA operation. GTX 1080 also has 8192 MB of global memory accessible to all the cores (in addition to local and shared memory on each SM).\n\nA typical CUDA application has the following flow:\n\nDefine and initialize the problem domain tensors (multidimensional arrays) in CPU memory.\nAllocate corresponding tensors in the GPU global memory.\nTransfer the input tensors from CPU to the corresponding GPU tensors.\nInvoke CUDA kernels (i.e., the GPU functions callable from CPU) that operate on the GPU tensors.\nTransfer the result tensors from GPU back to CPU.\nProcess tensors on CPU.\nRepeat steps 3-6 as needed.\n\nSome libraries, such as ArrayFire, hide the complexities of steps 2-5 behind a higher level of abstraction. However, here we take a lower level route. By using CUDA, we achieve a finer-grained control and higher performance. In return, we need to implement each step manually.\n\nCuArray is a thin abstraction layer over the CUDA API and allows us to define GPU-side tensors and copy data to and from them, but does not provide for operations on tensors. CUDAnative is a compiler that translates Julia functions designated as CUDA kernels into ptx (a high-level CUDA assembly language).","category":"section"},{"location":"examples/beeler_reuter/#The-CUDA-Code","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"The CUDA Code","text":"The key to fast CUDA programs is to minimize CPU/GPU memory transfers and global memory accesses. The implicit solver is currently CPU only, but it only requires access to the transmembrane potential. The rest of state variables reside on the GPU memory.\n\nWe modify BeelerReuterCpu into BeelerReuterGpu by defining the state variables as CuArrays instead of standard Julia Arrays. The name of each variable defined on the GPU is prefixed by d_ for clarity. Note that Δv is a temporary storage for the Laplacian and stays on the CPU side.\n\nimport CUDA\n\nmutable struct BeelerReuterGpu <: Function\n    t::Float64                  # the last timestep time to calculate Δt\n    diff_coef::Float64          # the diffusion-coefficient (coupling strength)\n\n    d_C::CuArray{Float32, 2}    # intracellular calcium concentration\n    d_M::CuArray{Float32, 2}    # sodium current activation gate (m)\n    d_H::CuArray{Float32, 2}    # sodium current inactivation gate (h)\n    d_J::CuArray{Float32, 2}    # sodium current slow inactivation gate (j)\n    d_D::CuArray{Float32, 2}    # calcium current activation gate (d)\n    d_F::CuArray{Float32, 2}    # calcium current inactivation gate (f)\n    d_XI::CuArray{Float32, 2}   # inward-rectifying potassium current (iK1)\n\n    d_u::CuArray{Float64, 2}    # place-holder for u in the device memory\n    d_du::CuArray{Float64, 2}   # place-holder for d_u in the device memory\n\n    Δv::Array{Float64, 2}       # place-holder for voltage gradient\n\n    function BeelerReuterGpu(u0, diff_coef)\n        self = new()\n\n        ny, nx = size(u0)\n        @assert (nx % 16 == 0) && (ny % 16 == 0)\n        self.t = 0.0\n        self.diff_coef = diff_coef\n\n        self.d_C = CuArray(fill(0.0001f0, (ny, nx)))\n        self.d_M = CuArray(fill(0.01f0, (ny, nx)))\n        self.d_H = CuArray(fill(0.988f0, (ny, nx)))\n        self.d_J = CuArray(fill(0.975f0, (ny, nx)))\n        self.d_D = CuArray(fill(0.003f0, (ny, nx)))\n        self.d_F = CuArray(fill(0.994f0, (ny, nx)))\n        self.d_XI = CuArray(fill(0.0001f0, (ny, nx)))\n\n        self.d_u = CuArray(u0)\n        self.d_du = CuArray(zeros(ny, nx))\n\n        self.Δv = zeros(ny, nx)\n\n        return self\n    end\nend\n\nThe Laplacian function remains unchanged. The main change to the explicit gating solvers is that exp and expm1 functions are prefixed by CUDAnative.. This is a technical nuisance that will hopefully be resolved in the future.\n\nfunction rush_larsen_gpu(g, α, β, Δt)\n    inf = α / (α + β)\n    τ = 1.0 / (α + β)\n    return clamp(g + (g - inf) * CUDAnative.expm1(-Δt / τ), 0.0f0, 1.0f0)\nend\n\nfunction update_M_gpu(g, v, Δt)\n    # the condition is needed here to prevent NaN when v == 47.0\n    α = isapprox(v, 47.0f0) ? 10.0f0 :\n        -(v + 47.0f0) / (CUDAnative.exp(-0.1f0 * (v + 47.0f0)) - 1.0f0)\n    β = (40.0f0 * CUDAnative.exp(-0.056f0 * (v + 72.0f0)))\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_H_gpu(g, v, Δt)\n    α = 0.126f0 * CUDAnative.exp(-0.25f0 * (v + 77.0f0))\n    β = 1.7f0 / (CUDAnative.exp(-0.082f0 * (v + 22.5f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_J_gpu(g, v, Δt)\n    α = (0.55f0 * CUDAnative.exp(-0.25f0 * (v + 78.0f0))) /\n        (CUDAnative.exp(-0.2f0 * (v + 78.0f0)) + 1.0f0)\n    β = 0.3f0 / (CUDAnative.exp(-0.1f0 * (v + 32.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_D_gpu(g, v, Δt)\n    α = γ * (0.095f0 * CUDAnative.exp(-0.01f0 * (v - 5.0f0))) /\n        (CUDAnative.exp(-0.072f0 * (v - 5.0f0)) + 1.0f0)\n    β = γ * (0.07f0 * CUDAnative.exp(-0.017f0 * (v + 44.0f0))) /\n        (CUDAnative.exp(0.05f0 * (v + 44.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_F_gpu(g, v, Δt)\n    α = γ * (0.012f0 * CUDAnative.exp(-0.008f0 * (v + 28.0f0))) /\n        (CUDAnative.exp(0.15f0 * (v + 28.0f0)) + 1.0f0)\n    β = γ * (0.0065f0 * CUDAnative.exp(-0.02f0 * (v + 30.0f0))) /\n        (CUDAnative.exp(-0.2f0 * (v + 30.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_XI_gpu(g, v, Δt)\n    α = (0.0005f0 * CUDAnative.exp(0.083f0 * (v + 50.0f0))) /\n        (CUDAnative.exp(0.057f0 * (v + 50.0f0)) + 1.0f0)\n    β = (0.0013f0 * CUDAnative.exp(-0.06f0 * (v + 20.0f0))) /\n        (CUDAnative.exp(-0.04f0 * (v + 20.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_C_gpu(c, d, f, v, Δt)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * CUDAnative.log(c)\n    kCa = C_s * g_s * d * f\n    iCa = kCa * (v - ECa)\n    inf = 1.0f-7 * (0.07f0 - c)\n    τ = 1.0f0 / 0.07f0\n    return c + (c - inf) * CUDAnative.expm1(-Δt / τ)\nend\n\nSimilarly, we modify the functions to calculate the individual currents by adding CUDAnative prefix.\n\n# iK1 is the inward-rectifying potassium current\nfunction calc_iK1(v)\n    ea = CUDAnative.exp(0.04f0 * (v + 85.0f0))\n    eb = CUDAnative.exp(0.08f0 * (v + 53.0f0))\n    ec = CUDAnative.exp(0.04f0 * (v + 53.0f0))\n    ed = CUDAnative.exp(-0.04f0 * (v + 23.0f0))\n    return 0.35f0 * (4.0f0 * (ea - 1.0f0) / (eb + ec)\n            +\n            0.2f0 * (isapprox(v, -23.0f0) ? 25.0f0 : (v + 23.0f0) / (1.0f0 - ed)))\nend\n\n# ix1 is the time-independent background potassium current\nfunction calc_ix1(v, xi)\n    ea = CUDAnative.exp(0.04f0 * (v + 77.0f0))\n    eb = CUDAnative.exp(0.04f0 * (v + 35.0f0))\n    return xi * 0.8f0 * (ea - 1.0f0) / eb\nend\n\n# iNa is the sodium current (similar to the classic Hodgkin-Huxley model)\nfunction calc_iNa(v, m, h, j)\n    return C_Na * (g_Na * m^3 * h * j + g_NaC) * (v - ENa)\nend\n\n# iCa is the calcium current\nfunction calc_iCa(v, d, f, c)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * CUDAnative.log(c)    # ECa is the calcium reversal potential\n    return C_s * g_s * d * f * (v - ECa)\nend","category":"section"},{"location":"examples/beeler_reuter/#CUDA-Kernels","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"CUDA Kernels","text":"A CUDA program does not directly deal with GPCs and SMs. The logical view of a CUDA program is in the term of blocks and threads. We have to specify the number of block and threads when running a CUDA kernel. Each thread runs on a single CUDA core. Threads are logically bundled into blocks, which are in turn specified on a grid. The grid stands for the entirety of the domain of interest.\n\nEach thread can find its logical coordinate by using few pre-defined indexing variables (threadIdx, blockIdx, blockDim and gridDim) in C/C++ and the corresponding functions (e.g., threadIdx()) in Julia. Their variables and functions are defined automatically for each thread and may return a different value depending on the calling thread. The return value of these functions is a 1-, 2-, or 3-dimensional structure whose elements can be accessed as .x, .y, and .z (for a 1-dimensional case, .x reports the actual index and .y and .z simply return 1). For example, if we deploy a kernel in 128 blocks and with 256 threads per block, each thread will see\n\n    gridDim.x = 128;\n    blockDim=256;\n\nwhile blockIdx.x ranges from 0 to 127 in C/C++ and 1 to 128 in Julia. Similarly, threadIdx.x will be between 0 and 255 in C/C++ (of course, in Julia the range will be 1 to 256).\n\nA C/C++ thread can calculate its index as\n\n    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\nIn Julia, we have to take into account base 1. Therefore, we use the following formula\n\n    idx = (blockIdx().x-UInt32(1)) * blockDim().x + threadIdx().x\n\nA CUDA programmer is free to interpret the calculated index however it fits the application, but in practice, it is usually interpreted as an index into input tensors.\n\nIn the GPU version of the solver, each thread works on a single element of the medium, indexed by a (x,y) pair. update_gates_gpu and update_du_gpu are very similar to their CPU counterparts but are in fact CUDA kernels where the for loops are replaced with CUDA-specific indexing. Note that CUDA kernels cannot return a valve; hence, nothing at the end.\n\nfunction update_gates_gpu(u, XI, M, H, J, D, F, C, Δt)\n    i = (blockIdx().x - UInt32(1)) * blockDim().x + threadIdx().x\n    j = (blockIdx().y - UInt32(1)) * blockDim().y + threadIdx().y\n\n    v = Float32(u[i, j])\n\n    let Δt = Float32(Δt)\n        XI[i, j] = update_XI_gpu(XI[i, j], v, Δt)\n        M[i, j] = update_M_gpu(M[i, j], v, Δt)\n        H[i, j] = update_H_gpu(H[i, j], v, Δt)\n        J[i, j] = update_J_gpu(J[i, j], v, Δt)\n        D[i, j] = update_D_gpu(D[i, j], v, Δt)\n        F[i, j] = update_F_gpu(F[i, j], v, Δt)\n\n        C[i, j] = update_C_gpu(C[i, j], D[i, j], F[i, j], v, Δt)\n    end\n    nothing\nend\n\nfunction update_du_gpu(du, u, XI, M, H, J, D, F, C)\n    i = (blockIdx().x - UInt32(1)) * blockDim().x + threadIdx().x\n    j = (blockIdx().y - UInt32(1)) * blockDim().y + threadIdx().y\n\n    v = Float32(u[i, j])\n\n    # calculating individual currents\n    iK1 = calc_iK1(v)\n    ix1 = calc_ix1(v, XI[i, j])\n    iNa = calc_iNa(v, M[i, j], H[i, j], J[i, j])\n    iCa = calc_iCa(v, D[i, j], F[i, j], C[i, j])\n\n    # total current\n    I_sum = iK1 + ix1 + iNa + iCa\n\n    # the reaction part of the reaction-diffusion equation\n    du[i, j] = -I_sum / C_m\n    nothing\nend","category":"section"},{"location":"examples/beeler_reuter/#Implicit-Solver-2","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"Implicit Solver","text":"Finally, the deriv function is modified to copy u to GPU and copy du back and to invoke CUDA kernels.\n\nfunction (f::BeelerReuterGpu)(du, u, p, t)\n    L = 16   # block size\n    Δt = t - f.t\n    copyto!(f.d_u, u)\n    ny, nx = size(u)\n\n    if Δt != 0 || t == 0\n        @cuda blocks=(ny÷L, nx÷L) threads=(L, L) update_gates_gpu(f.d_u, f.d_XI, f.d_M,\n            f.d_H, f.d_J, f.d_D,\n            f.d_F, f.d_C, Δt)\n        f.t = t\n    end\n\n    laplacian(f.Δv, u)\n\n    # calculate the reaction portion\n    @cuda blocks=(ny÷L, nx÷L) threads=(L, L) update_du_gpu(\n        f.d_du, f.d_u, f.d_XI, f.d_M,\n        f.d_H, f.d_J, f.d_D, f.d_F,\n        f.d_C)\n\n    copyto!(du, f.d_du)\n\n    # ...add the diffusion portion\n    du .+= f.diff_coef .* f.Δv\nend\n\nReady to test!\n\nimport DifferentialEquations as DE, Sundials\n\nderiv_gpu = BeelerReuterGpu(u0, 1.0);\nprob = DE.ODEProblem(deriv_gpu, u0, (0.0, 50.0));\n@time sol = DE.solve(prob, Sundials.CVODE_BDF(linear_solver = :GMRES), saveat = 100.0);\n\nPlots.heatmap(sol.u[end])","category":"section"},{"location":"examples/beeler_reuter/#Summary","page":"An Implicit/Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model","title":"Summary","text":"We achieve around a 6x speedup with running the explicit portion of our IMEX solver on a GPU. The major bottleneck of this technique is the communication between CPU and GPU. In its current form, not all the internals of the method utilize GPU acceleration. In particular, the implicit equations solved by GMRES are performed on the CPU. This partial CPU nature also increases the amount of data transfer that is required between the GPU and CPU (performed every f call). Compiling the full ODE solver to the GPU would solve both of these issues and potentially give a much larger speedup. JuliaDiffEq developers are currently working on solutions to alleviate these issues, but these will only be compatible with native Julia solvers (and not Sundials).\n\nimport SciMLTutorials\nSciMLTutorials.tutorial_footer(WEAVE_ARGS[:folder], WEAVE_ARGS[:file])","category":"section"},{"location":"features/callback_library/#callback_library","page":"Callback Library","title":"Callback Library","text":"DiffEqCallbacks.jl provides a library of various helpful callbacks which can be used with any component solver which implements the callback interface. It adds the following callbacks which are available to users of DifferentialEquations.jl.\n\nFor information, see the DiffEqCallbacks documentation.","category":"section"},{"location":"solvers/sde_solve/#sde_solve","page":"SDE Solvers","title":"SDE Solvers","text":"","category":"section"},{"location":"solvers/sde_solve/#Recommended-Methods","page":"SDE Solvers","title":"Recommended Methods","text":"For most Ito diagonal and scalar noise problems where a good amount of accuracy is required and where mild stiffness may be an issue, the SOSRI algorithm should do well. If the problem has additive noise, then SOSRA will be the optimal algorithm. At low tolerances (<1e-4?) SRA3 will be more efficient, though SOSRA is more robust to stiffness. For commutative noise, RKMilCommute is a strong order 1.0 method which utilizes the commutativity property to greatly speed up the stochastic iterated integral approximation and can choose between Ito and Stratonovich. For non-commutative noise, difficult problems usually require adaptive time stepping in order to be efficient. In this case, LambaEM and LambaEulerHeun are adaptive and handle general non-diagonal problems (for Ito and Stratonovich interpretations, respectively). If adaptivity isn't necessary, the EM and EulerHeun are good choices (for Ito and Stratonovich interpretations, respectively).\n\nFor stiff problems with additive noise, the high order adaptive method SKenCarp is highly preferred and will solve problems with similar efficiency as ODEs. If possible, stiff problems should be converted to make use of this additive noise solver. If the noise term is large/stiff, then the split-step methods are required in order for the implicit methods to be stable. For Ito in this case, use ISSEM and for Stratonovich use ISSEulerHeun. These two methods can handle any noise form.\n\nIf the noise term is not too large, for stiff problems with diagonal noise, ImplicitRKMil is the most efficient method and can choose between Ito and Stratonovich. For each of the theta methods, the parameter theta can be chosen. The default is theta=1/2 which will not dampen numerical oscillations and thus is symmetric (and almost symplectic) and will lead to less error when noise is sufficiently small. However, theta=1/2 is not L-stable in the drift term, and thus one can receive more stability (L-stability in the drift term) with theta=1, but with a tradeoff of error efficiency in the low noise case. In addition, the option symplectic=true will turn these methods into an implicit Midpoint extension, which is symplectic in distribution but has an accuracy tradeoff.\n\nIf only an estimate of the expected value of the solution is required, i.e., if one is only interested in an accurate draw from the distribution induced by a given SDE, the use of high weak order solvers is recommended. Specifically, DRI1 is preferred for a high number of Wiener processes. The weak stochastic Runge-Kutta solvers with weak order 2 due to Roessler are adaptive. All other high weak order solvers currently require a fixed step size.","category":"section"},{"location":"solvers/sde_solve/#special_noise_forms","page":"SDE Solvers","title":"Special Noise Forms","text":"Some solvers are for specialized forms of noise. Diagonal noise is the default setup. Non-diagonal noise is specified via setting noise_rate_prototype to a matrix in the SDEProblem type. A special form of non-diagonal noise, commutative noise, occurs when the noise satisfies the following condition:\n\nsum_i=1^d g_ij_1(tx) fracpartial g_kj_2(tx)partial x_i = sum_i=1^d g_ij_2(tx) fracpartial g_kj_1(tx)partial x_i\n\nfor every j_1j_2 and k. Additive noise is when g(tu)=g(t), i.e. is independent of u. Multiplicative noise is g_i(tu)=a_i u.","category":"section"},{"location":"solvers/sde_solve/#Iterated-Integral-Approximations","page":"SDE Solvers","title":"Iterated Integral Approximations","text":"The difficulty of higher strong order integrators stems from the presence of iterated stochastic integrals\n\nI(h) = int_0^hint_0^sdW^1_tdW^2_s\n\nin these schemes.\n\nThe approximation of these iterated integrals can be avoided, if the diffusion matrix satisfies the special commutativity condition given above. Because of this, many methods are only applicable to problems that satisfy the commutativity condition. In other words, many methods can only handle specific noise cases, like diagonal noise or commutative noise, because of how this iterated integral approximation is computed.\n\nHowever, the methods for general SDEs, like RKMilGeneral, perform a direct approximation of the iterated integrals. For those methods, the algorithms have an ii_approx keyword argument that allows one to specify the method for the approximation. The choices are:\n\nIICommutative: a simplification of the integral which assumes the noise commutativity property. If used on a non-commutative noise problem this will limit the strong convergence to 0.5.\nIILevyArea: computes the iterated integrals based on an approximation of the Levy area using the LevyArea.jl package: Kastner, F. and Rößler, A., arXiv: 2201.08424 Kastner, F. and Rößler, A., LevyArea.jl, 10.5281/ZENODO.5883748. The package supports the schemes: Fourier(), Milstein(), Wiktorsson(),MronRoe(). The optimal algorithm is automatically selected based on the dimension of the Brownian process and the step size. By passing a specific scheme, e.g., ii_approx=Fourier() methods can be manually selected. One must be careful when using the Levy area approximations in conjunction with adaptivity (adaptive=true) because the Levy area approximations draw random numbers that do not reflect the random numbers taken in a previous rejected step. This leads to a bias that increases with an increasing number of rejected steps.\n\nExample: RKMilGeneral(;ii_approx=IILevyArea()).","category":"section"},{"location":"solvers/sde_solve/#Special-Keyword-Arguments","page":"SDE Solvers","title":"Special Keyword Arguments","text":"save_noise: Determines whether the values of W are saved whenever the timeseries is saved. Defaults to false.\ndelta: The delta adaptivity parameter for the natural error estimator. Determines the balance between drift and diffusion error. For more details, see the publication.\nseed: Sets the seed for the random number generator. This overrides any seed set in the SDEProblem.","category":"section"},{"location":"solvers/sde_solve/#Full-List-of-Methods","page":"SDE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/sde_solve/#StochasticDiffEq.jl","page":"SDE Solvers","title":"StochasticDiffEq.jl","text":"Each of the StochasticDiffEq.jl solvers come with a linear interpolation. Orders are given in terms of strong order.","category":"section"},{"location":"solvers/sde_solve/#Nonstiff-Methods","page":"SDE Solvers","title":"Nonstiff Methods","text":"EM- The Euler-Maruyama method. Strong Order 0.5 in the Ito sense. Has an optional argument split=true for controlling step splitting. When splitting is enabled, the stability with large diffusion eigenvalues is improved. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Fixed time step only.†\nLambaEM- A modified Euler-Maruyama method with adaptive time stepping with an error estimator based on Lamba and Rackauckas. Has an optional argument split=true for controlling step splitting. When splitting is enabled, the stability with   large diffusion eigenvalues is improved. Strong Order 0.5 in the Ito sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nEulerHeun - The Euler-Heun method. Strong Order 0.5 in the Stratonovich sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Fixed time step only.†\nLambaEulerHeun - A modified Euler-Heun method with adaptive time stepping with an error estimator based on Lamba due to Rackauckas. Strong order 0.5 in the Stratonovich sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nRKMil - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method. Defaults to solving the Ito problem, but RKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. Only handles scalar and diagonal noise.†\nRKMilCommute - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method for commutative noise problems. Defaults to solving the Ito problem, but RKMilCommute(interpretation=:Stratonovich) makes it solve the Stratonovich problem. Uses a 1.5/2.0 error estimate for adaptive time stepping.†\nRKMilGeneral(;interpretation=:Ito, ii_approx=IILevyArea() - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method for general non-commutative noise problems. Allows for a choice of interpretation between :Ito and :Stratonovich. Allows for a choice of iterated integral approximation.\nWangLi3SMil_A - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_B - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_C - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_D - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_E - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_F - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nSRA - Adaptive strong order 1.5 methods for additive Ito and Stratonovich SDEs. Default tableau is for SRA1. Can handle diagonal, non-diagonal and scalar additive noise.\nSRI - Adaptive strong order 1.5 methods for diagonal/scalar Ito SDEs. Default tableau is for SRIW1.\nSRIW1 - Adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs.†\nSRIW2 - Adaptive strong order 1.5 and weak order 3.0 for diagonal/scalar Ito SDEs.†\nSOSRI - Stability-optimized adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs. Stable at high tolerances and robust to stiffness.†\nSOSRI2 - Stability-optimized adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs. Stable at high tolerances and robust to stiffness.†\nSRA1 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise.†\nSRA2 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise.†\nSRA3 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 3. Can handle non-diagonal and scalar additive noise.†\nSOSRA - A stability-optimized adaptive SRA. Strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise. Stable at high tolerances and robust to stiffness.†\nSOSRA2 - A stability-optimized adaptive SRA. Strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise. Stable at high tolerances and robust to stiffness.†\n\nExample usage:\n\nsol = solve(prob, SRIW1())\n\n3-stage Milstein Methods WangLi3SMil_A, WangLi3SMil_B, WangLi3SMil_D, WangLi3SMil_E and WangLi3SMil_F are currently implemented for 1-dimensional and diagonal noise only.","category":"section"},{"location":"solvers/sde_solve/#Tableau-Controls","page":"SDE Solvers","title":"Tableau Controls","text":"For SRA and SRI, the following option is allowed:\n\ntableau: The tableau for an :SRA or :SRI algorithm. Defaults to SRIW1 or SRA1.","category":"section"},{"location":"solvers/sde_solve/#S-ROCK-Methods","page":"SDE Solvers","title":"S-ROCK Methods","text":"SROCK1 - is a fixed step size stabilized explicit method for stiff problems. Defaults to solving the Ito problem but SROCK1(interpretation=:Stratonovich) can make it solve the Stratonovich problem. Strong order of convergence is 0.5 and weak order 1, but is optimized to get order 1 in case of scalar/diagonal noise.\nSROCKEM - is fixed step Euler-Mayurama with first order ROCK stabilization, and can thus handle stiff problems. Only for Ito problems. Defaults to strong and weak order 1.0, but can solve with weak order 0.5 as SROCKEM(strong_order_1=false). This method can handle 1-dimensional, diagonal and non-diagonal noise.\nSROCK2 - is a weak second order and strong first order fixed step stabilized method for stiff Ito problems. This method can handle 1-dimensional, diagonal and non-diagonal noise.\nSKSROCK - is fixed step stabilized explicit method for stiff Ito problems. Strong order 0.5 and weak order 1. This method has a better stability domain then SROCK1. Also, it allows special post-processing techniques in case of ergodic dynamical systems, in the context of ergodic Brownian dynamics, to achieve order 2 accuracy. SKSROCK(;post_processing=true) will make use of post-processing. By default, it doesn't use post-processing. Post-processing is optional and under development. The rest of the method is completely functional and can handle 1-dimensional, diagonal and non-diagonal noise.\nTangXiaoSROCK2 - is a fixed step size stabilized explicit method for stiff problems. Only for Ito problems. Weak order of 2 and strong order of 1. Has 5 versions with different stability domains which can be used as TangXiaoSROCK2(version_num=i) where i is 1-5. Under Development.","category":"section"},{"location":"solvers/sde_solve/#Stiff-Methods","page":"SDE Solvers","title":"Stiff Methods","text":"ImplicitEM - An order 0.5 Ito drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSTrapezoid - An alias for ImplicitEM with theta=1/2\nSImplicitMidpoint - An alias for ImplicitEM with theta=1/2 and symplectic=true\nImplicitEulerHeun - An order 0.5 Stratonovich drift-implicit method. This is a theta method which defaults to theta=1/2 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nImplicitRKMil - An order 1.0 drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. Defaults to solving the Ito problem, but ImplicitRKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Handles diagonal and scalar noise. Uses a 1.5/2.0 heuristic for adaptive time stepping.\nISSEM - An order 0.5 split-step Ito implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nISSEulerHeun - An order 0.5 split-step Stratonovich implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal,Q scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSKenCarp - Adaptive L-stable drift-implicit strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal and scalar additive noise.*†","category":"section"},{"location":"solvers/sde_solve/#Derivative-Based-Methods","page":"SDE Solvers","title":"Derivative-Based Methods","text":"The following methods require analytic derivatives of the diffusion term.\n\nPCEuler - The predictor corrector Euler method. Strong Order 0.5 in the Ito sense. Requires the ggprime function, which is defined as\n  textggprime^k(tx) = sum_j=1^m sum_i=1^d g_ij(tx) fracpartial g_kj(tx)partial x_i\nThis can also be understood more intuitively in vector/matrix form as,\ntextggprime(tx) = sum_j=1^m barmathcalJvec g^(j)(tx) vec g^(j)(tx)\nwhere vec g^(j) is the noise vector for the j'th noise channel and barmathcalJ is the Jacobian of the j'th   noise vector.\nThe default settings for the drift implicitness are theta=0.5 and the diffusion implicitness is eta=0.5.","category":"section"},{"location":"solvers/sde_solve/#High-Weak-Order-Methods","page":"SDE Solvers","title":"High Weak Order Methods","text":"Note that none of the following methods are adaptive.\n\nSimplifiedEM - A simplified Euler-Maruyama method with weak order 1.0 and fixed step size. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nDRI1 - Adaptive step weak order 2.0 for Ito SDEs with minimized error constants (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nDRI1NM - Adaptive step weak order 2.0 for Ito SDEs with minimized error constants (deterministic order 3). Can handle non-mixing diagonal (i.e., du[k] = f(u[k])) and scalar additive noise.†\nRI1 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI3 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI5 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI6 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI1WM - Fixed step weak order 1.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI2WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI3WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI4WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRS1 - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRS2 - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nPL1WM - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nPL1WMA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle additive noise.†\nNON - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 4). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nSIEA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the improved Euler method.\nSIEB - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the improved Euler method.\nSMEA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the modified Euler method.\nSMEB - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the modified Euler method.","category":"section"},{"location":"solvers/sde_solve/#StochasticCompositeAlgorithm","page":"SDE Solvers","title":"StochasticCompositeAlgorithm","text":"One unique feature of StochasticDiffEq.jl is the StochasticCompositeAlgorithm, which allows you to, with very minimal overhead, design a multimethod which switches between chosen algorithms as needed. The syntax is StochasticCompositeAlgorithm(algtup,choice_function) where algtup is a tuple of StochasticDiffEq.jl algorithms, and choice_function is a function which declares which method to use in the following step. For example, we can design a multimethod which uses EM() but switches to RKMil() whenever dt is too small:\n\nchoice_function(integrator) = (Int(integrator.dt < 0.001) + 1)\nalg_switch = StochasticCompositeAlgorithm((EM(), RKMil()), choice_function)\n\nThe choice_function takes in an integrator and thus all the features available in the Integrator Interface can be used in the choice function.","category":"section"},{"location":"solvers/sde_solve/#SimpleDiffEq.jl","page":"SDE Solvers","title":"SimpleDiffEq.jl","text":"This setup provides access to simplified versions of a few SDE solvers. They mostly exist for experimentation, but offer shorter compile times. They have limitations compared to StochasticDiffEq.jl.\n\nSimpleEM - A fixed timestep solve method for Euler-Maruyama. Only works with non-colored Gaussian noise.\n\nNote that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use SimpleDiffEq.jl:\n\nusing Pkg\nPkg.add(\"SimpleDiffEq\")\nimport SimpleDiffEq","category":"section"},{"location":"solvers/sde_solve/#BridgeDiffEq.jl","page":"SDE Solvers","title":"BridgeDiffEq.jl","text":"Bridge.jl is a set of fixed timestep algorithms written in Julia. These methods are made and optimized for out-of-place functions on immutable (static vector) types. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use BridgeDiffEq.jl:\n\nPkg.clone(\"https://github.com/SciML/BridgeDiffEq.jl\")\nimport BridgeDiffEq\n\nBridgeEuler - Strong order 0.5 Euler-Maruyama method for Ito equations.†\nBridgeHeun - Strong order 0.5 Euler-Heun method for Stratonovich equations.†\nBridgeSRK - Strong order 1.0 derivative-free stochastic Runge-Kutta method for scalar (<:Number) Ito equations.†","category":"section"},{"location":"solvers/sde_solve/#Notes","page":"SDE Solvers","title":"Notes","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.\n\n*:  Note that although SKenCarp uses the same table as KenCarp3, solving a ODE problem using SKenCarp by setting g(du,u,p,t) = du .= 0 will take much more steps than KenCarp3 because error estimator of SKenCarp is different (because of noise terms) and default value of qmax (maximum permissible ratio of relaxing/tightening dt for adaptive steps) is smaller for StochasticDiffEq algorithms.","category":"section"},{"location":"features/low_dep/#low_dep","page":"Reduced Compile Time, Optimizing Runtime, and Low Dependency Usage","title":"Reduced Compile Time, Optimizing Runtime, and Low Dependency Usage","text":"While DifferentialEquations.jl's defaults strike a balance between runtime and compile time performance, users should be aware that there are many options for controlling the dependency sizing and the compilation caching behavior to further refine this trade-off. The following methods are available for such controls.","category":"section"},{"location":"features/low_dep/#Controlling-Function-Specialization-and-Precompilation","page":"Reduced Compile Time, Optimizing Runtime, and Low Dependency Usage","title":"Controlling Function Specialization and Precompilation","text":"By default, DifferentialEquations.jl solvers make use of function wrapping techniques in order to fully precompile the solvers and thus decrease the compile time. However, in some cases you may wish to control this behavior, pushing more towards faster runtimes or faster compile times. This can be done by using the specialization arguments of the AbstractDEProblem constructors.\n\nFor example, with the ODEProblem we have ODEProblem{iip,specialize}(...). This second type parameter controls the specialization level with the following choices:\n\nSciMLBase.AutoSpecialize: the default. Uses a late wrapping scheme to hit a balance between runtime and compile time.\nSciMLBase.NoSpecialize: this will never specialize on the constituent functions, having the least compile time but the highest runtime.\nSciMLBase.FullSpecialize: this will fully re-specialize the solver on the given ODE, achieving the fastest runtimes while increasing the compile times. This is what is recommended when benchmarking and when running long computations, such as in optimization loops.\n\nFor more information on the specialization levels, please see the SciMLBase documentation on specialization levels.\n\nDifferentialEquations.jl and its ODE package OrdinaryDiffEq.jl precompile some standard problem types and solvers. The problem types include the three specialization levels described above and the default setting. The solvers include some\n\nstandard solvers for non-stiff problems such as Tsit5()\nstandard solvers for stiff problems such as Rosenbrock23()\nstandard solvers with stiffness detection such as AutoTsit5(Rosenbrock23())\nlow-storage methods for conservation laws such as SSPRK43() (precompilation disabled by default)\n\nTo adapt the amount of precompilation, you can use Preferences.jl. For example, to turn off precompilation for non-default problem types (specialization levels) and all stiff/implicit/low-storage solvers, you can execute the following code in your active project.\n\nimport Preferences, UUIDs\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileNonStiff\" => true)\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileStiff\" => false)\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileAutoSwitch\" => false)\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileLowStorage\" => false)\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileDefaultSpecialize\" => true)\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileAutoSpecialize\" => false)\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileFunctionWrapperSpecialize\" => false)\nPreferences.set_preferences!(UUIDs.UUID(\"1dea7af3-3e70-54e6-95c3-0bf5283fa5ed\"), \"PrecompileNoSpecialize\" => false)\n\nThis will create a LocalPreferences.toml file next to the currently active Project.toml file.","category":"section"},{"location":"features/low_dep/#Decreasing-Dependency-Size-by-Direct-Dependence-on-Specific-Solvers","page":"Reduced Compile Time, Optimizing Runtime, and Low Dependency Usage","title":"Decreasing Dependency Size by Direct Dependence on Specific Solvers","text":"DifferentialEquations.jl is a large library containing the functionality of many different solver and add-on packages. However, often you may want to cut down on the size of the dependency and only use the parts of the library which are essential to your application. This is possible due to SciML's modular package structure.","category":"section"},{"location":"features/low_dep/#Common-Example:-Using-only-OrdinaryDiffEq.jl","page":"Reduced Compile Time, Optimizing Runtime, and Low Dependency Usage","title":"Common Example: Using only OrdinaryDiffEq.jl","text":"One common example is using only the ODE solvers OrdinaryDiffEq.jl. The solvers all reexport SciMLBase.jl (which holds the problem and solution types) and so OrdinaryDiffEq.jl is all that's needed. Thus replacing\n\nimport DifferentialEquations as DE\n\nwith\n\n#Add the OrdinaryDiffEq Package first!\n#using Pkg; Pkg.add(\"OrdinaryDiffEq\")\nimport OrdinaryDiffEq as ODE\n\nwill work if these are the only features you are using.","category":"section"},{"location":"features/low_dep/#Generalizing-the-Idea","page":"Reduced Compile Time, Optimizing Runtime, and Low Dependency Usage","title":"Generalizing the Idea","text":"In general, you will always need SciMLBase.jl, since it defines all of the fundamental types, but the solvers will automatically reexport it. For solvers, you typically only require that solver package. So SciMLBase+Sundials, SciMLBase+LSODA, etc. will get you the common interface with that specific solver setup. SciMLBase.jl is a very lightweight dependency, so there is no issue here!\n\nFor the add-on packages, you will normally need SciMLBase, the solver package you choose, and the add-on package. So for example, for predefined callbacks you would likely want SciMLBase+OrdinaryDiffEq+DiffEqCallbacks. If you aren't sure which package a specific command is from, then use @which. For example, from the callback docs we have:\n\nimport DifferentialEquations as DE\nfunction fitz(du, u, p, t)\n    V, R = u\n    a, b, c = p\n    du[1] = c * (V - V^3 / 3 + R)\n    du[2] = -(1 / c) * (V - a - b * R)\nend\nu0 = [-1.0; 1.0]\ntspan = (0.0, 20.0)\np = (0.2, 0.2, 3.0)\nprob = DE.ODEProblem(fitz, u0, tspan, p)\ncb = DE.ProbIntsUncertainty(0.2, 1)\nensemble_prob = DE.EnsembleProblem(prob)\nsim = DE.solve(ensemble_prob, DE.Euler(), trajectories = 100, callback = cb, dt = 1 / 10)\n\nIf we wanted to know where ProbIntsUncertainty(0.2,1) came from, we can do:\n\nimport InteractiveUtils # hide\nInteractiveUtils.@which DE.ProbIntsUncertainty(0.2, 1)\n\nThis says it's in the DiffEqCallbacks.jl package. Thus in this case, we could have done\n\nimport OrdinaryDiffEq as ODE, DiffEqCallbacks as CB\nfunction fitz(du, u, p, t)\n    V, R = u\n    a, b, c = p\n    du[1] = c * (V - V^3 / 3 + R)\n    du[2] = -(1 / c) * (V - a - b * R)\nend\nu0 = [-1.0; 1.0]\ntspan = (0.0, 20.0)\np = (0.2, 0.2, 3.0)\nprob = ODE.ODEProblem(fitz, u0, tspan, p)\ncb = CB.ProbIntsUncertainty(0.2, 1)\nensemble_prob = ODE.EnsembleProblem(prob)\nsim = ODE.solve(ensemble_prob, ODE.Euler(), trajectories = 100, callback = cb, dt = 1 / 10)\n\ninstead of the full using DifferentialEquations. Note that due to the way Julia dependencies work, any internal function in the package will work. The only dependencies you need to explicitly using are the functions you are specifically calling. Thus, this method can be used to determine all of the DiffEq packages you are using.","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Commutative-Noise-Methods","page":"Commutative Noise Methods","title":"Commutative Noise Methods","text":"When multiple noise sources satisfy commutativity conditions, specialized methods can achieve higher accuracy and efficiency compared to general methods. These methods avoid expensive Lévy area computations while maintaining high order.","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Recommended-Commutative-Noise-Methods","page":"Commutative Noise Methods","title":"Recommended Commutative Noise Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#RKMilCommute-Runge-Kutta-Milstein-for-Commutative-Noise","page":"Commutative Noise Methods","title":"RKMilCommute - Runge-Kutta Milstein for Commutative Noise","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#RKMilGeneral-General-Milstein-for-Non-commutative-Noise","page":"Commutative Noise Methods","title":"RKMilGeneral - General Milstein for Non-commutative Noise","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Three-Stage-Milstein-Methods","page":"Commutative Noise Methods","title":"Three-Stage Milstein Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#WangLi3SMil-Family-Fixed-Step-Milstein-Methods","page":"Commutative Noise Methods","title":"WangLi3SMil Family - Fixed Step Milstein Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Understanding-Commutative-Noise","page":"Commutative Noise Methods","title":"Understanding Commutative Noise","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Commutativity-Condition","page":"Commutative Noise Methods","title":"Commutativity Condition","text":"Noise terms g₁, g₂, ..., gₘ are commutative if:\n\n[gᵢ, gⱼ] = gᵢ(∂gⱼ/∂x) - gⱼ(∂gᵢ/∂x) = 0\n\nfor all pairs (i,j).","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Examples-of-Commutative-Noise:","page":"Commutative Noise Methods","title":"Examples of Commutative Noise:","text":"Additive noise: g(u,t) = σ(t) (independent of u)\nScalar multiplicative: g(u,t) = σ(t)u (same u dependence)\nDiagonal with same function: gᵢ(u,t) = σᵢ(t)h(u)","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Examples-of-Non-commutative-Noise:","page":"Commutative Noise Methods","title":"Examples of Non-commutative Noise:","text":"Different multiplicative terms: g₁ = σ₁u₁, g₂ = σ₂u₂\nCross-coupling: g₁ = σ₁₁u₁ + σ₁₂u₂, g₂ = σ₂₁u₁ + σ₂₂u₂","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Method-Selection-Guide","page":"Commutative Noise Methods","title":"Method Selection Guide","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#For-Commutative-Noise:","page":"Commutative Noise Methods","title":"For Commutative Noise:","text":"RKMilCommute - Adaptive, excellent general choice\nWangLi3SMil methods - Fixed step, when dt is predetermined","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#For-Non-commutative-Noise:","page":"Commutative Noise Methods","title":"For Non-commutative Noise:","text":"RKMilGeneral - Handles general case with Lévy area\nFall back to SRI/SRA methods - More robust for complex noise","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#For-Uncertain-Commutativity:","page":"Commutative Noise Methods","title":"For Uncertain Commutativity:","text":"Test with RKMilCommute first\nIf results seem incorrect, switch to RKMilGeneral or SRI methods","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Computational-Advantages","page":"Commutative Noise Methods","title":"Computational Advantages","text":"Commutative case:\n\nNo Lévy area computation needed\nSimpler stochastic integrals\nHigher efficiency per step\nBetter scalability to high dimensions\n\nNon-commutative case:\n\nRequires Lévy area approximation\nMore expensive per step\nUses specialized algorithms (LevyArea.jl integration)","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Performance-Tips","page":"Commutative Noise Methods","title":"Performance Tips","text":"Verify commutativity before using specialized methods\nUse appropriate interpretation (Itô vs Stratonovich)\nConsider problem dimension - benefits increase with system size\nTest accuracy - commutative methods may be more sensitive","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#Iterated-Integrals-and-Lévy-Area","page":"Commutative Noise Methods","title":"Iterated Integrals and Lévy Area","text":"","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#For-Commutative-Noise-(IICommutative):","page":"Commutative Noise Methods","title":"For Commutative Noise (IICommutative):","text":"Only simple stochastic integrals ∫₀ᵗ dWₛ are needed.","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#For-Non-commutative-Noise-(IILevyArea):","page":"Commutative Noise Methods","title":"For Non-commutative Noise (IILevyArea):","text":"Requires double integrals ∫₀ᵗ ∫₀ˢ dWᵤdWₛ (Lévy area).\n\nRKMilGeneral automatically chooses optimal Lévy area algorithms via LevyArea.jl.","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#References","page":"Commutative Noise Methods","title":"References","text":"Kloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\"\nWang and Li, \"Three-stage stochastic Runge-Kutta methods for stochastic differential equations\"\nKastner, F. and Rößler, A., \"LevyArea.jl\" for Lévy area computation","category":"section"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.RKMilCommute","page":"Commutative Noise Methods","title":"StochasticDiffEq.RKMilCommute","text":"Kloeden, P.E., Platen, E., Numerical Solution of Stochastic Differential Equations. Springer. Berlin Heidelberg (2011)\n\nRKMilCommute(;interpretation=AlgorithmInterpretation.Ito, ii_approx=IICommutative())\n\nRKMilCommute: Runge-Kutta Milstein for Commutative Noise (Nonstiff) - Recommended for Commutative Noise\n\nExplicit Runge-Kutta discretization of the strong order 1.0 Milstein method specialized for commutative noise problems.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: Depends on tableau\nTime stepping: Adaptive (1.5/2.0 error estimate)\nNoise types: Commutative noise (multiple noise sources that commute)\nSDE interpretation: Configurable (Itô or Stratonovich)\n\nParameters\n\ninterpretation: Choose AlgorithmInterpretation.Ito (default) or AlgorithmInterpretation.Stratonovich\nii_approx: Iterated integral approximation method (default: IICommutative())\n\nWhen to Use\n\nRecommended for commutative noise problems\nWhen you have multiple noise sources that satisfy commutativity conditions\nFor multi-dimensional SDEs with commuting noise terms\nWhen higher order accuracy than Euler-Maruyama is needed\n\nCommutative Noise\n\nApplicable when the noise terms satisfy:\n\n[g_i, g_j] = g_i(∂g_j/∂x) - g_j(∂g_i/∂x) = 0\n\nfor all noise terms gi, gj.\n\nAlgorithm Description\n\nExtends the Milstein method to handle multiple commutative noise sources efficiently without requiring the full Lévy area computation.\n\nReferences\n\nKloeden, P.E., Platen, E., \"Numerical Solution of Stochastic Differential Equations\", Springer (1992)\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.RKMilGeneral","page":"Commutative Noise Methods","title":"StochasticDiffEq.RKMilGeneral","text":"Kloeden, P.E., Platen, E., Numerical Solution of Stochastic Differential Equations. Springer. Berlin Heidelberg (2011)\n\nRKMilGeneral(;interpretation=AlgorithmInterpretation.Ito, ii_approx=IILevyArea(), c=1, p=nothing, dt=nothing)\n\nRKMilGeneral: General Runge-Kutta Milstein Method (Nonstiff)\n\nExplicit Runge-Kutta discretization of the Milstein method for general non-commutative noise problems using Lévy area approximation.\n\nMethod Properties\n\nStrong Order: 1.0 (for general noise)\nWeak Order: Depends on tableau and Lévy area approximation\nTime stepping: Adaptive\nNoise types: All forms including non-commutative noise\nSDE interpretation: Configurable (Itô or Stratonovich)\n\nParameters\n\ninterpretation: Choose AlgorithmInterpretation.Ito (default) or AlgorithmInterpretation.Stratonovich\nii_approx: Iterated integral approximation method (default: IILevyArea())\nc::Int = 1: Truncation parameter for Lévy area\np: Truncation level (computed automatically if nothing)\ndt: Used for automatic truncation level computation\n\nWhen to Use\n\nFor general non-commutative noise problems\nWhen RKMilCommute is not applicable (noise doesn't commute)\nWhen higher accuracy than Euler methods is needed\nFor complex multi-dimensional noise structures\n\nLévy Area Approximation\n\nUses LevyArea.jl for efficient computation of iterated integrals:\n\nAutomatically selects optimal algorithms\nHandles truncation for practical computation\nSupports various approximation strategies\n\nComputational Cost\n\nMore expensive than commutative methods\nLévy area computation scales with noise dimension\nAdaptive truncation balances accuracy and efficiency\n\nReferences\n\nKastner, F. and Rößler, A., \"LevyArea.jl: A Julia package for Lévy area computation\", arXiv:2201.08424\nLevyArea.jl: https://github.com/stochastics-uni-luebeck/LevyArea.jl\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.WangLi3SMil_A","page":"Commutative Noise Methods","title":"StochasticDiffEq.WangLi3SMil_A","text":"WangLi3SMil_A()\n\nWangLi3SMil_A: 3-Stage Milstein Method A (Nonstiff)\n\nFixed step-size explicit 3-stage Milstein method with strong and weak order 1.0 for Itô SDEs.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 1.0\nTime stepping: Fixed step size\nNoise types: Depends on tableau (typically diagonal/scalar)\nSDE interpretation: Itô\n\nWhen to Use\n\nWhen fixed step size is preferred\nFor Itô SDEs requiring order 1.0 accuracy\nPart of WangLi family - compare performance with other variants\nWhen computational cost per step is less important than simplicity\n\nReferences\n\nWang and Li, \"Three-stage stochastic Runge-Kutta methods for stochastic differential equations\"\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.WangLi3SMil_B","page":"Commutative Noise Methods","title":"StochasticDiffEq.WangLi3SMil_B","text":"WangLi3SMil_B()\n\nWangLi3SMil_B: 3-Stage Milstein Method B (Nonstiff)\n\nAlternative 3-stage Milstein method with different stability and accuracy characteristics.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 1.0\nTime stepping: Fixed step size\nNoise types: Depends on tableau (typically diagonal/scalar)\nSDE interpretation: Itô\n\nReferences\n\nWang and Li, \"Three-stage stochastic Runge-Kutta methods for stochastic differential equations\"\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.WangLi3SMil_C","page":"Commutative Noise Methods","title":"StochasticDiffEq.WangLi3SMil_C","text":"WangLi3SMil_C()\n\nWangLi3SMil_C: 3-Stage Milstein Method C (Nonstiff)\n\nThird variant in the WangLi 3-stage Milstein family.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 1.0\nTime stepping: Fixed step size\nNoise types: Depends on tableau (typically diagonal/scalar)\nSDE interpretation: Itô\n\nReferences\n\nWang and Li, \"Three-stage stochastic Runge-Kutta methods for stochastic differential equations\"\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.WangLi3SMil_D","page":"Commutative Noise Methods","title":"StochasticDiffEq.WangLi3SMil_D","text":"WangLi3SMil_D()\n\nWangLi3SMil_D: 3-Stage Milstein Method D (Nonstiff)\n\nFourth variant in the WangLi 3-stage Milstein family.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 1.0\nTime stepping: Fixed step size\nNoise types: Depends on tableau (typically diagonal/scalar)\nSDE interpretation: Itô\n\nReferences\n\nWang and Li, \"Three-stage stochastic Runge-Kutta methods for stochastic differential equations\"\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.WangLi3SMil_E","page":"Commutative Noise Methods","title":"StochasticDiffEq.WangLi3SMil_E","text":"WangLi3SMil_E()\n\nWangLi3SMil_E: 3-Stage Milstein Method E (Nonstiff)\n\nFifth variant in the WangLi 3-stage Milstein family.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 1.0\nTime stepping: Fixed step size\nNoise types: Depends on tableau (typically diagonal/scalar)\nSDE interpretation: Itô\n\nReferences\n\nWang and Li, \"Three-stage stochastic Runge-Kutta methods for stochastic differential equations\"\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/nonstiff/commutative_noise/#StochasticDiffEq.WangLi3SMil_F","page":"Commutative Noise Methods","title":"StochasticDiffEq.WangLi3SMil_F","text":"WangLi3SMil_F()\n\nWangLi3SMil_F: 3-Stage Milstein Method F (Nonstiff)\n\nSixth and final variant in the WangLi 3-stage Milstein family.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 1.0\nTime stepping: Fixed step size\nNoise types: Depends on tableau (typically diagonal/scalar)\nSDE interpretation: Itô\n\nWhen to Use (WangLi Family)\n\nCompare all variants (A-F) to find best performance for your problem\nFixed step applications where step size is predetermined\nBenchmarking against adaptive methods\nWhen Milstein accuracy is needed with explicit fixed steps\n\nReferences\n\nWang and Li, \"Three-stage stochastic Runge-Kutta methods for stochastic differential equations\"\n\n\n\n\n\n","category":"type"},{"location":"features/io/#io","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"The ability to save and load solutions is important for handling large datasets and analyzing the results over multiple Julia sessions. This page explains the existing functionality for doing so.","category":"section"},{"location":"features/io/#Tabular-Data:-IterableTables","page":"I/O: Saving and Loading Solution Data","title":"Tabular Data: IterableTables","text":"An interface to IterableTables.jl is provided. This IterableTables link allows you to use a solution type as the data source to convert to other tabular data formats. For example, let's solve a 4x2 system of ODEs and get the DataFrame:\n\nimport OrdinaryDiffEq as ODE, DataFrames\nf_2dlinear = (du, u, p, t) -> du .= 1.01u;\ntspan = (0.0, 1.0)\nprob = ODE.ODEProblem(f_2dlinear, rand(2, 2), tspan);\nsol = ODE.solve(prob, ODE.Euler(); dt = 1 // 2^(4));\ndf = DataFrames.DataFrame(sol)\n\nIf we set syms in the DiffEqFunction, then those names will be used:\n\nf = ODE.ODEFunction(f_2dlinear, syms = [:a, :b, :c, :d])\nprob = ODE.ODEProblem(f, rand(2, 2), (0.0, 1.0));\nsol = ODE.solve(prob, ODE.Euler(); dt = 1 // 2^(4));\ndf = DataFrames.DataFrame(sol)\n\nMany modeling frameworks will automatically set syms for this feature. Additionally, this data can be saved to a CSV:\n\nimport CSV\nCSV.write(\"out.csv\", df)","category":"section"},{"location":"features/io/#JLD2-and-BSON.jl","page":"I/O: Saving and Loading Solution Data","title":"JLD2 and BSON.jl","text":"JLD2.jl and BSON.jl will work with the full solution type if you bring the required functions back into scope before loading. For example, if we save the solution:\n\nsol = ODE.solve(prob, ODE.Euler(); dt = 1 // 2^(4))\nimport JLD2\nJLD2.@save \"out.jld2\" sol\n\nthen we can get the full solution type back, interpolations and all, if we load the dependent functions first:\n\n# New session\nimport JLD2\nimport OrdinaryDiffEq as ODE\nJLD2.@load \"out.jld2\" sol\n\nThe example with BSON.jl is:\n\nsol = ODE.solve(prob, ODE.Euler(); dt = 1 // 2^(4))\nimport BSON\nBSON.bson(\"test.bson\", Dict(:sol => sol))\n\n# New session\nimport OrdinaryDiffEq as ODE\nimport BSON\n# BSON.load(\"test.bson\") # currently broken: https://github.com/JuliaIO/BSON.jl/issues/109\n\nIf you load it without the DE function then for some algorithms the interpolation may not work, and for all algorithms you'll need at least a solver package or SciMLBase.jl in scope in order for the solution interface (plot recipes, array indexing, etc.) to work. If none of these are put into scope, the solution type will still load and hold all of the values (so sol.u and sol.t will work), but none of the interface will be available.\n\nIf you want a copy of the solution that contains no function information you can use the function SciMLBase.strip_solution(sol). This will return a copy of the solution that doesn't have any functions, which you can serialize and deserialize without having any of the problems that typically come with serializing functions.","category":"section"},{"location":"features/io/#JLD","page":"I/O: Saving and Loading Solution Data","title":"JLD","text":"Don't use JLD. It's dead. Julia types can be saved via JLD.jl. However, they cannot save types which have functions, which means that the solution type is currently not compatible with JLD.\n\nimport JLD\nJLD.save(\"out.jld\", \"sol\", sol)","category":"section"},{"location":"tutorials/rode_example/#rode_example","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"section"},{"location":"tutorials/rode_example/#Example-1:-Scalar-RODEs","page":"Random Ordinary Differential Equations","title":"Example 1: Scalar RODEs","text":"In this example, we will solve the equation\n\ndu = f(uptW)dt\n\nwhere f(uptW)=2usin(W) and W(t) is a Wiener process (Gaussian process).\n\nimport StochasticDiffEq as SDE\nimport Plots\nfunction f3(u, p, t, W)\n    2u * sin(W)\nend\nu0 = 1.00\ntspan = (0.0, 5.0)\nprob = SDE.RODEProblem(f3, u0, tspan)\nsol = SDE.solve(prob, SDE.RandomEM(); dt = 1 / 100)\nPlots.plot(sol)\n\nThe random process defaults to a Gaussian/Wiener process, so there is nothing else required here! See the documentation on NoiseProcesses for details on how to define other noise processes.","category":"section"},{"location":"tutorials/rode_example/#Example-2:-Systems-of-RODEs","page":"Random Ordinary Differential Equations","title":"Example 2: Systems of RODEs","text":"As with the other problem types, there is an in-place version which is more efficient for systems. The signature is f(du,u,p,t,W). For example,\n\nimport StochasticDiffEq as SDE\nimport Plots\nfunction f(du, u, p, t, W)\n    du[1] = 2u[1] * sin(W[1] - W[2])\n    du[2] = -2u[2] * cos(W[1] + W[2])\nend\nu0 = [1.00; 1.00]\ntspan = (0.0, 5.0)\nprob = SDE.RODEProblem(f, u0, tspan)\nsol = SDE.solve(prob, SDE.RandomEM(); dt = 1 / 100)\nPlots.plot(sol)\n\nBy default, the size of the noise process matches the size of u0. However, you can use the rand_prototype keyword to explicitly set the size of the random process:\n\nimport StochasticDiffEq as SDE\nimport Plots\nfunction f(du, u, p, t, W)\n    du[1] = -2W[3] * u[1] * sin(W[1] - W[2])\n    du[2] = -2u[2] * cos(W[1] + W[2])\nend\nu0 = [1.00; 1.00]\ntspan = (0.0, 5.0)\nprob = SDE.RODEProblem(f, u0, tspan; rand_prototype = zeros(3))\nsol = SDE.solve(prob, SDE.RandomEM(); dt = 1 / 100)\nPlots.plot(sol)","category":"section"},{"location":"basics/integrator/#integrator","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator interface gives one the ability to interactively step through the numerical solving of a differential equation. Through this interface, one can easily monitor results, modify the problem during a run, and dynamically continue solving as one sees fit.","category":"section"},{"location":"basics/integrator/#Initialization-and-Stepping","page":"Integrator Interface","title":"Initialization and Stepping","text":"To initialize an integrator, use the syntax:\n\nintegrator = init(prob, alg; kwargs...)\n\nThe keyword args which are accepted are the same as the solver options used by solve and the returned value is an integrator which satisfies typeof(integrator)<:DEIntegrator. One can manually choose to step via the step! command:\n\nstep!(integrator)\n\nwhich will take one successful step. Additionally:\n\nstep!(integrator, dt, false)\n\npassing a dt will make the integrator continue to step until at least integrator.t+dt, and passing true as the third argument will add a tstop to force it to step up to integrator.t+dt, exactly.\n\nTo check whether the integration step was successful, you can call check_error(integrator) which returns one of the return codes.\n\nThis type also implements an iterator interface, so one can step n times (or to the last tstop) using the take iterator:\n\nfor i in take(integrator, n)\nend\n\nOne can loop to the end by using solve!(integrator) or using the iterator interface:\n\nfor i in integrator\nend\n\nIn addition, some helper iterators are provided to help monitor the solution. For example, the tuples iterator lets you view the values:\n\nfor (u, t) in tuples(integrator)\n    @show u, t\nend\n\nand the intervals iterator lets you view the full interval:\n\nfor (uprev, tprev, u, t) in intervals(integrator)\n    @show tprev, t\nend\n\nAdditionally, you can make the iterator return specific time points via the TimeChoiceIterator:\n\nts = range(0, stop = 1, length = 11)\nfor (u, t) in TimeChoiceIterator(integrator, ts)\n    @show u, t\nend\n\nLastly, one can dynamically control the “endpoint”. The initialization simply makes prob.tspan[2] the last value of tstop, and many of the iterators are made to stop at the final tstop value. However, step! will always take a step, and one can dynamically add new values of tstops by modifying the variable in the options field: add_tstop!(integrator,new_t).\n\nFinally, to solve to the last tstop, call solve!(integrator). Doing init and then solve! is equivalent to solve.","category":"section"},{"location":"basics/integrator/#Handling-Integrators","page":"Integrator Interface","title":"Handling Integrators","text":"The integrator<:DEIntegrator type holds all the information for the intermediate solution of the differential equation. Useful fields are:\n\nt - time of the proposed step\nu - value at the proposed step\np - user-provided data\nopts - common solver options\nalg - the algorithm associated with the solution\nf - the function being solved\nsol - the current state of the solution\ntprev - the last timepoint\nuprev - the value at the last timepoint\ntdir - the sign for the direction of time\n\nThe function f is usually a wrapper of the function provided when creating the specific problem. For example, when solving an ODEProblem, f will be an ODEFunction. To access the right-hand side function provided by the user when creating the ODEProblem, please use SciMLBase.unwrapped_f(integrator.f.f).\n\nThe p is the (parameter) data which is provided by the user as a keyword arg in init. opts holds all the common solver options, and can be mutated to change the solver characteristics. For example, to modify the absolute tolerance for the future timesteps, one can do:\n\nintegrator.opts.abstol = 1e-9\n\nThe sol field holds the current solution. This current solution includes the interpolation function if available, and thus integrator.sol(t) lets one interpolate efficiently over the whole current solution. Additionally, a “current interval interpolation function” is provided on the integrator type via integrator(t,deriv::Type=Val{0};idxs=nothing,continuity=:left). This uses only the solver information from the interval [tprev,t] to compute the interpolation, and is allowed to extrapolate beyond that interval.","category":"section"},{"location":"basics/integrator/#Note-about-mutating","page":"Integrator Interface","title":"Note about mutating","text":"Be cautious: one should not directly mutate the t and u fields of the integrator. Doing so will destroy the accuracy of the interpolator and can harm certain algorithms. Instead, if one wants to introduce discontinuous changes, one should use the callbacks. Modifications within a callback affect! surrounded by saves provides an error-free handling of the discontinuity.\n\nAs low-level alternative to the callbacks, one can use set_t!, set_u! and set_ut! to mutate integrator states.  Note that certain integrators may not have efficient ways to modify u and t.  In such case, set_*! are as inefficient as reinit!.","category":"section"},{"location":"basics/integrator/#Integrator-vs-Solution","page":"Integrator Interface","title":"Integrator vs Solution","text":"The integrator and the solution have very different actions because they have very different meanings. The typeof(sol) <: DESolution type is a type with history: it stores all the (requested) timepoints and interpolates/acts using the values closest in time. On the other hand, the typeof(integrator)<:DEIntegrator type is a local object. It only knows the times of the interval it currently spans, the current caches and values, and the current state of the solver (the current options, tolerances, etc.). These serve very different purposes:\n\nThe integrator's interpolation can extrapolate, both forward and backward in time. This is used to estimate events and is internally used for predictions.\nThe integrator is fully mutable upon iteration. This means that every time an iterator affect is used, it will take timesteps from the current time. This means that first(integrator)!=first(integrator) since the integrator will step once to evaluate the left and then step once more (not backtracking). This allows the iterator to keep dynamically stepping, though one should note that it may violate some immutability assumptions commonly made about iterators.\n\nIf one wants the solution object, then one can find it in integrator.sol.","category":"section"},{"location":"basics/integrator/#Function-Interface","page":"Integrator Interface","title":"Function Interface","text":"In addition to the type interface, a function interface is provided which allows for safe modifications of the integrator type, and allows for uniform usage throughout the ecosystem (for packages/algorithms which implement the functions). The following functions make up the interface:","category":"section"},{"location":"basics/integrator/#Saving-Controls","page":"Integrator Interface","title":"Saving Controls","text":"","category":"section"},{"location":"basics/integrator/#Caches","page":"Integrator Interface","title":"Caches","text":"","category":"section"},{"location":"basics/integrator/#stepping_controls","page":"Integrator Interface","title":"Stepping Controls","text":"","category":"section"},{"location":"basics/integrator/#Resizing","page":"Integrator Interface","title":"Resizing","text":"","category":"section"},{"location":"basics/integrator/#Reinitialization","page":"Integrator Interface","title":"Reinitialization","text":"","category":"section"},{"location":"basics/integrator/#Misc","page":"Integrator Interface","title":"Misc","text":"warning: Warning\nNote that not all these functions will be implemented for every algorithm. Some have hard limitations. For example, Sundials.jl cannot resize problems. When a function is not limited, an error will be thrown.","category":"section"},{"location":"basics/integrator/#Additional-Options","page":"Integrator Interface","title":"Additional Options","text":"The following options can additionally be specified in init (or be mutated in the opts) for further control of the integrator:\n\nadvance_to_tstop: This makes step! continue to the next value in tstop.\nstop_at_next_tstop: This forces the iterators to stop at the next value of tstop.\n\nFor example, if one wants to iterate but only stop at specific values, one can choose:\n\nintegrator = DE.init(\n    prob, DE.Tsit5(); dt = 1 // 2^(4), tstops = [0.5], advance_to_tstop = true)\nfor (u, t) in tuples(integrator)\n    @test t ∈ [0.5, 1.0]\nend\n\nwhich will only enter the loop body at the values in tstops (here, prob.tspan[2]==1.0 and thus there are two values of tstops which are hit). Additionally, one can solve! only to 0.5 via:\n\nintegrator = DE.init(prob, DE.Tsit5(); dt = 1 // 2^(4), tstops = [0.5])\nintegrator.opts.stop_at_next_tstop = true\nsolve!(integrator)","category":"section"},{"location":"basics/integrator/#Plot-Recipe","page":"Integrator Interface","title":"Plot Recipe","text":"Like the DESolution type, a plot recipe is provided for the DEIntegrator type. Since the DEIntegrator type is a local state type on the current interval, plot(integrator) returns the solution on the current interval. The same options for the plot recipe are provided as for sol, meaning one can choose variables via the idxs keyword argument, or change the plotdensity / turn on/off denseplot.\n\nAdditionally, since the integrator is an iterator, this can be used in the Plots.jl animate command to iteratively build an animation of the solution while solving the differential equation.\n\nFor an example of manually chaining together the iterator interface and plotting, one should try the following:\n\nimport DifferentialEquations as DE, DiffEqProblemLibrary, Plots\n\n# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = DE.ODEProblem((u, p, t) -> 1.01u, 0.5, (0.0, 1.0))\n\nimport Plots\nintegrator = DE.init(prob, DE.Tsit5(); dt = 1 // 2^(4), tstops = [0.5])\npyplot(show = true)\nPlots.plot(integrator)\nfor i in integrator\n    display(Plots.plot!(integrator, idxs = (0, 1), legend = false))\nend\nDE.step!(integrator);\nPlots.plot!(integrator, idxs = (0, 1), legend = false);\nsavefig(\"iteratorplot.png\")\n\n(Image: Iterator Plot)","category":"section"},{"location":"basics/integrator/#CommonSolve.step!","page":"Integrator Interface","title":"CommonSolve.step!","text":"step!(integ::DEIntegrator [, dt [, stop_at_tdt]])\n\nPerform one (successful) step on the integrator.\n\nAlternative, if a dt is given, then step! the integrator until there is a temporal difference ≥ dt in integ.t.  When true is passed to the optional third argument, the integrator advances exactly dt.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.check_error","page":"Integrator Interface","title":"SciMLBase.check_error","text":"check_error(integrator)\n\nCheck state of integrator and return one of the Return Codes\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.check_error!","page":"Integrator Interface","title":"SciMLBase.check_error!","text":"check_error!(integrator)\n\nSame as check_error but also set solution's return code (integrator.sol.retcode) and run postamble!.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.set_t!","page":"Integrator Interface","title":"SciMLBase.set_t!","text":"set_t!(integrator::DEIntegrator, t)\n\nSet current time point of the integrator to t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.set_u!","page":"Integrator Interface","title":"SciMLBase.set_u!","text":"set_u!(integrator::DEIntegrator, u)\nset_u!(integrator::DEIntegrator, sym, val)\n\nSet current state of the integrator to u. Alternatively, set the state of variable sym to value val.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.set_ut!","page":"Integrator Interface","title":"SciMLBase.set_ut!","text":"set_ut!(integrator::DEIntegrator, u, t)\n\nSet current state of the integrator to u and t\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.savevalues!","page":"Integrator Interface","title":"SciMLBase.savevalues!","text":"savevalues!(integrator::DEIntegrator,\n  force_save=false) -> Tuple{Bool, Bool}\n\nTry to save the state and time variables at the current time point, or the saveat point by using interpolation when appropriate. It returns a tuple that is (saved, savedexactly). If savevalues! saved value, then saved is true, and if savevalues! saved at the current time point, then savedexactly is true.\n\nThe saving priority/order is as follows:\n\nsave_on\nsaveat\nforce_save\nsave_everystep\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.get_tmp_cache","page":"Integrator Interface","title":"SciMLBase.get_tmp_cache","text":"get_tmp_cache(i::DEIntegrator)\n\nReturns a tuple of internal cache vectors which are safe to use as temporary arrays. This should be used for integrator interface and callbacks which need arrays to write into in order to be non-allocating. The length of the tuple is dependent on the method.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.full_cache","page":"Integrator Interface","title":"SciMLBase.full_cache","text":"full_cache(i::DEIntegrator)\n\nReturns an iterator over the cache arrays of the method. This can be used to change internal values as needed.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.u_modified!","page":"Integrator Interface","title":"SciMLBase.u_modified!","text":"u_modified!(i::DEIntegrator,bool)\n\nSets bool which states whether a change to u occurred, allowing the solver to handle the discontinuity. By default, this is assumed to be true if a callback is used. This will result in the re-calculation of the derivative at t+dt, which is not necessary if the algorithm is FSAL and u does not experience a discontinuous change at the end of the interval. Thus, if u is unmodified in a callback, a single call to the derivative calculation can be eliminated by u_modified!(integrator,false).\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.get_proposed_dt","page":"Integrator Interface","title":"SciMLBase.get_proposed_dt","text":"get_proposed_dt(i::DEIntegrator)\n\nGets the proposed dt for the next timestep.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.set_proposed_dt!","page":"Integrator Interface","title":"SciMLBase.set_proposed_dt!","text":"set_proposed_dt!(i::DEIntegrator,dt)\nset_proposed_dt!(i::DEIntegrator,i2::DEIntegrator)\n\nSets the proposed dt for the next timestep. If the second argument isa DEIntegrator, then it sets the timestepping of the first argument to match that of the second one. Note that due to PI control and step acceleration, this is more than matching the factors in most cases.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.terminate!","page":"Integrator Interface","title":"SciMLBase.terminate!","text":"terminate!(i::DEIntegrator[, retcode = :Terminated])\n\nTerminates the integrator by emptying tstops. This can be used in events and callbacks to immediately end the solution process.  Optionally, retcode may be specified (see: Return Codes (RetCodes)).\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.change_t_via_interpolation!","page":"Integrator Interface","title":"SciMLBase.change_t_via_interpolation!","text":"change_t_via_interpolation!(integrator::DEIntegrator,t,modify_save_endpoint=Val{false},reinitialize_alg=nothing)\n\nModifies the current t and changes all of the corresponding values using the local interpolation. If the current solution has already been saved, one can provide the optional value modify_save_endpoint to also modify the endpoint of sol in the same manner.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.add_tstop!","page":"Integrator Interface","title":"SciMLBase.add_tstop!","text":"add_tstop!(i::DEIntegrator,t)\n\nAdds a tstop at time t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.has_tstop","page":"Integrator Interface","title":"SciMLBase.has_tstop","text":"has_tstop(i::DEIntegrator)\n\nChecks if integrator has any stopping times defined.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.first_tstop","page":"Integrator Interface","title":"SciMLBase.first_tstop","text":"first_tstop(i::DEIntegrator)\n\nGets the first stopping time of the integrator.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.pop_tstop!","page":"Integrator Interface","title":"SciMLBase.pop_tstop!","text":"pop_tstop!(i::DEIntegrator)\n\nPops the last stopping time from the integrator.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.add_saveat!","page":"Integrator Interface","title":"SciMLBase.add_saveat!","text":"add_saveat!(i::DEIntegrator,t)\n\nAdds a saveat time point at t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Base.resize!","page":"Integrator Interface","title":"Base.resize!","text":"resize!(integrator::DEIntegrator,k::Int)\n\nResizes the DE to a size k. This chops off the end of the array, or adds blank values at the end, depending on whether k > length(integrator.u).\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Base.deleteat!","page":"Integrator Interface","title":"Base.deleteat!","text":"deleteat!(integrator::DEIntegrator,idxs)\n\nShrinks the ODE by deleting the idxs components.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.addat!","page":"Integrator Interface","title":"SciMLBase.addat!","text":"addat!(integrator::DEIntegrator,idxs,val)\n\nGrows the ODE by adding the idxs components. Must be contiguous indices.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.resize_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.resize_non_user_cache!","text":"resize_non_user_cache!(integrator::DEIntegrator,k::Int)\n\nResizes the non-user facing caches to be compatible with a DE of size k. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, resize! simply resizes full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.deleteat_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.deleteat_non_user_cache!","text":"deleteat_non_user_cache!(integrator::DEIntegrator,idxs)\n\ndeleteat!s the non-user facing caches at indices idxs. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, deleteat! simply deleteat!s full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.addat_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.addat_non_user_cache!","text":"addat_non_user_cache!(i::DEIntegrator,idxs)\n\naddat!s the non-user facing caches at indices idxs. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, addat! simply addat!s full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.reinit!","page":"Integrator Interface","title":"SciMLBase.reinit!","text":"reinit!(integrator::DEIntegrator,args...; kwargs...)\n\nThe reinit function lets you restart the integration at a new value.\n\nArguments\n\nu0: Value of u to start at. Default value is integrator.sol.prob.u0\n\nKeyword Arguments\n\nt0: Starting timepoint. Default value is integrator.sol.prob.tspan[1]\ntf: Ending timepoint. Default value is integrator.sol.prob.tspan[2]\nerase_sol=true: Whether to start with no other values in the solution, or keep the previous solution.\ntstops, d_discontinuities, & saveat: Cache where these are stored. Default is the original cache.\nreset_dt: Set whether to reset the current value of dt using the automatic dt determination algorithm. Default is (integrator.dtcache == zero(integrator.dt)) && integrator.opts.adaptive\nreinit_callbacks: Set whether to run the callback initializations again (and initialize_save is for that). Default is true.\nreinit_cache: Set whether to re-run the cache initialization function (i.e. resetting FSAL, not allocating vectors) which should usually be true for correctness. Default is true.\n\nAdditionally, once can access auto_dt_reset! which will run the auto dt initialization algorithm.\n\n\n\n\n\nreinit!(integrator::DDEIntegrator[, u0 = integrator.sol.prob.u0;\n        t0 = integrator.sol.prob.tspan[1],\n        tf = integrator.sol.prob.tspan[2],\n        erase_sol = true,\n        kwargs...])\n\nReinitialize integrator with (optionally) different initial state u0, different integration interval from t0 to tf, and erased solution if erase_sol = true.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.auto_dt_reset!","page":"Integrator Interface","title":"SciMLBase.auto_dt_reset!","text":"auto_dt_reset!(integrator::DEIntegrator)\n\nRun the auto dt initialization algorithm.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.get_du","page":"Integrator Interface","title":"SciMLBase.get_du","text":"get_du(i::DEIntegrator)\n\nReturns the derivative at t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.get_du!","page":"Integrator Interface","title":"SciMLBase.get_du!","text":"get_du!(out,i::DEIntegrator)\n\nWrite the current derivative at t into out.\n\n\n\n\n\n","category":"function"},{"location":"tutorials/sde_example/#Stochastic-Differential-Equations","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"section"},{"location":"tutorials/sde_example/#Example-1:-Scalar-SDEs","page":"Stochastic Differential Equations","title":"Example 1: Scalar SDEs","text":"In this example, we will solve the equation\n\ndu = f(upt)dt + g(upt)dW\n\nwhere f(upt)=αu and g(upt)=βu. We know via Stochastic calculus that the solution to this equation is\n\nu(tWₜ)=u₀expleftleft(α-fracβ^22right)t+βWₜright\n\nTo solve this numerically, we define a stochastic problem type using SDEProblem by specifying f(u, p, t), g(u, p, t), and the initial condition:\n\nimport StochasticDiffEq as SDE\nα = 1\nβ = 1\nu₀ = 1 / 2\nf(u, p, t) = α * u\ng(u, p, t) = β * u\ndt = 1 // 2^4\ntspan = (0.0, 1.0)\nprob = SDE.SDEProblem(f, g, u₀, tspan)\n\nThe solve interface is then the same as ODEs. Here, we will use the classic Euler-Maruyama algorithm EM and plot the solution:\n\nsol = SDE.solve(prob, SDE.EM(), dt = dt);\nnothing # hide\n\nimport Plots\nPlots.plot(sol)","category":"section"},{"location":"tutorials/sde_example/#Using-Higher-Order-Methods","page":"Stochastic Differential Equations","title":"Using Higher Order Methods","text":"One unique feature of DifferentialEquations.jl is that higher-order methods for stochastic differential equations are included. To illustrate it, let us compare the accuracy of the SDE.EM() method and a higher-order method SDE.SRIW1() with the analytical solution. This is a good way to judge the accuracy of a given algorithm, and is also useful to test convergence of new methods being developed. To setup our problem, we define u_analytic(u₀, p, t, W) and pass it to the SDEFunction as:\n\nu_analytic(u₀, p, t, W) = u₀ * exp((α - (β^2) / 2) * t + β * W)\nff = SDE.SDEFunction(f, g, analytic = u_analytic)\nprob = SDE.SDEProblem(ff, u₀, (0.0, 1.0))\n\nWe can now compare the SDE.EM() solution with the analytic one:\n\nsol = SDE.solve(prob, SDE.EM(), dt = dt);\nnothing # hide\n\nPlots.plot(sol, plot_analytic = true)\n\nNow, we choose a higher-order solver SDE.SRIW1() for better accuracy. By default, the higher order methods are adaptive. Let's first switch off adaptivity and compare the numerical and analytic solutions :\n\nsol = SDE.solve(prob, SDE.SRIW1(), dt = dt, adaptive = false);\nnothing # hide\n\nPlots.plot(sol, plot_analytic = true)\n\nNow, let's allow the solver to automatically determine a starting dt. This estimate at the beginning is conservative (small) to ensure accuracy.\n\nsol = SDE.solve(prob, SDE.SRIW1());\nnothing # hide\n\nPlots.plot(sol, plot_analytic = true)\n\nWe can instead start the method with a larger dt by passing it to solve:\n\nsol = SDE.solve(prob, SDE.SRIW1(), dt = dt);\nnothing # hide\n\nPlots.plot(sol, plot_analytic = true)","category":"section"},{"location":"tutorials/sde_example/#Ensemble-Simulations","page":"Stochastic Differential Equations","title":"Ensemble Simulations","text":"Instead of solving single trajectories, we can turn our problem into a EnsembleProblem to solve many trajectories all at once. This is done by the EnsembleProblem constructor:\n\nensembleprob = SDE.EnsembleProblem(prob)\n\nThe solver commands are defined at the Parallel Ensemble Simulations page. For example, we can choose to have 1000 trajectories via trajectories=1000. In addition, this will automatically parallelize using Julia native parallelism if extra processes are added via addprocs(), but we can change this to use multithreading via SDE.EnsembleThreads(). Together, this looks like:\n\nsol = SDE.solve(ensembleprob, SDE.EnsembleThreads(), trajectories = 1000);\n@info \"Ensemble solution computed with $(length(sol)) trajectories\" # hide\nnothing # hide\n\nwarning: Warning\nIf you use a custom noise process, you might need to specify it in a custom prob_func in the EnsembleProblem constructor, as each trajectory needs its own noise process.\n\nMany more controls are defined at the Ensemble simulations page, including analysis tools. A very simple analysis can be done with the EnsembleSummary, which builds mean/var statistics and has an associated plot recipe. For example, we can get the statistics at every 0.01 timesteps and plot the average + error using:\n\nimport StochasticDiffEq as SDE\nsumm = SDE.EnsembleSummary(sol, 0:0.01:1)\nPlots.plot(summ, labels = \"Middle 95%\")\nsumm = SDE.EnsembleSummary(sol, 0:0.01:1; quantiles = [0.25, 0.75])\nPlots.plot!(summ, labels = \"Middle 50%\", legend = true)\n\nAdditionally, we can easily calculate the correlation between the values at t=0.2 and t=0.7 via\n\nSDE.EnsembleAnalysis.timepoint_meancor(sol, 0.2, 0.7) # Gives both means and then the correlation coefficient","category":"section"},{"location":"tutorials/sde_example/#Example-2:-Systems-of-SDEs-with-Diagonal-Noise","page":"Stochastic Differential Equations","title":"Example 2: Systems of SDEs with Diagonal Noise","text":"In general, a system of SDEs\n\ndu = f(upt)dt + g(upt)dW\n\nwhere u is now a vector of variables, f is a vector, and g is a matrix, is numerically integrated in the same way as ODEs. A common scenario, which is the default for DifferentialEquations.jl, is that every variable in the system gets a different random kick. This is the case when g is a diagonal matrix. Correspondingly, we say that we have a diagonal noise.\n\nWe handle this in a simple manner by defining the deterministic part f!(du,u,p,t) and the stochastic part g!(du2,u,p,t) as in-place functions, but note that our convention is that the function g! only defines and modifies the diagonal entries of g matrix.\n\nAs an example, we consider a stochastic variant of the Lorenz equations, where we add to each of u₁, u₂, u₃ their own simple noise 3*N(0,dt). Here, N is the normal distribution and dt is the time step. This is done as follows:\n\nimport StochasticDiffEq as SDE\nimport Plots\n\nfunction f!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\nfunction g!(du, u, p, t)  # It actually represents a diagonal matrix [3.0 0 0; 0 3.0 0; 0 0 3.0]\n    du[1] = 3.0\n    du[2] = 3.0\n    du[3] = 3.0\nend\n\nprob_sde_lorenz = SDE.SDEProblem(f!, g!, [1.0, 0.0, 0.0], (0.0, 10.0))\nsol = SDE.solve(prob_sde_lorenz);\nnothing # hide\n\nPlots.plot(sol, idxs = (1, 2, 3))\n\nNote that it's okay for the noise function to mix terms. For example\n\nfunction g!(du, u, p, t)\n    du[1] = sin(u[3]) * 3.0\n    du[2] = u[2] * u[1] * 3.0\n    du[3] = 3.0\nend\n\nis a valid noise function.","category":"section"},{"location":"tutorials/sde_example/#Example-3:-Systems-of-SDEs-with-Scalar-Noise","page":"Stochastic Differential Equations","title":"Example 3: Systems of SDEs with Scalar Noise","text":"In this example, we'll solve a system of SDEs with scalar noise. This means that the same noise process is applied to all SDEs. We need to define a scalar noise process using the Noise Process interface. Since we want a WienerProcess that starts at 0.0 at time 0.0, we use the command W = SDE.WienerProcess(0.0,0.0,0.0) to define the Brownian motion we want, and then give this to the noise option in the SDEProblem. For a full example, let's solve a linear SDE with scalar noise using a high order algorithm:\n\nimport StochasticDiffEq as SDE\nimport Plots\nf!(du, u, p, t) = (du .= u)\ng!(du, u, p, t) = (du .= u)\nu0 = rand(4, 2)\n\nW = SDE.WienerProcess(0.0, 0.0, 0.0)\nprob = SDE.SDEProblem(f!, g!, u0, (0.0, 1.0), noise = W)\nsol = SDE.solve(prob, SDE.SRIW1());\nnothing # hide\n\nPlots.plot(sol)","category":"section"},{"location":"tutorials/sde_example/#Example-4:-Systems-of-SDEs-with-Non-Diagonal-Noise","page":"Stochastic Differential Equations","title":"Example 4: Systems of SDEs with Non-Diagonal Noise","text":"In the previous examples we had diagonal noise, that is a vector of random numbers dW whose size matches the output of g where the noise is applied element-wise, and scalar noise where a single random variable is applied to all dependent variables. However, a more general type of noise allows for the terms to linearly mixed via g being a general nondiagonal matrix.\n\nNote that nonlinear mixings are not SDEs but fall under the more general class of random ordinary differential equations (RODEs) which have a separate set of solvers.\n\nLet's define a problem with four Wiener processes and two dependent random variables. In this case, we will want the output of g to be a 2x4 matrix, such that the solution is g(u,p,t)*dW, the matrix multiplication. For example, we can do the following:\n\nimport StochasticDiffEq as SDE\nf!(du, u, p, t) = du .= 1.01u\nfunction g!(du, u, p, t)\n    du[1, 1] = 0.3u[1]\n    du[1, 2] = 0.6u[1]\n    du[1, 3] = 0.9u[1]\n    du[1, 4] = 0.12u[1]\n    du[2, 1] = 1.2u[2]\n    du[2, 2] = 0.2u[2]\n    du[2, 3] = 0.3u[2]\n    du[2, 4] = 1.8u[2]\nend\nprob = SDE.SDEProblem(f!, g!, ones(2), (0.0, 1.0), noise_rate_prototype = zeros(2, 4))\n\nIn our g! we define the functions for computing the values of the matrix. We can now think of the SDE that this solves as the system of equations\n\ndu_1 = f_1(upt)dt + g_11(upt)dW_1 + g_12(upt)dW_2 + g_13(upt)dW_3 + g_14(upt)dW_4 \ndu_2 = f_2(upt)dt + g_21(upt)dW_1 + g_22(upt)dW_2 + g_23(upt)dW_3 + g_24(upt)dW_4\n\nmeaning that for example du[1,1] and du[2,1] correspond to stochastic changes with the same random number in the first and second SDEs.\n\nnote: Note\nThis problem can only be solved my SDE methods which are compatible with non-diagonal noise. This is discussed in the SDE solvers page.\n\nThe matrix itself is determined by the keyword argument noise_rate_prototype in the SDEProblem constructor. This is a prototype for the type that du will be in g!. This can be any AbstractMatrix type. Thus, we can define the problem as\n\n# Define a sparse matrix by making a dense matrix and setting some values as not zero\nimport SparseArrays\nA = zeros(2, 4)\nA[1, 1] = 1\nA[1, 4] = 1\nA[2, 4] = 1\nA = SparseArrays.sparse(A)\n\n# Make `g!` write the sparse matrix values\nfunction g!(du, u, p, t)\n    du[1, 1] = 0.3u[1]\n    du[1, 4] = 0.12u[2]\n    du[2, 4] = 1.8u[2]\nend\n\n# Make `g!` use the sparse matrix\nprob = SDE.SDEProblem(f!, g!, ones(2), (0.0, 1.0), noise_rate_prototype = A)\n\nand now g!(u,p,t) writes into a sparse matrix, and g!(u,p,t)*dW is sparse matrix multiplication.","category":"section"},{"location":"tutorials/sde_example/#Example-5:-Colored-Noise","page":"Stochastic Differential Equations","title":"Example 5: Colored Noise","text":"Colored noise can be defined using the Noise Process interface. In that portion of the docs, it is shown how to define your own noise process my_noise, which can be passed to the SDEProblem\n\nSDE.SDEProblem(f!, g!, u0, tspan, noise = my_noise)\n\nNote that general colored noise problems are only compatible with the EM and EulerHeun methods. This is discussed in the SDE solvers page.","category":"section"},{"location":"tutorials/sde_example/#Example:-Spatially-Colored-Noise-in-the-Heston-Model","page":"Stochastic Differential Equations","title":"Example: Spatially-Colored Noise in the Heston Model","text":"Let's define the Heston equation from financial mathematics:\n\nbeginalign*\ndS = μS  dt + sqrtvS  dW_1 \ndv = κ(Θ-v)  dt + σsqrtv  dW_2 \ndW_1  dW_2 = ρ  dt\nendalign*\n\nIn this problem, we have a diagonal noise problem given by:\n\nfunction f!(du, u, p, t)\n    du[1] = μ * u[1]\n    du[2] = κ * (Θ - u[2])\nend\nfunction g!(du, u, p, t)\n    du[1] = √u[2] * u[1]\n    du[2] = σ * √u[2]\nend\n\nHowever, our noise has a correlation matrix for some constant ρ. Choosing ρ=0.2:\n\nρ = 0.2\nΓ = [1 ρ; ρ 1]\n\nTo solve this, we can define a CorrelatedWienerProcess which starts at zero (W(0)=0) via:\n\ntspan = (0.0, 1.0)\nheston_noise = SDE.CorrelatedWienerProcess!(Γ, tspan[1], zeros(2), zeros(2))\n\nThis is then used to build the SDE:\n\nSDE.SDEProblem(f!, g!, ones(2), tspan, noise = heston_noise)\n\nOf course, to fully define this problem, we need to define our constants. Constructors for making common models like this easier to define can be found in the modeling toolkits. For example, the HestonProblem is pre-defined as part of the financial modeling tools.","category":"section"},{"location":"features/linear_nonlinear/#linear_nonlinear","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Specifying (Non)Linear Solvers and Preconditioners","text":"One of the key features of DifferentialEquations.jl is its flexibility. Keeping with this trend, many of the native Julia solvers provided by DifferentialEquations.jl allow you to choose the method for linear and nonlinear solving. This section details how to make that choice.\n\nnote: Note\nWe highly recommend looking at the Solving Large Stiff Equations tutorial, which goes through these options in a real-world example.\n\nwarning: Warning\nThese options do not apply to the Sundials differential equation solvers (CVODE_BDF, CVODE_Adams, ARKODE, and IDA). For complete descriptions of similar functionality for Sundials, see the Sundials ODE solver documentation and Sundials DAE solver documentation.","category":"section"},{"location":"features/linear_nonlinear/#Linear-Solvers:-linsolve-Specification","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Linear Solvers: linsolve Specification","text":"For linear solvers, DifferentialEquations.jl uses LinearSolve.jl. Any LinearSolve.jl algorithm can be used as the linear solver simply by passing the algorithm choice to linsolve. For example, the following tells TRBDF2 to use KLU.jl\n\nTRBDF2(linsolve = KLUFactorization())\n\nMany choices exist, including GPU offloading, so consult the LinearSolve.jl documentation for more details on the choices.","category":"section"},{"location":"features/linear_nonlinear/#Preconditioners:-precs-Specification","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Preconditioners: precs Specification","text":"Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\n\nW: the current Jacobian of the nonlinear system. Specified as either I - gamma J or Igamma - J depending on the algorithm. This will commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy representation of the operator. Users can construct the W-matrix on demand by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since the last call to precs. It is recommended that this is checked to only update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function. Solver-dependent and subject to change.\n\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used.\n\nAdditionally, precs must supply the dispatch:\n\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\n\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr).\n\nThe default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\n\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing","category":"section"},{"location":"features/linear_nonlinear/#Nonlinear-Solvers:-nlsolve-Specification","page":"Specifying (Non)Linear Solvers and Preconditioners","title":"Nonlinear Solvers: nlsolve Specification","text":"All the Julia-based implicit solvers (OrdinaryDiffEq.jl, StochasticDiffEq.jl, etc.) allow for choosing the nonlinear solver that is used to handle the implicit system. While fully modifiable and customizable, most users should stick to the pre-defined nonlinear solver choices. These are:\n\nNLNewton(; κ = 1 // 100, max_iter = 10, fast_convergence_cutoff = 1 // 5, new_W_dt_cutoff = 1 // 5, always_new = false, check_div = true, relax = 0 // 1): A quasi-Newton method. The default.\nNLAnderson(; κ=1//100, max_iter=10, max_history::Int=5, aa_start::Int=1, droptol=nothing, fast_convergence_cutoff=1//5): Anderson acceleration. While more stable than functional iteration, this method is less stable than Newton's method, but does not require a Jacobian.\nNLFunctional(; κ=1//100, max_iter=10, fast_convergence_cutoff=1//5): This method is the least stable, but does not require a Jacobian. It should only be used for non-stiff ODEs.\n\nThe NLNewton solver allows for relaxation via the relax keyword parameter. Numerical values of relax must lie in the half open unit interval [0,1) where 0 corresponds to no relaxation. Alternatively, relax may be set to a line search algorithm from LineSearches.jl in order to include a line search relaxation step in the Newton iterations. For example, the well known Newton-Armijo iterative scheme can be employed by setting relax=BackTracking() where BackTracking is provided by LineSearches.","category":"section"},{"location":"features/diffeq_arrays/#diffeq_arrays","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"In many cases, a standard array may not be enough to fully hold the data for a model. Many of the solvers in DifferentialEquations.jl (only the native Julia methods) allow you to solve problems on AbstractArray types, which allow you to extend the meaning of an array. This page describes some AbstractArray types which can be helpful for modeling differential equations problems.","category":"section"},{"location":"features/diffeq_arrays/#ArrayPartitions","page":"DiffEq-Specific Array Types","title":"ArrayPartitions","text":"ArrayPartitions in DiffEq are used for heterogeneous arrays. For example, DynamicalODEProblem solvers use them internally to turn the separate parts into a single array. You can construct an ArrayPartition using RecursiveArrayTools.jl:\n\nimport RecursiveArrayTools\nA = RecursiveArrayTools.ArrayPartition(x::AbstractArray...)\n\nwhere x is an array of arrays. Then, A will act like a single array, and its broadcast will be type stable, allowing for it to be efficiently used inside the native Julia DiffEq solvers. This is a good way to generate an array which has different units for different parts, or different amounts of precision.","category":"section"},{"location":"features/diffeq_arrays/#Usage","page":"DiffEq-Specific Array Types","title":"Usage","text":"An ArrayPartition acts like a single array. A[i] indexes through the first array, then the second, etc. all linearly. But A.x is where the arrays are stored. Thus for\n\nimport RecursiveArrayTools\nA = RecursiveArrayTools.ArrayPartition(y, z)\n\nWe would have A.x[1]==y and A.x[2]==z. Broadcasting like f.(A) is efficient.","category":"section"},{"location":"features/diffeq_arrays/#Example:-Dynamics-Equations","page":"DiffEq-Specific Array Types","title":"Example: Dynamics Equations","text":"In this example, we will show using heterogeneous units in dynamics equations. Our arrays will be:\n\nimport Unitful, RecursiveArrayTools, DifferentialEquations as DE\nimport LinearAlgebra\n\nr0 = [1131.340, -2282.343, 6672.423]Unitful.u\"km\"\nv0 = [-5.64305, 4.30333, 2.42879]Unitful.u\"km/s\"\nΔt = 86400.0 * 365Unitful.u\"s\"\nμ = 398600.4418Unitful.u\"km^3/s^2\"\nrv0 = RecursiveArrayTools.ArrayPartition(r0, v0)\n\nHere, r0 is the initial positions, and v0 are the initial velocities. rv0 is the ArrayPartition initial condition. We now write our update function in terms of the ArrayPartition:\n\nfunction f(dy, y, μ, t)\n    r = LinearAlgebra.norm(y.x[1])\n    dy.x[1] .= y.x[2]\n    dy.x[2] .= -μ .* y.x[1] / r^3\nend\n\nNotice that y.x[1] is the r part of y, and y.x[2] is the v part of y. Using this kind of indexing is type stable, even though the array itself is heterogeneous. Note that one can also use things like 2y or y.+x and the broadcasting will be efficient.\n\nNow to solve our equations, we do the same thing as always in DiffEq:\n\nprob = DE.ODEProblem(f, rv0, (0.0Unitful.u\"s\", Δt), μ)\nsol = DE.solve(prob, DE.Vern8())","category":"section"},{"location":"features/diffeq_arrays/#MultiScaleArrays","page":"DiffEq-Specific Array Types","title":"MultiScaleArrays","text":"The multi-scale modeling functionality is provided by MultiScaleArrays.jl. It allows for designing a multi-scale model as an extension of an array, which in turn can be directly used in the native Julia solvers of DifferentialEquations.jl.\n\nFor more information, please see the MultiScaleArrays.jl README.","category":"section"},{"location":"tutorials/advanced_ode_example/#stiff","page":"Solving Large Stiff Equations","title":"Solving Large Stiff Equations","text":"This tutorial is for getting into the extra features for solving large stiff ordinary differential equations efficiently. Solving stiff ordinary differential equations requires specializing the linear solver on properties of the Jacobian in order to cut down on the mathcalO(n^3) linear solve and the mathcalO(n^2) back-solves. Note that these same functions and controls also extend to stiff SDEs, DDEs, DAEs, etc. This tutorial is for large-scale models, such as those derived for semi-discretizations of partial differential equations (PDEs). For example, we will use the stiff Brusselator partial differential equation (BRUSS).\n\nnote: Note\nThis tutorial is for advanced users to dive into advanced features! DifferentialEquations.jl automates most of this usage, so we recommend users try solve(prob) with the automatic algorithm first!","category":"section"},{"location":"tutorials/advanced_ode_example/#Definition-of-the-Brusselator-Equation","page":"Solving Large Stiff Equations","title":"Definition of the Brusselator Equation","text":"note: Note\nFeel free to skip this section: it simply defines the example problem.\n\nThe Brusselator PDE is defined on a unit square periodic domain as follows:\n\nbeginalign*\nfracpartial Upartial t = 1 + U^2V - 44U + alpha nabla^2 U + f(x y t)\nfracpartial Vpartial t = 34U - U^2V + alpha nabla^2 V\nendalign*\n\nwhere\n\nf(x y t) = begincases\n5  quad textif  (x-03)^2+(y-06)^2  01^2 text and  t  11\n0  quad textelse\nendcases\n\nand nabla^2 = fracpartial^2partial x^2 + fracpartial^2partial y^2 is the two dimensional Laplacian operator. The above equations are to be solved for a time interval t in 0 115 subject to the initial conditions\n\nbeginalign*\nU(x y 0) = 22cdot (y(1-y))^32 \nV(x y 0) = 27cdot (x(1-x))^32\nendalign*\n\nand the periodic boundary conditions\n\nbeginalign*\nU(x+1yt) = U(xyt) \nV(xy+1t) = V(xyt)\nendalign*\n\nTo solve this PDE, we will discretize it into a system of ODEs with the finite difference method. We discretize the unit square domain with N grid points in each direction. U[i,j] and V[i,j] then represent the value of the discretized field at a given point in time, i.e.\n\nU[i,j] = U(i*dx,j*dy)\nV[i,j] = V(i*dx,j*dy)\n\nwhere dx = dy = 1/N. To implement our ODE system, we collect both U and V in a single array u of size (N,N,2) with u[i,j,1] = U[i,j] and u[i,j,2] = V[i,j]. This approach can be easily generalized to PDEs with larger number of field variables.\n\nUsing a three-point stencil, the Laplacian operator discretizes into a tridiagonal matrix with elements [1 -2 1] and a 1 in the top, bottom, left, and right corners coming from the periodic boundary conditions. The nonlinear terms are implemented pointwise in a straightforward manner.\n\nThe resulting ODEProblem definition is:\n\nimport DifferentialEquations as DE, LinearAlgebra, SparseArrays\n\nconst N = 32\nconst xyd_brusselator = range(0, stop = 1, length = N)\nbrusselator_f(x, y, t) = (((x - 0.3)^2 + (y - 0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.0\nlimit(a, N) = a == N + 1 ? 1 : a == 0 ? N : a\nfunction brusselator_2d_loop(du, u, p, t)\n    A, B, alpha, dx = p\n    alpha = alpha / dx^2\n    @inbounds for I in CartesianIndices((N, N))\n        i, j = Tuple(I)\n        x, y = xyd_brusselator[I[1]], xyd_brusselator[I[2]]\n        ip1, im1, jp1,\n        jm1 = limit(i + 1, N), limit(i - 1, N), limit(j + 1, N),\n        limit(j - 1, N)\n        du[i, j, 1] = alpha * (u[im1, j, 1] + u[ip1, j, 1] + u[i, jp1, 1] + u[i, jm1, 1] -\n                       4u[i, j, 1]) +\n                      B + u[i, j, 1]^2 * u[i, j, 2] - (A + 1) * u[i, j, 1] +\n                      brusselator_f(x, y, t)\n        du[i, j, 2] = alpha * (u[im1, j, 2] + u[ip1, j, 2] + u[i, jp1, 2] + u[i, jm1, 2] -\n                       4u[i, j, 2]) +\n                      A * u[i, j, 1] - u[i, j, 1]^2 * u[i, j, 2]\n    end\nend\np = (3.4, 1.0, 10.0, step(xyd_brusselator))\n\nfunction init_brusselator_2d(xyd)\n    N = length(xyd)\n    u = zeros(N, N, 2)\n    for I in CartesianIndices((N, N))\n        x = xyd[I[1]]\n        y = xyd[I[2]]\n        u[I, 1] = 22 * (y * (1 - y))^(3 / 2)\n        u[I, 2] = 27 * (x * (1 - x))^(3 / 2)\n    end\n    u\nend\nu0 = init_brusselator_2d(xyd_brusselator)\nprob_ode_brusselator_2d = DE.ODEProblem(brusselator_2d_loop, u0, (0.0, 11.5), p)","category":"section"},{"location":"tutorials/advanced_ode_example/#Choosing-Jacobian-Types","page":"Solving Large Stiff Equations","title":"Choosing Jacobian Types","text":"When one is using an implicit or semi-implicit differential equation solver, the Jacobian must be built at many iterations, and this can be one of the most expensive steps. There are two pieces that must be optimized in order to reach maximal efficiency when solving stiff equations: the sparsity pattern and the construction of the Jacobian. The construction is filling the matrix J with values, while the sparsity pattern is what J to use.\n\nThe sparsity pattern is given by a prototype matrix, the jac_prototype, which will be copied to be used as J. The default is for J to be a Matrix, i.e. a dense matrix. However, if you know the sparsity of your problem, then you can pass a different matrix type. For example, a SparseMatrixCSC will give a sparse matrix. Other sparse matrix types include:\n\nBidiagonal\nTridiagonal\nSymTridiagonal\nBandedMatrix (BandedMatrices.jl)\nBlockBandedMatrix (BlockBandedMatrices.jl)\n\nDifferentialEquations.jl will internally use this matrix type, making the factorizations faster by using the specialized forms.","category":"section"},{"location":"tutorials/advanced_ode_example/#Declaring-a-Sparse-Jacobian-with-Automatic-Sparsity-Detection","page":"Solving Large Stiff Equations","title":"Declaring a Sparse Jacobian with Automatic Sparsity Detection","text":"Jacobian sparsity is declared by the jac_prototype argument in the ODEFunction. Note that you should only do this if the sparsity is high, for example, 0.1% of the matrix is non-zeros, otherwise the overhead of sparse matrices can be higher than the gains from sparse differentiation!\n\nADTypes.jl provides a common interface for automatic sparsity detection via its function jacobian_sparsity. This function can be called using sparsity detectors from SparseConnectivityTracer.jl or Symbolics.jl.\n\nWe can give an example du and u and call jacobian_sparsity on our function with the example arguments, and it will kick out a sparse matrix with our pattern, that we can turn into our jac_prototype.\n\nLet's try SparseConnectivityTracer's TracerSparsityDetector:\n\nimport SparseConnectivityTracer, ADTypes\ndetector = SparseConnectivityTracer.TracerSparsityDetector()\ndu0 = copy(u0)\njac_sparsity = ADTypes.jacobian_sparsity(\n    (du, u) -> brusselator_2d_loop(du, u, p, 0.0), du0, u0, detector)\n\nUsing a different backend for sparsity detection just requires swapping out the detector, e.g. for Symbolics' SymbolicsSparsityDetector.\n\nNotice that Julia gives a nice print out of the sparsity pattern. That's neat, and would be tedious to build by hand! Now we just pass it to the ODEFunction like as before:\n\nf = DE.ODEFunction(brusselator_2d_loop; jac_prototype = float.(jac_sparsity))\n\nBuild the ODEProblem:\n\nprob_ode_brusselator_2d_sparse = DE.ODEProblem(f, u0, (0.0, 11.5), p)\n\nNow let's see how the version with sparsity compares to the version without:\n\nimport BenchmarkTools as BT # for @btime\nBT.@btime DE.solve(prob_ode_brusselator_2d, DE.TRBDF2(); save_everystep = false);\nBT.@btime DE.solve(prob_ode_brusselator_2d_sparse, DE.TRBDF2(); save_everystep = false);\nBT.@btime DE.solve(\n    prob_ode_brusselator_2d_sparse, DE.KenCarp47(; linsolve = DE.KLUFactorization());\n    save_everystep = false);\nnothing # hide\n\nNote that depending on the properties of the sparsity pattern, one may want to try alternative linear solvers such as DE.TRBDF2(linsolve = DE.KLUFactorization()) or DE.TRBDF2(linsolve = DE.UMFPACKFactorization()).","category":"section"},{"location":"tutorials/advanced_ode_example/#Using-Jacobian-Free-Newton-Krylov","page":"Solving Large Stiff Equations","title":"Using Jacobian-Free Newton-Krylov","text":"A completely different way to optimize the linear solvers for large sparse matrices is to use a Krylov subspace method. This requires choosing a linear solver for changing to a Krylov method. To swap the linear solver out, we use the linsolve command and choose the GMRES linear solver.\n\nBT.@btime DE.solve(prob_ode_brusselator_2d, DE.KenCarp47(; linsolve = DE.KrylovJL_GMRES());\n    save_everystep = false);\nnothing # hide\n\nNotice that this acceleration does not require the definition of a sparsity pattern, and can thus be an easier way to scale for large problems. For more information on linear solver choices, see the linear solver documentation. linsolve choices are any valid LinearSolve.jl solver.\n\nnote: Note\nSwitching to a Krylov linear solver will automatically change the ODE solver into Jacobian-free mode, dramatically reducing the memory required. This can be overridden by adding concrete_jac=true to the algorithm.","category":"section"},{"location":"tutorials/advanced_ode_example/#Adding-a-Preconditioner","page":"Solving Large Stiff Equations","title":"Adding a Preconditioner","text":"Any LinearSolve.jl-compatible preconditioner can be used as a preconditioner in the linear solver interface. To define preconditioners, one must define a precs function in compatible stiff ODE solvers which returns the left and right preconditioners, matrices which approximate the inverse of W = I - gamma*J used in the solution of the ODE. An example of this with using IncompleteLU.jl is as follows:\n\nimport IncompleteLU\nfunction incompletelu(W, du, u, p, t, newW, Plprev, Prprev, solverdata)\n    if newW === nothing || newW\n        Pl = IncompleteLU.ilu(convert(AbstractMatrix, W), τ = 50.0)\n    else\n        Pl = Plprev\n    end\n    Pl, nothing\nend\n\n# Required due to a bug in Krylov.jl: https://github.com/JuliaSmoothOptimizers/Krylov.jl/pull/477\nBase.eltype(::IncompleteLU.ILUFactorization{Tv, Ti}) where {Tv, Ti} = Tv\n\nBT.@btime DE.solve(prob_ode_brusselator_2d_sparse,\n    DE.KenCarp47(; linsolve = DE.KrylovJL_GMRES(), precs = incompletelu,\n        concrete_jac = true); save_everystep = false);\nnothing # hide\n\nNotice a few things about this preconditioner. This preconditioner uses the sparse Jacobian, and thus we set concrete_jac=true to tell the algorithm to generate the Jacobian (otherwise, a Jacobian-free algorithm is used with GMRES by default). Then newW = true whenever a new W matrix is computed, and newW=nothing during the startup phase of the solver. Thus, we do a check newW === nothing || newW and when true, it's only at these points when we update the preconditioner, otherwise we just pass on the previous version. We use convert(AbstractMatrix,W) to get the concrete W matrix (matching jac_prototype, thus SparseMatrixCSC) which we can use in the preconditioner's definition. Then we use IncompleteLU.ilu on that sparse matrix to generate the preconditioner. We return Pl,nothing to say that our preconditioner is a left preconditioner, and that there is no right preconditioning.\n\nThis method thus uses both the Krylov solver and the sparse Jacobian. Not only that, it is faster than both implementations! IncompleteLU is fussy in that it requires a well-tuned τ parameter. Another option is to use AlgebraicMultigrid.jl which is more automatic. The setup is very similar to before:\n\nimport AlgebraicMultigrid\nfunction algebraicmultigrid(W, du, u, p, t, newW, Plprev, Prprev, solverdata)\n    if newW === nothing || newW\n        Pl = AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.ruge_stuben(convert(AbstractMatrix, W)))\n    else\n        Pl = Plprev\n    end\n    Pl, nothing\nend\n\nBT.@btime DE.solve(prob_ode_brusselator_2d_sparse,\n    DE.KenCarp47(; linsolve = DE.KrylovJL_GMRES(), precs = algebraicmultigrid,\n        concrete_jac = true); save_everystep = false);\nnothing # hide\n\nor with a Jacobi smoother:\n\nfunction algebraicmultigrid2(W, du, u, p, t, newW, Plprev, Prprev, solverdata)\n    if newW === nothing || newW\n        A = convert(AbstractMatrix, W)\n        Pl = AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.ruge_stuben(A,\n            presmoother = AlgebraicMultigrid.Jacobi(rand(size(A, 1))),\n            postsmoother = AlgebraicMultigrid.Jacobi(rand(size(A, 1)))))\n    else\n        Pl = Plprev\n    end\n    Pl, nothing\nend\n\nBT.@btime DE.solve(prob_ode_brusselator_2d_sparse,\n    DE.KenCarp47(; linsolve = DE.KrylovJL_GMRES(), precs = algebraicmultigrid2,\n        concrete_jac = true); save_everystep = false);\nnothing # hide\n\nFor more information on the preconditioner interface, see the linear solver documentation.","category":"section"},{"location":"tutorials/advanced_ode_example/#Sundials-Specific-Handling","page":"Solving Large Stiff Equations","title":"Sundials-Specific Handling","text":"While much of the setup makes the transition to using Sundials automatic, there are some differences between the pure Julia implementations and the Sundials implementations which must be taken note of. These are all detailed in the Sundials solver documentation, but here we will highlight the main details which one should make note of.\n\nDefining a sparse matrix and a Jacobian for Sundials works just like any other package. The core difference is in the choice of the linear solver. With Sundials, the linear solver choice is done with a Symbol in the linear_solver from a preset list. Particular choices of note are :Band for a banded matrix and :GMRES for using GMRES. If you are using Sundials, :GMRES will not require defining the JacVecOperator, and instead will always make use of a Jacobian-Free Newton Krylov (with numerical differentiation). Thus, on this problem we could do:\n\nimport Sundials\nBT.@btime DE.solve(prob_ode_brusselator_2d, Sundials.CVODE_BDF(); save_everystep = false);\n# Simplest speedup: use :LapackDense\nBT.@btime DE.solve(\n    prob_ode_brusselator_2d, Sundials.CVODE_BDF(; linear_solver = :LapackDense);\n    save_everystep = false);\n# GMRES Version: Doesn't require any extra stuff!\nBT.@btime DE.solve(prob_ode_brusselator_2d, Sundials.CVODE_BDF(; linear_solver = :GMRES);\n    save_everystep = false);\nnothing # hide\n\nNotice that using sparse matrices with Sundials requires an analytical Jacobian function. We will use ModelingToolkit.jl's modelingtoolkitize to automatically generate this:\n\nimport ModelingToolkit as MTK\nprob_ode_brusselator_2d_mtk = DE.ODEProblem(\n    MTK.complete(MTK.modelingtoolkitize(prob_ode_brusselator_2d_sparse)),\n    [], (0.0, 11.5); jac = true, sparse = true);\n# BT.@btime DE.solve(prob_ode_brusselator_2d_mtk,Sundials.CVODE_BDF(linear_solver=:KLU),save_everystep=false); # compiles very slowly\nnothing # hide","category":"section"},{"location":"tutorials/advanced_ode_example/#Using-Preconditioners-with-Sundials","page":"Solving Large Stiff Equations","title":"Using Preconditioners with Sundials","text":"Details for setting up a preconditioner with Sundials can be found at the Sundials solver page. Sundials algorithms are very different from the standard Julia-based algorithms in that they require the user does all handling of the Jacobian matrix. To do this, you must define a psetup function that sets up the preconditioner and then a prec function that is the action of the preconditioner on a vector. For the psetup function, we need to first compute the W = I - gamma*J matrix before computing the preconditioner on it. For the ILU example above, this is done for Sundials like:\n\nimport LinearAlgebra\nu0 = prob_ode_brusselator_2d_mtk.u0\np = prob_ode_brusselator_2d_mtk.p\nconst jaccache = prob_ode_brusselator_2d_mtk.f.jac(u0, p, 0.0)\nconst W = LinearAlgebra.I - 1.0 * jaccache\n\nprectmp = IncompleteLU.ilu(W, τ = 50.0)\nconst preccache = Ref(prectmp)\n\nfunction psetupilu(p, t, u, du, jok, jcurPtr, gamma)\n    if jok\n        prob_ode_brusselator_2d_mtk.f.jac(jaccache, u, p, t)\n        jcurPtr[] = true\n\n        # W = I - gamma*J\n        @. W = -gamma * jaccache\n        idxs = LinearAlgebra.diagind(W)\n        @. @view(W[idxs]) = @view(W[idxs]) + 1\n\n        # Build preconditioner on W\n        preccache[] = IncompleteLU.ilu(W, τ = 5.0)\n    end\nend\n\nThen the preconditioner action is to simply use the ldiv! of the generated preconditioner:\n\nfunction precilu(z, r, p, t, y, fy, gamma, delta, lr)\n    ldiv!(z, preccache[], r)\nend\n\nWe then simply pass these functions to the Sundials solver, with a choice of prec_side=1 to indicate that it is a left-preconditioner:\n\nBT.@btime DE.solve(prob_ode_brusselator_2d_sparse,\n    Sundials.CVODE_BDF(; linear_solver = :GMRES, prec = precilu, psetup = psetupilu,\n        prec_side = 1); save_everystep = false);\n\nAnd similarly for algebraic multigrid:\n\nprectmp2 = AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.ruge_stuben(W,\n    presmoother = AlgebraicMultigrid.Jacobi(rand(size(W, 1))),\n    postsmoother = AlgebraicMultigrid.Jacobi(rand(size(W, 1)))))\nconst preccache2 = Ref(prectmp2)\nfunction psetupamg(p, t, u, du, jok, jcurPtr, gamma)\n    if jok\n        prob_ode_brusselator_2d_mtk.f.jac(jaccache, u, p, t)\n        jcurPtr[] = true\n\n        # W = I - gamma*J\n        @. W = -gamma * jaccache\n        idxs = LinearAlgebra.diagind(W)\n        @. @view(W[idxs]) = @view(W[idxs]) + 1\n\n        # Build preconditioner on W\n        preccache2[] = AlgebraicMultigrid.aspreconditioner(AlgebraicMultigrid.ruge_stuben(\n            W,\n            presmoother = AlgebraicMultigrid.Jacobi(rand(size(W, 1))),\n            postsmoother = AlgebraicMultigrid.Jacobi(rand(size(W, 1)))))\n    end\nend\n\nfunction precamg(z, r, p, t, y, fy, gamma, delta, lr)\n    ldiv!(z, preccache2[], r)\nend\n\nBT.@btime DE.solve(prob_ode_brusselator_2d_sparse,\n    Sundials.CVODE_BDF(; linear_solver = :GMRES, prec = precamg, psetup = psetupamg,\n        prec_side = 1); save_everystep = false);","category":"section"},{"location":"api/daskr/#daskr","page":"DASKR.jl","title":"DASKR.jl","text":"This is a wrapper package for importing solvers from DASKR into the SciML interface. DASKR.jl is not automatically included by DifferentialEquations.jl. To use this algorithm, you will need to install and use the package:\n\nusing Pkg\nPkg.add(\"DASKR\")\nimport DASKR\n\nThese methods can be used independently of the rest of DifferentialEquations.jl.","category":"section"},{"location":"api/daskr/#DAE-Solver-APIs","page":"DASKR.jl","title":"DAE Solver APIs","text":"","category":"section"},{"location":"api/daskr/#DASKR.daskr","page":"DASKR.jl","title":"DASKR.daskr","text":"function daskr(;linear_solver=:Dense,\n                  jac_upper=0,jac_lower=0,max_order = 5,\n                  non_negativity_enforcement = 0,\n                  non_negativity_enforcement_array = nothing,\n                  max_krylov_iters = nothing,\n                  num_krylov_vectors = nothing,\n                  max_number_krylov_restarts = 5,\n                  krylov_convergence_test_constant = 0.05,\n                  exclude_algebraic_errors = false)\n\nThis is a wrapper for the well-known DASKR algorithm.\n\nDASKR is a solver for systems of differential-algebraic equations (DAEs). It includes options for both direct and iterative (Krylov) methods for the solution of the linear systems arising at each (implicit) time step. DASKR is a variant of the DASPK package [1].  In addition to all the capabilities of DASPK, DASKR includes the ability to find the roots of a given set of functions while integrating the DAE system.\n\nIn contrast to the older DASSL package, DASKR includes a procedure for calculating consistent initial conditions for a large class of problems (which includes semi-explicit index-1 systems) [2].  This procedure includes options for inequality constraints on selected components.  The package also includes an option to omit the algebraic components from the local error control.\n\nLinear Solver Choice\n\nChoices for the linear solver are:\n\n:Dense\n:Banded\n:SPIGMR, a Krylov method\n\nOther Keyword Arguments\n\njac_upper=0,jac_lower=0: used for setting the upper and lower bands for a banded Jacobian. Defaults to 0. Ignored unless the linear solver is :Banded.\nmax_order = 5: the maximum order of the BDF method.\nnon_negativity_enforcement = 0, whether to enforce non-negativty in the solution. Defaults to 0 or false, can be set to 1 to enable.\nnon_negativity_enforcement_array = nothing, an array of 0's and 1's for specifying non-negativity enforcement to a subset of states.\nmax_krylov_iters=nothing: maximum number of iterations for the Krylov subspace linear solver before rejecting a step. Defaults to nothing or an automatic detection.\nnum_krylov_vectors=nothing: maximum number of history states in the GMRES vector. Defaults to nothing or an automatic choice\nmax_number_krylov_restarts=5: If you figure out what this is, open an issue or PR.\nkrylov_convergence_test_constant = 0.05: Some constant in DASKR's convergence test.\nexclude_algebraic_errors = false: whether algebraic variables are included in the adaptive time stepping error check. Defaults to false.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK","text":"Low-storage Runge-Kutta methods are specialized explicit schemes designed to minimize memory requirements while maintaining high-order accuracy. These methods are essential for large-scale computational fluid dynamics and wave propagation problems where memory constraints are critical.","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Key-Properties","page":"OrdinaryDiffEqLowStorageRK","title":"Key Properties","text":"Low-storage RK methods provide:\n\nDrastically reduced memory usage (typically 2-4 registers vs 7-10 for standard RK)\nHigh-order accuracy comparable to standard RK methods\nPreservation of important stability properties (low dissipation/dispersion)\nScalability to very large PDE discretizations","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#When-to-Use-Low-Storage-RK-Methods","page":"OrdinaryDiffEqLowStorageRK","title":"When to Use Low-Storage RK Methods","text":"These methods are recommended for:\n\nLarge-scale PDE discretizations where memory is the limiting factor\nComputational fluid dynamics and wave propagation simulations\nHigh-performance computing applications with memory constraints\nGPU computations where memory bandwidth is critical\nCompressible flow simulations and aerodynamics\nSeismic wave propagation and acoustic simulations\nProblems with millions or billions of unknowns","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Memory-Efficiency-Comparison","page":"OrdinaryDiffEqLowStorageRK","title":"Memory Efficiency Comparison","text":"Registers refer to the number of copies of the u0 vector that must be stored in memory during integration:\n\nStandard Tsit5: ~9 registers (copies of the state vector)\nLow-storage methods: 2-4 registers (copies of the state vector)\nPractical example: If u0 is from a PDE semi-discretization requiring 2 GB, then Tsit5 needs 18 GB of working memory, while a 2-register method only needs 4 GB and can achieve the same order\nTrade-off: These methods achieve memory reduction by being less computationally efficient, trading compute performance for lower memory requirements","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqLowStorageRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#General-purpose-low-storage","page":"OrdinaryDiffEqLowStorageRK","title":"General-purpose low-storage","text":"CarpenterKennedy2N54: Fourth-order, 5-stage, excellent general choice\nRDPK3Sp510: Fifth-order with only 3 registers, very memory efficient","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Wave-propagation-optimized","page":"OrdinaryDiffEqLowStorageRK","title":"Wave propagation optimized","text":"ORK256: Second-order, 5-stage, optimized for wave equations\nCFRLDDRK64: Low-dissipation and low-dispersion variant\nTSLDDRK74: Seventh-order for high accuracy wave propagation","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Discontinuous-Galerkin-optimized","page":"OrdinaryDiffEqLowStorageRK","title":"Discontinuous Galerkin optimized","text":"DGLDDRK73_C: Optimized for DG discretizations (constrained)\nDGLDDRK84_C, DGLDDRK84_F: Fourth-order DG variants","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Specialized-high-order","page":"OrdinaryDiffEqLowStorageRK","title":"Specialized high-order","text":"NDBLSRK124, NDBLSRK144: Multi-stage fourth-order methods\nSHLDDRK64: Low dissipation and dispersion properties\nRK46NL: Six-stage fourth-order method","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Computational-fluid-dynamics","page":"OrdinaryDiffEqLowStorageRK","title":"Computational fluid dynamics","text":"Carpenter-Kennedy-Lewis series (CKLLSRK*): Optimized for Navier-Stokes equations\nParsani-Ketcheson-Deconinck series (ParsaniKetcheson*): CFD-optimized variants\nRanocha-Dalcin-Parsani-Ketcheson series (RDPK*): Modern CFD methods","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Performance-Considerations","page":"OrdinaryDiffEqLowStorageRK","title":"Performance Considerations","text":"Use only when memory-bound: Standard RK methods are often more efficient when memory is not limiting\nBest for large systems: Most beneficial for problems with >10⁶ unknowns\nGPU acceleration: Particularly effective on memory-bandwidth limited hardware\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqLowStorageRK\", \"CarpenterKennedy2N54\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#Full-list-of-solvers","page":"OrdinaryDiffEqLowStorageRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ORK256","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ORK256","text":"ORK256(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         thread = OrdinaryDiffEq.False(),\n         williamson_condition = true)\n\nExplicit Runge-Kutta Method.  A second-order, five-stage method for wave propagation equations. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nMatteo Bernardini, Sergio Pirozzoli.     A General Strategy for the Optimization of Runge-Kutta Schemes for Wave     Propagation Phenomena.     Journal of Computational Physics, 228(11), pp 4182-4199, 2009.     doi: https://doi.org/10.1016/j.jcp.2009.02.032\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.DGLDDRK73_C","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.DGLDDRK73_C","text":"DGLDDRK73_C(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False(),\n              williamson_condition = true)\n\nExplicit Runge-Kutta Method.  7-stage, third order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems. Optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nT. Toulorge, W. Desmet.     Optimal Runge–Kutta Schemes for Discontinuous Galerkin Space Discretizations     Applied to Wave Propagation Problems.     Journal of Computational Physics, 231(4), pp 2067-2091, 2012.     doi: https://doi.org/10.1016/j.jcp.2011.11.024\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CarpenterKennedy2N54","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CarpenterKennedy2N54","text":"CarpenterKennedy2N54(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                       step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                       thread = OrdinaryDiffEq.False(),\n                       williamson_condition = true)\n\nExplicit Runge-Kutta Method.  A fourth-order, five-stage low-storage method of Carpenter and Kennedy (free 3rd order Hermite interpolant). Fixed timestep only. Designed for hyperbolic PDEs (stability properties).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\n@article{carpenter1994fourth,     title={Fourth-order 2N-storage Runge-Kutta schemes},     author={Carpenter, Mark H and Kennedy, Christopher A},     year={1994}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.NDBLSRK124","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.NDBLSRK124","text":"NDBLSRK124(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             thread = OrdinaryDiffEq.False(),\n             williamson_condition = true)\n\nExplicit Runge-Kutta Method.  12-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nJens Niegemann, Richard Diehl, Kurt Busch.     Efficient Low-Storage Runge–Kutta Schemes with Optimized Stability Regions.     Journal of Computational Physics, 231, pp 364-372, 2012.     doi: https://doi.org/10.1016/j.jcp.2011.09.003\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.NDBLSRK144","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.NDBLSRK144","text":"NDBLSRK144(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             thread = OrdinaryDiffEq.False(),\n             williamson_condition = true)\n\nExplicit Runge-Kutta Method.  14-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nJens Niegemann, Richard Diehl, Kurt Busch.     Efficient Low-Storage Runge–Kutta Schemes with Optimized Stability Regions.     Journal of Computational Physics, 231, pp 364-372, 2012.     doi: https://doi.org/10.1016/j.jcp.2011.09.003\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CFRLDDRK64","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CFRLDDRK64","text":"CFRLDDRK64(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 6-stage, fourth order low-storage, low-dissipation, low-dispersion scheme. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nM. Calvo, J. M. Franco, L. Randez. A New Minimum Storage Runge–Kutta Scheme     for Computational Acoustics. Journal of Computational Physics, 201, pp 1-12, 2004.     doi: https://doi.org/10.1016/j.jcp.2004.05.012\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.TSLDDRK74","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.TSLDDRK74","text":"TSLDDRK74(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 7-stage, fourth order low-storage low-dissipation, low-dispersion scheme with maximal accuracy and stability limit along the imaginary axes. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nKostas Tselios, T. E. Simos. Optimized Runge–Kutta Methods with Minimal Dispersion and Dissipation     for Problems arising from Computational Acoustics. Physics Letters A, 393(1-2), pp 38-47, 2007.     doi: https://doi.org/10.1016/j.physleta.2006.10.072\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.DGLDDRK84_C","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.DGLDDRK84_C","text":"DGLDDRK84_C(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False(),\n              williamson_condition = true)\n\nExplicit Runge-Kutta Method.  8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems. Optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nT. Toulorge, W. Desmet.     Optimal Runge–Kutta Schemes for Discontinuous Galerkin Space Discretizations     Applied to Wave Propagation Problems.     Journal of Computational Physics, 231(4), pp 2067-2091, 2012.     doi: https://doi.org/10.1016/j.jcp.2011.11.024\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.DGLDDRK84_F","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.DGLDDRK84_F","text":"DGLDDRK84_F(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False(),\n              williamson_condition = true)\n\nExplicit Runge-Kutta Method.  8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems. Optimized for PDE discretizations when the maximum spatial step size is not constrained. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nT. Toulorge, W. Desmet.     Optimal Runge–Kutta Schemes for Discontinuous Galerkin Space Discretizations     Applied to Wave Propagation Problems.     Journal of Computational Physics, 231(4), pp 2067-2091, 2012.     doi: https://doi.org/10.1016/j.jcp.2011.11.024\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.SHLDDRK64","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.SHLDDRK64","text":"SHLDDRK64(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False(),\n            williamson_condition = true)\n\nExplicit Runge-Kutta Method.  A fourth-order, six-stage low-storage method. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nD. Stanescu, W. G. Habashi.     2N-Storage Low Dissipation and Dispersion Runge-Kutta Schemes for Computational     Acoustics.     Journal of Computational Physics, 143(2), pp 674-681, 1998.     doi: https://doi.org/10.1006/jcph.1998.5986     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.RK46NL","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.RK46NL","text":"RK46NL(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nJulien Berland, Christophe Bogey, Christophe Bailly. Low-Dissipation and Low-Dispersion Fourth-Order Runge-Kutta Algorithm. Computers & Fluids, 35(10), pp 1459-1463, 2006. doi: https://doi.org/10.1016/j.compfluid.2005.04.003\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S32","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S32","text":"ParsaniKetchesonDeconinck3S32(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 3-stage, second order (3S) low-storage scheme, optimized  the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S82","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S82","text":"ParsaniKetchesonDeconinck3S82(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 8-stage, second order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S53","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S53","text":"ParsaniKetchesonDeconinck3S53(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 5-stage, third order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S173","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S173","text":"ParsaniKetchesonDeconinck3S173(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 17-stage, third order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S94","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S94","text":"ParsaniKetchesonDeconinck3S94(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 9-stage, fourth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S184","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S184","text":"ParsaniKetchesonDeconinck3S184(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 18-stage, fourth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S105","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S105","text":"ParsaniKetchesonDeconinck3S105(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 10-stage, fifth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S205","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.ParsaniKetchesonDeconinck3S205","text":"ParsaniKetchesonDeconinck3S205(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                                 thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 20-stage, fifth order (3S) low-storage scheme, optimized for the spectral difference method applied to wave propagation problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nParsani, Matteo, David I. Ketcheson, and W. Deconinck.     Optimized explicit Runge–Kutta schemes for the spectral difference method applied to wave propagation problems.     SIAM Journal on Scientific Computing 35.2 (2013): A957-A986.     doi: https://doi.org/10.1137/120885899\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK43_2","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK43_2","text":"CKLLSRK43_2(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n              thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 4-stage, third order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK54_3C","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK54_3C","text":"CKLLSRK54_3C(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK95_4S","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK95_4S","text":"CKLLSRK95_4S(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 9-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK95_4C","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK95_4C","text":"CKLLSRK95_4C(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 9-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK95_4M","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK95_4M","text":"CKLLSRK95_4M(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n               thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 9-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK54_3C_3R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK54_3C_3R","text":"CKLLSRK54_3C_3R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK54_3M_3R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK54_3M_3R","text":"CKLLSRK54_3M_3R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK54_3N_3R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK54_3N_3R","text":"CKLLSRK54_3N_3R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK85_4C_3R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK85_4C_3R","text":"CKLLSRK85_4C_3R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK85_4M_3R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK85_4M_3R","text":"CKLLSRK85_4M_3R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK85_4P_3R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK85_4P_3R","text":"CKLLSRK85_4P_3R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK54_3N_4R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK54_3N_4R","text":"CKLLSRK54_3N_4R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK54_3M_4R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK54_3M_4R","text":"CKLLSRK54_3M_4R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 5-stage, fourth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK65_4M_4R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK65_4M_4R","text":"CKLLSRK65_4M_4R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  6-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK85_4FM_4R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK85_4FM_4R","text":"CKLLSRK85_4FM_4R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                   step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                   thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low-Storage Method 8-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.CKLLSRK75_4M_5R","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.CKLLSRK75_4M_5R","text":"CKLLSRK75_4M_5R(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                  thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  CKLLSRK754M5R: Low-Storage Method 7-stage, fifth order low-storage scheme, optimized for compressible Navier–Stokes equations.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{kennedy2000low, title={Low-storage, explicit Runge–Kutta schemes for the compressible Navier–Stokes equations}, author={Kennedy, Christopher A and Carpenter, Mark H and Lewis, R Michael}, journal={Applied numerical mathematics}, volume={35}, number={3}, pages={177–219}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.RDPK3Sp35","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.RDPK3Sp35","text":"RDPK3Sp35(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, five-stage method with embedded error estimator designed for spectral element discretizations of compressible fluid mechanics.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRanocha, Dalcin, Parsani, Ketcheson (2021)     Optimized Runge-Kutta Methods with Automatic Step Size Control for     Compressible Computational Fluid Dynamics     arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.RDPK3SpFSAL35","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.RDPK3SpFSAL35","text":"RDPK3SpFSAL35(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A third-order, five-stage method with embedded error estimator using the FSAL property designed for spectral element discretizations of compressible fluid mechanics.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRanocha, Dalcin, Parsani, Ketcheson (2021)     Optimized Runge-Kutta Methods with Automatic Step Size Control for     Compressible Computational Fluid Dynamics     arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.RDPK3Sp49","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.RDPK3Sp49","text":"RDPK3Sp49(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A fourth-order, nine-stage method with embedded error estimator designed for spectral element discretizations of compressible fluid mechanics.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRanocha, Dalcin, Parsani, Ketcheson (2021)     Optimized Runge-Kutta Methods with Automatic Step Size Control for     Compressible Computational Fluid Dynamics     arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.RDPK3SpFSAL49","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.RDPK3SpFSAL49","text":"RDPK3SpFSAL49(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A fourth-order, nine-stage method with embedded error estimator using the FSAL property designed for spectral element discretizations of compressible fluid mechanics.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRanocha, Dalcin, Parsani, Ketcheson (2021)     Optimized Runge-Kutta Methods with Automatic Step Size Control for     Compressible Computational Fluid Dynamics     arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.RDPK3Sp510","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.RDPK3Sp510","text":"RDPK3Sp510(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A fifth-order, ten-stage method with embedded error estimator designed for spectral element discretizations of compressible fluid mechanics.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRanocha, Dalcin, Parsani, Ketcheson (2021)     Optimized Runge-Kutta Methods with Automatic Step Size Control for     Compressible Computational Fluid Dynamics     arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.RDPK3SpFSAL510","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.RDPK3SpFSAL510","text":"RDPK3SpFSAL510(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                 step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n                 thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  A fifth-order, ten-stage method with embedded error estimator using the FSAL property designed for spectral element discretizations of compressible fluid mechanics.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nRanocha, Dalcin, Parsani, Ketcheson (2021)     Optimized Runge-Kutta Methods with Automatic Step Size Control for     Compressible Computational Fluid Dynamics     arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.HSLDDRK64","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.HSLDDRK64","text":"HSLDDRK64(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False(),\n            williamson_condition = true)\n\nExplicit Runge-Kutta Method.  Low-Storage Method 6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nD. Stanescu, W. G. Habashi.     2N-Storage Low Dissipation and Dispersion Runge-Kutta Schemes for Computational     Acoustics.     Journal of Computational Physics, 143(2), pp 674-681, 1998.     doi: https://doi.org/10.1006/jcph.1998.5986     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.NDBLSRK134","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.NDBLSRK134","text":"NDBLSRK134(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             thread = OrdinaryDiffEq.False(),\n             williamson_condition = true)\n\nExplicit Runge-Kutta Method.  13-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nwilliamson_condition: allows for an optimization that allows fusing broadcast expressions with the function call f. However, it only works for Array types.\n\nReferences\n\nJens Niegemann, Richard Diehl, Kurt Busch.     Efficient Low-Storage Runge–Kutta Schemes with Optimized Stability Regions.     Journal of Computational Physics, 231, pp 364-372, 2012.     doi: https://doi.org/10.1016/j.jcp.2011.09.003\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.SHLDDRK_2N","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.SHLDDRK_2N","text":"SHLDDRK_2N(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n             thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low dissipation and dispersion Runge-Kutta schemes for computational acoustics\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{stanescu19982n,     title={2N-storage low dissipation and dispersion Runge-Kutta schemes for computational acoustics},     author={Stanescu, D and Habashi, WG},     journal={Journal of Computational Physics},     volume={143},     number={2},     pages={674–681},     year={1998},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/LowStorageRK/#OrdinaryDiffEqLowStorageRK.SHLDDRK52","page":"OrdinaryDiffEqLowStorageRK","title":"OrdinaryDiffEqLowStorageRK.SHLDDRK52","text":"SHLDDRK52(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n            thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Low dissipation and dispersion Runge-Kutta schemes for computational acoustics\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{stanescu19982n,     title={2N-storage low dissipation and dispersion Runge-Kutta schemes for computational acoustics},     author={Stanescu, D and Habashi, WG},     journal={Journal of Computational Physics},     volume={143},     number={2},     pages={674–681},     year={1998},     publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"examples/outer_solar_system/#Simulating-the-Outer-Solar-System","page":"Simulating the Outer Solar System","title":"Simulating the Outer Solar System","text":"","category":"section"},{"location":"examples/outer_solar_system/#Data","page":"Simulating the Outer Solar System","title":"Data","text":"The chosen units are masses relative to the sun, meaning the sun has mass 1. We have taken m_0 = 100000597682 to take account of the inner planets. Distances are in astronomical units, times in earth days, and the gravitational constant is thus G = 295912208286 cdot 10^-4.\n\nplanet mass initial position initial velocity\nJupiter m_1 = 0000954786104043 [-3.5023653,   -3.8169847,  -1.5507963] [0.00565429, -0.00412490, -0.00190589]\nSaturn m_2 = 0000285583733151 [9.0755314,    -3.0458353,  -1.6483708] [0.00168318,  0.00483525,  0.00192462]\nUranus m_3 = 00000437273164546 [8.3101420,   -16.2901086,  -7.2521278] [0.00354178,  0.00137102,  0.00055029]\nNeptune m_4 = 00000517759138449 [11.4707666,  -25.7294829, -10.8169456] [0.00288930,  0.00114527,  0.00039677]\nPluto m_5 = 1(13 cdot 10^8 ) [-15.5387357, -25.2225594,  -3.1902382] [0.00276725, -0.00170702, -0.00136504]\n\nThe data is taken from the book “Geometric Numerical Integration” by E. Hairer, C. Lubich and G. Wanner.\n\nimport Plots, OrdinaryDiffEq as ODE\nimport ModelingToolkit as MTK\nusing ModelingToolkit: t_nounits as t, D_nounits as D, @mtkbuild, @variables\nPlots.gr()\n\nG = 2.95912208286e-4\nM = [\n    1.00000597682,\n    0.000954786104043,\n    0.000285583733151,\n    0.0000437273164546,\n    0.0000517759138449,\n    1 / 1.3e8\n]\nplanets = [\"Sun\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"Pluto\"]\n\npos = [0.0 -3.5023653 9.0755314 8.310142 11.4707666 -15.5387357\n       0.0 -3.8169847 -3.0458353 -16.2901086 -25.7294829 -25.2225594\n       0.0 -1.5507963 -1.6483708 -7.2521278 -10.8169456 -3.1902382]\nvel = [0.0 0.00565429 0.00168318 0.00354178 0.0028893 0.00276725\n       0.0 -0.0041249 0.00483525 0.00137102 0.00114527 -0.00170702\n       0.0 -0.00190589 0.00192462 0.00055029 0.00039677 -0.00136504]\ntspan = (0.0, 200_000.0)\n\nThe N-body problem's Hamiltonian is\n\nH(pq) = frac12sum_i=0^Nfracp_i^T p_im_i - Gsum_i=1^N sum_j=0^i-1fracm_i m_jleftlVert q_i - q_j rightrVert\n\nwhere each p_i and q_i is a 3-dimensional vector describing the planet's position and momentum, respectively.\n\nHere, we want to solve for the motion of the five outer planets relative to the sun, namely, Jupiter, Saturn, Uranus, Neptune, and Pluto.\n\nconst ∑ = sum\nconst N = 6\n@variables u(t)[1:3, 1:N]\nu = collect(u)\npotential = -G *\n            ∑(\n    i -> ∑(j -> (M[i] * M[j]) / √(∑(k -> (u[k, i] - u[k, j])^2, 1:3)), 1:(i - 1)),\n    2:N)","category":"section"},{"location":"examples/outer_solar_system/#Hamiltonian-System","page":"Simulating the Outer Solar System","title":"Hamiltonian System","text":"NBodyProblem constructs a second order ODE problem under the hood. We know that a Hamiltonian system has the form of\n\ndotp = -fracpartial Hpartial q quad dotq = fracpartial Hpartial p\n\nFor an N-body system, we can simplify this as:\n\ndotp = -nabla V(q) quad dotq = M^-1 p\n\nThus, dotq is defined by the masses. We only need to define dotp, and this is done internally by taking the gradient of V. Therefore, we only need to pass the potential function and the rest is taken care of.\n\neqs = vec(@. D(D(u))) .~ .-MTK.gradient(potential, vec(u)) ./\n                         repeat(M, inner = 3)\n@mtkbuild sys = MTK.System(eqs, t)\nprob = ODE.ODEProblem(sys, [vec(u .=> pos); vec(D.(u) .=> vel)], tspan)\nsol = ODE.solve(prob, ODE.Tsit5());\n\nplt = Plots.plot()\nfor i in 1:N\n    Plots.plot!(plt, sol, idxs = (u[:, i]...,), lab = planets[i])\nend\nPlots.plot!(plt; xlab = \"x\", ylab = \"y\", zlab = \"z\", title = \"Outer solar system\")","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#OrdinaryDiffEqStabilizedRK","page":"OrdinaryDiffEqStabilizedRK","title":"OrdinaryDiffEqStabilizedRK","text":"Stabilized Runge-Kutta methods are explicit schemes designed to handle moderately stiff problems by extending the stability region through careful tableau construction. These methods use an upper bound on the spectral radius of the Jacobian to achieve much larger stable timesteps than conventional explicit methods. These methods are good for large real eigenvalue problems, but not for problems where the complex eigenvalues are large.","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Key-Properties","page":"OrdinaryDiffEqStabilizedRK","title":"Key Properties","text":"Stabilized RK methods provide:\n\nExtended stability regions for moderately stiff problems\nExplicit formulation avoiding nonlinear solvers\nLarge stable timestep sizes compared to standard explicit methods\nAutomatic spectral radius estimation or user-supplied bounds\nEfficient for parabolic PDEs with moderate stiffness\nGood performance on problems with well-separated timescales","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#When-to-Use-Stabilized-RK-Methods","page":"OrdinaryDiffEqStabilizedRK","title":"When to Use Stabilized RK Methods","text":"These methods are recommended for:\n\nModerately stiff problems where implicit methods are overkill\nParabolic PDEs with diffusion-dominated behavior\nProblems with large spatial grids where implicit methods become expensive\nSystems with well-separated timescales but not extreme stiffness\nCases where explicit is preferred but standard methods are unstable\nLarge-scale problems where linear algebra cost of implicit methods is prohibitive","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Mathematical-Background","page":"OrdinaryDiffEqStabilizedRK","title":"Mathematical Background","text":"Stabilized methods achieve extended stability by constructing tableaus with enlarged stability regions, often using Chebyshev polynomials or orthogonal polynomial techniques. The stable timestep is determined by the spectral radius bound rather than the CFL condition. Important: These methods extend stability primarily along the negative real axis, making them effective for large real eigenvalues but ineffective when complex eigenvalues dominate the stiffness.","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Spectral-Radius-Estimation","page":"OrdinaryDiffEqStabilizedRK","title":"Spectral Radius Estimation","text":"Users can supply an upper bound on the spectral radius using:\n\neigen_est = (integrator) -> integrator.eigen_est = upper_bound\n\nIf not provided, the methods include automatic estimation procedures.","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqStabilizedRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Recommended-stabilized-methods","page":"OrdinaryDiffEqStabilizedRK","title":"Recommended stabilized methods","text":"ROCK2: Second-order ROW-type stabilized method with extended stability\nROCK4: Fourth-order stabilized method for higher accuracy requirements","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Performance-Guidelines","page":"OrdinaryDiffEqStabilizedRK","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#When-stabilized-methods-excel","page":"OrdinaryDiffEqStabilizedRK","title":"When stabilized methods excel","text":"Large real eigenvalue problems where stiffness comes from real eigenvalues\nModerate stiffness ratio (10³ to 10⁶) dominated by real eigenvalues\nLarge spatial discretizations where implicit solver cost is high\nVery large systems where stabilized RK methods are more efficient than BDF methods due to no linear algebra requirements\nParabolic PDEs with diffusion-dominated (real eigenvalue) stiffness\nProblems where spectral radius can be estimated reliably","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#When-to-use-alternatives","page":"OrdinaryDiffEqStabilizedRK","title":"When to use alternatives","text":"Complex eigenvalue dominated problems: Use implicit methods (BDF, SDIRK, Rosenbrock)\nNon-stiff problems: Use standard explicit methods (Tsit5, Verner)","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Usage-Considerations","page":"OrdinaryDiffEqStabilizedRK","title":"Usage Considerations","text":"Spectral radius estimation is crucial for performance\nMethod efficiency depends on stiffness ratio\nTest against implicit methods for highly stiff problems\nConsider adaptive spectral radius estimation for varying stiffness\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqStabilizedRK\", \"ROCK4\")","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#Full-list-of-solvers","page":"OrdinaryDiffEqStabilizedRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#OrdinaryDiffEqStabilizedRK.ROCK2","page":"OrdinaryDiffEqStabilizedRK","title":"OrdinaryDiffEqStabilizedRK.ROCK2","text":"ROCK2(; min_stages = 0,\n        max_stages = 200,\n        eigen_est = nothing)\n\nStabilized Explicit Method. High stability for real eigenvalues. Second order method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\n\nKeyword Arguments\n\nmin_stages: The minimum degree of the Chebyshev polynomial.\nmax_stages: The maximumdegree of the Chebyshev polynomial.\neigen_est: function of the form   (integrator) -> integrator.eigen_est = upper_bound,   where upper_bound is an estimated upper bound on the spectral radius of the Jacobian matrix.   If eigen_est is not provided, upper_bound will be estimated using the power iteration.\n\nReferences\n\nAssyr Abdulle, Alexei A. Medovikov. Second Order Chebyshev Methods based on Orthogonal Polynomials. Numerische Mathematik, 90 (1), pp 1-18, 2001. doi: https://dx.doi.org/10.1007/s002110100292\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#OrdinaryDiffEqStabilizedRK.ROCK4","page":"OrdinaryDiffEqStabilizedRK","title":"OrdinaryDiffEqStabilizedRK.ROCK4","text":"ROCK4(; min_stages = 0,\n        max_stages = 152,\n        eigen_est = nothing)\n\nStabilized Explicit Method. High stability for real eigenvalues. Fourth order method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\n\nKeyword Arguments\n\nmin_stages: The minimum degree of the Chebyshev polynomial.\nmax_stages: The maximumdegree of the Chebyshev polynomial.\neigen_est: function of the form   (integrator) -> integrator.eigen_est = upper_bound,   where upper_bound is an estimated upper bound on the spectral radius of the Jacobian matrix.   If eigen_est is not provided, upper_bound will be estimated using the power iteration.\n\nReferences\n\nAssyr Abdulle. Fourth Order Chebyshev Methods With Recurrence Relation. 2002 Society for Industrial and Applied Mathematics Journal on Scientific Computing, 23(6), pp 2041-2054, 2001. doi: https://doi.org/10.1137/S1064827500379549\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#OrdinaryDiffEqStabilizedRK.RKC","page":"OrdinaryDiffEqStabilizedRK","title":"OrdinaryDiffEqStabilizedRK.RKC","text":"RKC(; eigen_est = nothing)\n\nStabilized Explicit Method. Second order method. Exhibits high stability for real eigenvalues.\n\nKeyword Arguments\n\neigen_est: function of the form   (integrator) -> integrator.eigen_est = upper_bound,   where upper_bound is an estimated upper bound on the spectral radius of the Jacobian matrix.   If eigen_est is not provided, upper_bound will be estimated using the power iteration.\n\nReferences\n\nB. P. Sommeijer, L. F. Shampine, J. G. Verwer. RKC: An Explicit Solver for Parabolic PDEs, Journal of Computational and Applied Mathematics, 88(2), pp 315-326, 1998. doi: https://doi.org/10.1016/S0377-0427(97)00219-7\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#OrdinaryDiffEqStabilizedRK.SERK2","page":"OrdinaryDiffEqStabilizedRK","title":"OrdinaryDiffEqStabilizedRK.SERK2","text":"SERK2(; controller = :PI\n        eigen_est = nothing)\n\nStabilized Explicit Method. Second order method.\n\nKeyword Arguments\n\ncontroller: TBD\neigen_est: function of the form   (integrator) -> integrator.eigen_est = upper_bound,   where upper_bound is an estimated upper bound on the spectral radius of the Jacobian matrix.   If eigen_est is not provided, upper_bound will be estimated using the power iteration.\n\nReferences\n\n@article{kleefeld2013serk2v2, title={SERK2v2: A new second-order stabilized explicit Runge-Kutta method for stiff problems}, author={Kleefeld, B and Martin-Vaquero, J}, journal={Numerical Methods for Partial Differential Equations}, volume={29}, number={1}, pages={170–185}, year={2013}, publisher={Wiley Online Library}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#OrdinaryDiffEqStabilizedRK.ESERK4","page":"OrdinaryDiffEqStabilizedRK","title":"OrdinaryDiffEqStabilizedRK.ESERK4","text":"ESERK4(; eigen_est = nothing)\n\nStabilized Explicit Method. Fourth order method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\n\nKeyword Arguments\n\neigen_est: function of the form   (integrator) -> integrator.eigen_est = upper_bound,   where upper_bound is an estimated upper bound on the spectral radius of the Jacobian matrix.   If eigen_est is not provided, upper_bound will be estimated using the power iteration.\n\nReferences\n\nJ. Martín-Vaquero, B. Kleefeld. Extrapolated stabilized explicit Runge-Kutta methods, Journal of Computational Physics, 326, pp 141-155, 2016. doi: https://doi.org/10.1016/j.jcp.2016.08.042.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semiimplicit/StabilizedRK/#OrdinaryDiffEqStabilizedRK.ESERK5","page":"OrdinaryDiffEqStabilizedRK","title":"OrdinaryDiffEqStabilizedRK.ESERK5","text":"ESERK5(; eigen_est = nothing)\n\nStabilized Explicit Method. Fifth order method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\n\nKeyword Arguments\n\neigen_est: function of the form   (integrator) -> integrator.eigen_est = upper_bound,   where upper_bound is an estimated upper bound on the spectral radius of the Jacobian matrix.   If eigen_est is not provided, upper_bound will be estimated using the power iteration.\n\nReferences\n\nJ. Martín-Vaquero, A. Kleefeld. ESERK5: A fifth-order extrapolated stabilized explicit Runge-Kutta method, Journal of Computational and Applied Mathematics, 356, pp 22-36, 2019. doi: https://doi.org/10.1016/j.cam.2019.01.040.\n\n\n\n\n\n","category":"type"},{"location":"examples/min_and_max/#Finding-Maxima-and-Minima-of-ODEs-Solutions","page":"Finding Maxima and Minima of ODEs Solutions","title":"Finding Maxima and Minima of ODEs Solutions","text":"","category":"section"},{"location":"examples/min_and_max/#Setup","page":"Finding Maxima and Minima of ODEs Solutions","title":"Setup","text":"In this tutorial, we will show how to use Optimization.jl to find the maxima and minima of solutions. Let's take a look at the double pendulum:\n\n#Constants and setup\nimport OrdinaryDiffEq as ODE\ninitial = [0.01, 0.01, 0.01, 0.01]\ntspan = (0.0, 100.0)\n\n#Define the problem\nfunction double_pendulum_hamiltonian(udot, u, p, t)\n    α, lα, β, lβ = u\n    udot .= [2(lα - (1 + cos(β))lβ) / (3 - cos(2β)),\n        -2sin(α) - sin(α + β),\n        2(-(1 + cos(β))lα + (3 + 2cos(β))lβ) / (3 - cos(2β)),\n        -sin(α + β) - 2sin(β) * (((lα - lβ)lβ) / (3 - cos(2β))) +\n        2sin(2β) * ((lα^2 - 2(1 + cos(β))lα * lβ + (3 + 2cos(β))lβ^2) / (3 - cos(2β))^2)]\nend\n\n#Pass to solvers\npoincare = ODE.ODEProblem(double_pendulum_hamiltonian, initial, tspan)\n\nsol = ODE.solve(poincare, ODE.Tsit5())\n\nIn time, the solution looks like:\n\nimport Plots;\nPlots.gr();\nPlots.plot(sol, vars = [(0, 3), (0, 4)], leg = false, plotdensity = 10000)\n\nwhile it has the well-known phase-space plot:\n\nPlots.plot(sol, vars = (3, 4), leg = false)","category":"section"},{"location":"examples/min_and_max/#Local-Optimization","page":"Finding Maxima and Minima of ODEs Solutions","title":"Local Optimization","text":"Let's find out what some of the local maxima and minima are. Optim.jl can be used to minimize functions, and the solution type has a continuous interpolation which can be used. Let's look for the local optima for the 4th variable around t=20. Thus, our optimization function is:\n\nf(t, _) = sol(first(t), idxs = 4)\n\nfirst(t) is the same as t[1] which transforms the array of size 1 into a number. idxs=4 is the same as sol(first(t))[4] but does the calculation without a temporary array and thus is faster. To find a local minimum, we can solve the optimization problem where the loss function is f:\n\nimport Optimization as OPT, OptimizationNLopt as OptNL, ForwardDiff\noptf = OPT.OptimizationFunction(f, OPT.AutoForwardDiff())\nmin_guess = 18.0\noptprob = OPT.OptimizationProblem(optf, [min_guess], lb = [0.0], ub = [100.0])\nopt = OPT.solve(optprob, OptNL.NLopt.LD_LBFGS())\n\nFrom this printout, we see that the minimum is at t=18.63 and the value is -2.79e-2. We can get these in code-form via:\n\nprintln(opt.u)\n\nTo get the maximum, we just minimize the negative of the function:\n\nfminus(t, _) = -sol(first(t), idxs = 4)\n\noptf = OPT.OptimizationFunction(fminus, OPT.AutoForwardDiff())\nmin_guess = 22.0\noptprob2 = OPT.OptimizationProblem(optf, [min_guess], lb = [0.0], ub = [100.0])\nopt2 = OPT.solve(optprob2, OptNL.NLopt.LD_LBFGS())\n\nLet's add the maxima and minima to the plots:\n\nPlots.plot(sol, vars = (0, 4), plotdensity = 10000)\nPlots.scatter!([opt.u], [opt.minimum], label = \"Local Min\")\nPlots.scatter!([opt2.u], [-opt2.minimum], label = \"Local Max\")","category":"section"},{"location":"examples/min_and_max/#Global-Optimization","page":"Finding Maxima and Minima of ODEs Solutions","title":"Global Optimization","text":"If we instead want to find global maxima and minima, we need to look somewhere else. There are many choices for this. A pure Julia option are the BlackBoxOptim solvers within Optimization.jl, but I will continue the story with the  OptimizationNLopt methods. To do this, we simply swap out to one of the global optimizers in the list. Let's try GN_ORIG_DIRECT_L:\n\ngopt = OPT.solve(optprob, OptNL.NLopt.GN_ORIG_DIRECT_L())\ngopt2 = OPT.solve(optprob2, OptNL.NLopt.GN_ORIG_DIRECT_L())\n\n@show gopt.u, gopt2.u\n\nPlots.plot(sol, vars = (0, 4), plotdensity = 10000)\nPlots.scatter!([gopt.u], [gopt.minimum], label = \"Global Min\")\nPlots.scatter!([gopt2.u], [-gopt2.minimum], label = \"Global Max\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#OrdinaryDiffEqPRK","page":"OrdinaryDiffEqPRK","title":"OrdinaryDiffEqPRK","text":"Parallel Runge-Kutta (PRK) methods are explicit solvers specifically designed to exploit parallelism by making multiple independent evaluations of the ODE function f simultaneously. These methods are optimized for parallel computing environments where function evaluations can be distributed across multiple processors.\n\nwarning: Research and Development\nThese methods are currently in research and development and not intended for general use.","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Key-Properties","page":"OrdinaryDiffEqPRK","title":"Key Properties","text":"PRK methods provide:\n\nExplicit parallelism in function evaluations within each timestep\nFixed processor count optimization for specific parallel architectures\nIndependent stage evaluations that can run simultaneously\nMaintained accuracy while achieving parallel speedup\nSpecialized tableaus designed for parallel efficiency","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#When-to-Use-PRK-Methods","page":"OrdinaryDiffEqPRK","title":"When to Use PRK Methods","text":"These methods are recommended for:\n\nParallel computing environments with multiple processors available\nExpensive function evaluations that benefit from parallelization\nSystems where function evaluation dominates computational cost\nApplications with fixed parallel architecture (e.g., exactly 2 or 5 processors)\nProblems where parallel speedup outweighs method overhead","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Important-Considerations","page":"OrdinaryDiffEqPRK","title":"Important Considerations","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Parallel-Requirements","page":"OrdinaryDiffEqPRK","title":"Parallel Requirements","text":"Requires multiple processors to achieve benefits\nFunction evaluations must be parallelizable (no data dependencies)\nParallel overhead must be less than speedup gains\nFixed processor count optimization may not match available hardware","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#When-NOT-to-Use","page":"OrdinaryDiffEqPRK","title":"When NOT to Use","text":"Sequential computing environments\nCheap function evaluations where parallel overhead dominates\nMemory-bound problems where parallelism doesn't help\nVariable processor availability scenarios\nLarge systems where LU factorization of implicit steps parallelizes efficiently (around 200×200 matrices and larger on modern processors)","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Mathematical-Background","page":"OrdinaryDiffEqPRK","title":"Mathematical Background","text":"PRK methods rearrange traditional Runge-Kutta tableaus to allow stage evaluations to be computed independently and simultaneously. The specific processor count determines the tableau structure and achievable parallelism.","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqPRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Available-methods","page":"OrdinaryDiffEqPRK","title":"Available methods","text":"KuttaPRK2p5: Fifth-order method optimized for 2 processors","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Usage-considerations","page":"OrdinaryDiffEqPRK","title":"Usage considerations","text":"Best with exactly 2 processors for KuttaPRK2p5\nFunction evaluation must support parallel execution\nTest parallel efficiency against sequential high-order methods\nConsider problem-specific parallel architecture","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Performance-Guidelines","page":"OrdinaryDiffEqPRK","title":"Performance Guidelines","text":"Measure actual speedup vs sequential methods on target hardware\nAccount for parallel overhead in performance comparisons\nConsider memory bandwidth limitations in parallel environments\nCompare against other parallelization strategies (e.g., spatial domain decomposition)","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Alternative-Parallelization-Approaches","page":"OrdinaryDiffEqPRK","title":"Alternative Parallelization Approaches","text":"For most problems, consider these alternatives:\n\nSpatial domain decomposition for PDE problems\nMultiple trajectory parallelism for Monte Carlo simulations\nVectorized operations within function evaluations\nHigh-order sequential methods with better single-thread performance\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqPRK\", \"KuttaPRK2p5\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#Full-list-of-solvers","page":"OrdinaryDiffEqPRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/PRK/#OrdinaryDiffEqPRK.KuttaPRK2p5","page":"OrdinaryDiffEqPRK","title":"OrdinaryDiffEqPRK.KuttaPRK2p5","text":"KuttaPRK2p5(; thread = OrdinaryDiffEq.True())\n\nExplicit Runge-Kutta Method A 5 parallel, 2 processor method of 5th order.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{jackson1995potential, title={The potential for parallelism in Runge–Kutta methods. Part 1: RK formulas in standard form}, author={Jackson, Kenneth R and Norsett, Syvert Paul}, journal={SIAM journal on numerical analysis}, volume={32}, number={1}, pages={49–82}, year={1995}, publisher={SIAM}}\n\n\n\n\n\n","category":"type"},{"location":"solvers/split_ode_solve/#split_ode_solve","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The solvers which are available for a SplitODEProblem depend on the input linearity and number of components. Each solver has functional form (or many) that it allows.","category":"section"},{"location":"solvers/split_ode_solve/#Implicit-Explicit-(IMEX)-ODE","page":"Split ODE Solvers","title":"Implicit-Explicit (IMEX) ODE","text":"The Implicit-Explicit (IMEX) ODE is a SplitODEProblem with two functions:\n\nfracdudt =  f_1(tu) + f_2(tu)\n\nwhere the first function is the stiff part and the second function is the non-stiff part (implicit integration on f1, explicit integration on f2).","category":"section"},{"location":"solvers/split_ode_solve/#Recommended-Methods","page":"Split ODE Solvers","title":"Recommended Methods","text":"The recommended method in most cases is KenCarp4. In cases of extreme stiffness or for high tolerances, KenCarp3 can be a good choice. The ARKODE methods are generally inefficient and diverge unless the options are tweaked to match the problem, though for large enough PDEs the ARKODE method with linear_solver=:GMRES is a good choice.","category":"section"},{"location":"solvers/split_ode_solve/#OrdinaryDiffEq.jl","page":"Split ODE Solvers","title":"OrdinaryDiffEq.jl","text":"SplitEuler: 1st order fully explicit method. Used for testing accuracy of splits.\nIMEXEuler : 1st order explicit Euler mixed with implicit Euler. Fixed time step only.\nCNAB2: Crank-Nicolson Adams Bashforth Order 2. Fixed time step only.\nCNLF2: Crank-Nicolson Leapfrog of Order 2. Fixed time step only.\nSBDF2 : 2nd order IMEX BDF method. Fixed time step only.\nSBDF3 : 3rd order IMEX BDF method. Fixed time step only. In development.\nSBDF4 : 4th order IMEX BDF method. Fixed time step only. In development.\nKenCarp3: An A-L stable stiffly-accurate 3rd order ESDIRK method.\nKenCarp4: An A-L stable stiffly-accurate 4th order ESDIRK method.\nKenCarp47 - An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method with splitting\nKenCarp5: An A-L stable stiffly-accurate 5th order ESDIRK method.\nKenCarp58 - An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method with splitting","category":"section"},{"location":"solvers/split_ode_solve/#Sundials.jl","page":"Split ODE Solvers","title":"Sundials.jl","text":"ARKODE: An additive Runge-Kutta method. Order between 3rd and 5th. For a list of available options, please see its ODE solver page.","category":"section"},{"location":"solvers/split_ode_solve/#Semilinear-ODE","page":"Split ODE Solvers","title":"Semilinear ODE","text":"The Semilinear ODE is a SplitODEProblem with one linear operator and one nonlinear function:\n\nfracdudt =  Au + f(tu)\n\nSee the documentation page for SciMLOperators for details about how to define linear operators from a matrix or finite difference discretization of derivative operators.\n\nThe appropriate algorithms for this form are:","category":"section"},{"location":"solvers/split_ode_solve/#OrdinaryDiffEq.jl-2","page":"Split ODE Solvers","title":"OrdinaryDiffEq.jl","text":"LawsonEuler - First order exponential Euler scheme. Fixed timestepping only.\nNorsettEuler - First order exponential-RK scheme. Fixed timestepping only. Alias: ETD1.\nETD2 - Second order Exponential Time Differencing method (in development). Fixed timestepping only. Doesn't support Krylov approximation.\nETDRK2 - 2nd order exponential-RK scheme. Fixed timestepping only.\nETDRK3 - 3rd order exponential-RK scheme. Fixed timestepping only.\nETDRK4 - 4th order exponential-RK scheme. Fixed timestepping only.\nHochOst4 - 4th order exponential-RK scheme with stiff order 4. Fixed timestepping only.\n\nNote that the generic algorithms GenericIIF1 and GenericIIF2 allow for a choice of nlsolve.\n\nBy default, the exponential methods cache matrix functions such as exp(dt*A) to accelerate the time stepping for small systems. For large systems, using Krylov-based versions of the methods can allow for lazy calculation of exp(dt*A)*v and similar entities, and thus improve performance.\n\nTo tell a solver to use Krylov methods, pass krylov=true to its constructor. You can also manually set the size of the Krylov subspace by setting the m parameter, which defaults to 30. For example\n\nLawsonEuler(krylov = true, m = 50)\n\nconstructs a Lawson-Euler method, which uses a size-50 Krylov subspace. Note that m only sets an upper bound to the Krylov subspace size. If a convergence criterion is met (determined by the reltol of the integrator), “happy breakdown” will occur and the Krylov subspace will only be constructed partially.\n\nFor more advanced control over the Krylov algorithms, you can change the length of the incomplete orthogonalization procedure (IOP) [1] by setting the iop parameter in the constructor. By default, IOP is turned off and full Arnoldi iteration is used. Note that if the linear operator is hermitian, then the Lanczos algorithm will always be used and IOP setting is ignored.\n\n[1]: Koskela, A. (2015). Approximating the matrix exponential of an advection-diffusion operator using the incomplete orthogonalization method. In Numerical Mathematics and Advanced Applications-ENUMATH 2013 (pp. 345-353). Springer, Cham.","category":"section"},{"location":"tutorials/dde_example/#Delay-Differential-Equations","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"This tutorial will introduce you to the functionality for solving delay differential equations.\n\nnote: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.\n\nDelay differential equations are equations which have a delayed argument. To allow for specifying the delayed argument, the function definition for a delay differential equation is expanded to include a history function h(p, t) which uses interpolations throughout the solution's history to form a continuous extension of the solver's past and depends on parameters p and time t. The function signature for a delay differential equation is f(u, h, p, t) for not in-place computations, and f(du, u, h, p, t) for in-place computations.\n\nIn this example, we will solve a model of breast cancer growth kinetics:\n\nbeginaligned\ndx_0 = fracv_01+beta_0left(x_2(t-tau)right)^2left(p_0-q_0right)x_0(t)-d_0x_0(t)\ndx_1 = fracv_01+beta_0left(x_2(t-tau)right)^2left(1-p_0+q_0right)x_0(t)\n       + fracv_11+beta_1left(x_2(t-tau)right)^2left(p_1-q_1right)x_1(t)-d_1x_1(t)\ndx_2 = fracv_11+beta_1left(x_2(t-tau)right)^2left(1-p_1+q_1right)x_1(t)-d_2x_2(t)\nendaligned\n\nFor this problem, we note that tau is constant, and thus we can use a method which exploits this behavior. We first write out the equation using the appropriate function signature. Most of the equation writing is the same, though we use the history function by first interpolating and then choosing the components. Thus, the ith component at time t-tau is given by h(p, t-tau)[i]. Components with no delays are written as in the ODE.\n\nThus, the function for this model is given by:\n\nimport DelayDiffEq as DDE, DifferentialEquations as DE\nfunction bc_model(du, u, h, p, t)\n    p0, q0, v0, d0, p1, q1, v1, d1, d2, beta0, beta1, tau = p\n    hist3 = h(p, t - tau)[3]\n    du[1] = (v0 / (1 + beta0 * (hist3^2))) * (p0 - q0) * u[1] - d0 * u[1]\n    du[2] = (v0 / (1 + beta0 * (hist3^2))) * (1 - p0 + q0) * u[1] +\n            (v1 / (1 + beta1 * (hist3^2))) * (p1 - q1) * u[2] - d1 * u[2]\n    du[3] = (v1 / (1 + beta1 * (hist3^2))) * (1 - p1 + q1) * u[2] - d2 * u[3]\nend\n\nNow we build a DDEProblem. The signature\n\nprob = DDE.DDEProblem(f, u0, h, tspan, p = SciMLBase.NullParameters();\n    constant_lags = [], dependent_lags = [], kwargs...)\n\nis very similar to ODEs, where we now have to give the lags and a function h. h is the history function that declares what the values were before the time the model starts. Here we will assume that for all time before t0 the values were 1 and define h as an out-of-place function:\n\nh(p, t) = ones(3)\n\nTo use the constant lag model, we have to declare the lags. Here we will use tau=1.\n\ntau = 1\nlags = [tau]\n\nNext, we choose to solve on the timespan (0.0,10.0) and create the problem type:\n\np0 = 0.2;\nq0 = 0.3;\nv0 = 1;\nd0 = 5;\np1 = 0.2;\nq1 = 0.3;\nv1 = 1;\nd1 = 1;\nd2 = 1;\nbeta0 = 1;\nbeta1 = 1;\np = (p0, q0, v0, d0, p1, q1, v1, d1, d2, beta0, beta1, tau)\ntspan = (0.0, 10.0)\nu0 = [1.0, 1.0, 1.0]\n\nprob = DDE.DDEProblem(bc_model, u0, h, tspan, p; constant_lags = lags)\n\nAn efficient way to solve this problem (given the constant lags) is with the MethodOfSteps solver. Through the magic that is Julia, it translates an OrdinaryDiffEq.jl ODE solver method into a method for delay differential equations, which is highly efficient due to sweet compiler magic. A good choice is the order 5 method DDE.Tsit5():\n\nalg = DDE.MethodOfSteps(DE.Tsit5())\n\nFor lower tolerance solving, one can use the DDE.BS3() algorithm to good effect (this combination is similar to the MATLAB dde23, but more efficient tableau), and for high tolerances the DDE.Vern6() algorithm will give a 6th order solution.\n\nTo solve the problem with this algorithm, we do the same thing we'd do with other methods on the common interface:\n\nsol = DDE.solve(prob, alg);\n@info \"Solution computed with $(length(sol.t)) timesteps\" # hide\nnothing # hide\n\nNote that everything available to OrdinaryDiffEq.jl can be used here, including event handling and other callbacks. The solution object has the same interface as for ODEs. For example, we can use the same plot recipes to view the results:\n\nimport Plots\nPlots.plot(sol)","category":"section"},{"location":"tutorials/dde_example/#Speeding-Up-Interpolations-with-Idxs","page":"Delay Differential Equations","title":"Speeding Up Interpolations with Idxs","text":"We can speed up the previous problem in two different ways. First of all, if we need to interpolate multiple values from a previous time, we can use the in-place form for the history function h(out, p, t) which writes the output to out. In this case, we must supply the history initial conditions as in-place as well. For the previous example, that's simply\n\nh(out, p, t) = (out .= 1.0)\n\nand then our DDE is:\n\nconst out = zeros(3) # Define a cache variable\nfunction bc_model(du, u, h, p, t)\n    h(out, p, t - tau) # updates out to be the correct history function\n    du[1] = (v0 / (1 + beta0 * (out[3]^2))) * (p0 - q0) * u[1] - d0 * u[1]\n    du[2] = (v0 / (1 + beta0 * (out[3]^2))) * (1 - p0 + q0) * u[1] +\n            (v1 / (1 + beta1 * (out[3]^2))) * (p1 - q1) * u[2] - d1 * u[2]\n    du[3] = (v1 / (1 + beta1 * (out[3]^2))) * (1 - p1 + q1) * u[2] - d2 * u[3]\nend\n\nHowever, we can do something even slicker in most cases. We only ever needed to interpolate past values at index 3. Instead of generating a bunch of arrays, we can instead ask specifically for that value by passing the keyword idxs = 3. The DDE function bc_model is now:\n\nfunction bc_model(du, u, h, p, t)\n    u3_past_sq = h(p, t - tau; idxs = 3)^2\n    du[1] = (v0 / (1 + beta0 * (u3_past_sq))) * (p0 - q0) * u[1] - d0 * u[1]\n    du[2] = (v0 / (1 + beta0 * (u3_past_sq))) * (1 - p0 + q0) * u[1] +\n            (v1 / (1 + beta1 * (u3_past_sq))) * (p1 - q1) * u[2] - d1 * u[2]\n    du[3] = (v1 / (1 + beta1 * (u3_past_sq))) * (1 - p1 + q1) * u[2] - d2 * u[3]\nend\n\nNote that this requires that we define the historical values\n\nh(p, t; idxs = nothing) = typeof(idxs) <: Number ? 1.0 : ones(3)\n\nwhere idxs can be an integer for which variable in the history to compute, and here for any number idxs we give back 1.0. Note that if we wanted to use past values of the ith derivative, then we would call the history function h(p, t, Val{i}) in our DDE function and would have to define a dispatch like\n\nh(p, t, ::Type{Val{1}}) = zeros(3)\n\nto say that derivatives before t0 are zero for any index. Again, we could use an in-place function instead or only compute specific indices by passing an idxs keyword.\n\nThe functional forms for the history function are also discussed on the DDEProblem page.","category":"section"},{"location":"tutorials/dde_example/#Undeclared-Delays-and-State-Dependent-Delays-via-Residual-Control","page":"Delay Differential Equations","title":"Undeclared Delays and State-Dependent Delays via Residual Control","text":"You might have noticed DifferentialEquations.jl allows you to solve problems with undeclared delays, since you can interpolate h at any value. This is a feature, but use it with caution. Undeclared delays can increase the error in the solution. It's recommended that you use a method with a residual control, such as MethodOfSteps(DDE.RK4()) whenever there are undeclared delays. With this, you can use interpolated derivatives, solve functional differential equations by using quadrature on the interpolant, etc. However, note that residual control solves with a low level of accuracy, so the tolerances should be made very small, and the solution should not be trusted for more than 2-3 decimal places.\n\nNote: MethodOfSteps(DDE.RK4()) with undeclared delays is similar to MATLAB's ddesd. Thus, for example, the following is similar to solving the example from above with residual control:\n\nprob = DDE.DDEProblem(bc_model, u0, h, tspan)\nalg = DDE.MethodOfSteps(DE.RK4())\nsol = DDE.solve(prob, alg);\nnothing # hide\n\nNote that this method can solve problems with state-dependent delays.","category":"section"},{"location":"tutorials/dde_example/#State-Dependent-Delay-Discontinuity-Tracking","page":"Delay Differential Equations","title":"State-Dependent Delay Discontinuity Tracking","text":"State-dependent delays are problems where the delay is allowed to be a function of the current state. They can be more efficiently solved with discontinuity tracking. To do this, in DifferentialEquations.jl, requires passing lag functions g(u,p,t) as keyword dependent_lags to the DDEProblem definition. Other than that, everything else is the same, and one solves that problem using the common interface.\n\nWe can solve the above problem with dependent delay tracking by declaring the dependent lags and solving with a MethodOfSteps algorithm:\n\nprob = DDE.DDEProblem(bc_model, u0, h, tspan; dependent_lags = ((u, p, t) -> tau,))\nalg = DDE.MethodOfSteps(DE.Tsit5())\nsol = DDE.solve(prob, alg);\nnothing # hide\n\nHere, we treated the single lag t-tau as a state-dependent delay. Of course, you can then replace that tuple of functions with whatever functions match your lags.","category":"section"},{"location":"features/progress_bar/#Progress-Bar-Integration","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"DifferentialEquations.jl integrates with the VS Code progress bar in order to make long calculations more manageable. By default, this feature is off for ODE and SDE solvers, but can be turned on via the keyword argument progress=true. The progress bar updates every progress_steps timesteps, which has a default value of 1000. Note that making this value really low could cause a performance hit, though from some basic testing it seems that with updates of at least 1000 steps on number (the fastest problems), there's no discernible performance degradation, giving a high upper bound.\n\nNote that the progress bar also includes a time estimate. This time-estimate is provided by linear extrapolation for how long it has taken to get to what percentage. For adaptive timestepping methods this should only be used as a rough estimate since the timesteps may (and will) change. By scrolling over the progress bar, one will also see the current timestep. This can be used to track the solution's progress and find tough locations for the solvers.","category":"section"},{"location":"features/progress_bar/#Using-Progress-Bars-with-VS-Code","page":"Progress Bar Integration","title":"Using Progress Bars with VS Code","text":"If using VS Code, progress bars are enabled via the ProgressLogging.jl package. For example:\n\nimport OrdinaryDiffEq as ODE, ProgressLogging\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0(u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 1000000.0)\nprob = ODE.ODEProblem(lorenz!, u0, tspan)\nsol = ODE.solve(prob, ODE.Tsit5(), progress = true)","category":"section"},{"location":"features/progress_bar/#Using-Progress-Bars-in-the-Terminal","page":"Progress Bar Integration","title":"Using Progress Bars in the Terminal","text":"import OrdinaryDiffEq as ODE, TerminalLoggers\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0(u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 1000000.0)\nprob = ODE.ODEProblem(lorenz!, u0, tspan)\nsol = ODE.solve(prob, ODE.Tsit5(), progress = true)\n\nTo use progress bars in the terminal, use TerminalLoggers.jl. Follow these directions to add TerminalLogging to your startup.jl, if you want it enabled by default.\n\nOtherwise, follow the example down below. Note that global_logger is initialized before any other Julia call. This step is crucial. Otherwise, no logging will appear in the terminal.\n\nimport OrdinaryDiffEq as ODE\nimport Logging: global_logger\nimport TerminalLoggers: TerminalLogger\nglobal_logger(TerminalLogger())\n\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0(u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 1000000.0)\nprob = ODE.ODEProblem(lorenz!, u0, tspan)\nsol = ODE.solve(prob, ODE.Tsit5(), progress = true, maxiters = 1e8)","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#OrdinaryDiffEqQPRK","page":"OrdinaryDiffEqQPRK","title":"OrdinaryDiffEqQPRK","text":"Quadruple-precision parallel Runge-Kutta (QPRK) methods are high-order explicit solvers specifically designed for ultra-high precision computations using quad-precision arithmetic (Float128). These methods combine parallel evaluation capabilities with coefficients optimized for extended precision arithmetic. Note: These methods are still under-benchmarked and need more research.","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Key-Properties","page":"OrdinaryDiffEqQPRK","title":"Key Properties","text":"QPRK methods provide:\n\nUltra-high-order accuracy (9th order) for maximum precision\nQuadruple-precision optimization specifically designed for Float128\nParallel function evaluations for computational efficiency\nExtreme precision capabilities for very demanding applications\nOptimized coefficients for extended precision arithmetic","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#When-to-Use-QPRK-Methods","page":"OrdinaryDiffEqQPRK","title":"When to Use QPRK Methods","text":"These methods are recommended for:\n\nUltra-high precision requirements demanding Float128 arithmetic\nExtremely low tolerances (< 1e-20) where standard precision fails\nScientific applications requiring maximum possible accuracy\nParallel computing environments with quad-precision support\nResearch applications exploring limits of numerical precision\nLong-time integration where error accumulation must be minimized to extreme levels","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Important-Requirements","page":"OrdinaryDiffEqQPRK","title":"Important Requirements","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Precision-Requirements","page":"OrdinaryDiffEqQPRK","title":"Precision Requirements","text":"Must use Float128 or higher precision number types\nAll problem components should support extended precision\nTolerances should match the precision capabilities (< 1e-20)","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Computational-Considerations","page":"OrdinaryDiffEqQPRK","title":"Computational Considerations","text":"Slower than standard precision methods due to extended precision arithmetic\nHigher memory usage due to extended precision\nLimited hardware support for quad-precision operations","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Mathematical-Background","page":"OrdinaryDiffEqQPRK","title":"Mathematical Background","text":"QPRK methods use tableaus with coefficients computed in extended precision to maintain accuracy throughout the ultra-high precision computation. The parallel structure allows independent function evaluations to be computed simultaneously.","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqQPRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Available-methods","page":"OrdinaryDiffEqQPRK","title":"Available methods","text":"QPRK98: Ninth-order method optimized for quad-precision arithmetic with parallel evaluation","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Usage-guidelines","page":"OrdinaryDiffEqQPRK","title":"Usage guidelines","text":"Essential to use Float128 for the state vector and parameters\nConsider MultiFloats.jl for higher precision number types\nSet very low tolerances (e.g., 1e-25) to utilize full precision\nTest against alternatives like Feagin methods with BigFloat","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Performance-Considerations","page":"OrdinaryDiffEqQPRK","title":"Performance Considerations","text":"Slower than standard precision methods due to extended precision arithmetic\nMemory intensive due to extended precision storage\nHardware dependent - some architectures lack efficient quad-precision support","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Alternative-High-Precision-Methods","page":"OrdinaryDiffEqQPRK","title":"Alternative High-Precision Methods","text":"For ultra-high precision, also consider:\n\nFeagin methods with BigFloat for arbitrary precision\nArbitrary precision extrapolation methods\nVerner methods with BigFloat for slightly lower but efficient precision\nTaylor series methods with automatic differentiation for extreme precision","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Usage-Example","page":"OrdinaryDiffEqQPRK","title":"Usage Example","text":"using OrdinaryDiffEqQPRK\n# Ensure using Float128 for ultra-high precision\nu0 = Float128[1.0, 0.0]\ntspan = (Float128(0.0), Float128(10.0))\nprob = ODEProblem(f, u0, tspan)\nsol = solve(prob, QPRK98(), abstol = 1e-25, reltol = 1e-25)\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqQPRK\", \"QPRK98\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#Full-list-of-solvers","page":"OrdinaryDiffEqQPRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/QPRK/#OrdinaryDiffEqQPRK.QPRK98","page":"OrdinaryDiffEqQPRK","title":"OrdinaryDiffEqQPRK.QPRK98","text":"QPRK98(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Runge–Kutta pairs of orders 9(8) for use in quadruple precision computations\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nKovalnogov VN, Fedorov RV, Karpukhina TV, Simos TE, Tsitouras C. Runge–Kutta pairs      of orders 9 (8) for use in quadruple precision computations. Numerical Algorithms, 2023.      doi: https://doi.org/10.1007/s11075-023-01632-8\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#Stabilized-Methods-(SROCK-Family)","page":"Stabilized Methods (SROCK Family)","title":"Stabilized Methods (SROCK Family)","text":"Stabilized Runge-Kutta Chebyshev (SROCK) methods provide stability for mildly stiff problems through extended stability regions rather than implicit treatment. These methods are particularly effective for parabolic PDEs discretized by method of lines.","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#SROCK-Methods","page":"Stabilized Methods (SROCK Family)","title":"SROCK Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#SROCK1-First-Order-Stabilized-Method","page":"Stabilized Methods (SROCK Family)","title":"SROCK1 - First Order Stabilized Method","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#SROCK2,-KomBurSROCK2,-SROCKC2-Second-Order-Methods","page":"Stabilized Methods (SROCK Family)","title":"SROCK2, KomBurSROCK2, SROCKC2 - Second Order Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#SROCKEM-Stabilized-Euler-Maruyama","page":"Stabilized Methods (SROCK Family)","title":"SROCKEM - Stabilized Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#SKSROCK-Stabilized-Method-with-Post-Processing","page":"Stabilized Methods (SROCK Family)","title":"SKSROCK - Stabilized Method with Post-Processing","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#TangXiaoSROCK2-Alternative-Second-Order-Method","page":"Stabilized Methods (SROCK Family)","title":"TangXiaoSROCK2 - Alternative Second Order Method","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#When-to-Use-Stabilized-Methods","page":"Stabilized Methods (SROCK Family)","title":"When to Use Stabilized Methods","text":"Ideal for:\n\nParabolic PDEs discretized by method of lines\nProblems with moderate stiffness (not extremely stiff)\nLarge systems where implicit methods are expensive\nWhen eigenvalue spectrum is primarily negative real\n\nAdvantages over implicit methods:\n\nNo linear system solves required\nBetter for large systems\nParallelizable\nNo Jacobian computation needed\n\nDisadvantages:\n\nLimited to moderate stiffness\nMay require eigenvalue estimation\nNot effective for highly oscillatory problems","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#Stability-Regions","page":"Stabilized Methods (SROCK Family)","title":"Stability Regions","text":"SROCK methods extend stability along the negative real axis:\n\nStandard explicit: Stability region ~ [-2, 0]\nSROCK methods: Stability region ~ [-s², 0] where s is the number of stages\n\nThe number of stages s is chosen based on estimated eigenvalues.","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#Eigenvalue-Estimation","page":"Stabilized Methods (SROCK Family)","title":"Eigenvalue Estimation","text":"Most SROCK methods accept an eigen_est parameter:\n\n# Automatic estimation (default)\nSROCK1()\n\n# Manual estimation\nSROCK1(eigen_est = -100.0)  # Largest eigenvalue magnitude\n\n# Custom estimation function\nSROCK1(eigen_est = my_estimator)","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#Method-Selection-Guide","page":"Stabilized Methods (SROCK Family)","title":"Method Selection Guide","text":"SROCK1: Basic first-order method, most robust\nSROCK2: Higher accuracy, good general choice\nSROCKEM: When Euler-Maruyama structure is preferred\nSKSROCK: Advanced features, post-processing options\nSROCKC2: Conservative second-order variant","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#Problem-Suitability","page":"Stabilized Methods (SROCK Family)","title":"Problem Suitability","text":"Well-suited:\n\nReaction-diffusion equations\nHeat equations with stochastic terms\nLarge sparse systems\nMethod of lines discretizations\n\nNot well-suited:\n\nHighly stiff problems (use implicit methods)\nProblems with complex eigenvalue spectra\nSmall dense systems (overhead not justified)","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#Configuration-Tips","page":"Stabilized Methods (SROCK Family)","title":"Configuration Tips","text":"# For PDE problems\nSROCK2(eigen_est = estimate_spectral_radius(A))\n\n# For uncertain problems, start conservatively\nSROCK1()  # Most robust\n\n# For higher accuracy\nSROCK2()  # Good balance","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#Performance-Considerations","page":"Stabilized Methods (SROCK Family)","title":"Performance Considerations","text":"Stage count increases with stiffness\nEigenvalue estimation cost\nMemory requirements for internal stages\nBetter scalability than implicit methods","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#References","page":"Stabilized Methods (SROCK Family)","title":"References","text":"Chebyshev methods for parabolic problems\nROCK methods for stiff ODEs\nStabilized explicit methods for PDEs","category":"section"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#StochasticDiffEq.SROCK1","page":"Stabilized Methods (SROCK Family)","title":"StochasticDiffEq.SROCK1","text":"SROCK1(;interpretation=AlgorithmInterpretation.Ito, eigen_est=nothing)\n\nSROCK1: First-Order Stabilized Runge-Kutta Chebyshev Method\n\nFixed step size stabilized explicit method designed for mildly stiff SDE problems, particularly effective for parabolic PDEs discretized by method of lines.\n\nMethod Properties\n\nStrong Order: 0.5 (optimized to 1.0 for scalar/diagonal noise)\nWeak Order: 1.0\nTime stepping: Fixed step size with extended stability\nNoise types: All forms (diagonal, non-diagonal, scalar, additive)\nSDE interpretation: Configurable (Itô or Stratonovich)\nStability: Extended along negative real axis\n\nParameters\n\ninterpretation: Choose AlgorithmInterpretation.Ito (default) or AlgorithmInterpretation.Stratonovich\neigen_est: Eigenvalue estimation for stability region (automatic if nothing)\n\nWhen to Use\n\nParabolic PDEs with stochastic terms\nMildly stiff problems where implicit methods are too expensive\nLarge sparse systems from method of lines\nWhen stability region extension is more important than high accuracy\n\nStability\n\nExtends stability region to approximately [-s², 0] where s is number of stages\nNumber of stages chosen based on eigenvalue estimates\nMore efficient than implicit methods for moderate stiffness\n\nReferences\n\nChebyshev methods for parabolic stochastic PDEs\nROCK methods for stiff problems\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#StochasticDiffEq.SROCKEM","page":"Stabilized Methods (SROCK Family)","title":"StochasticDiffEq.SROCKEM","text":"SROCKEM(;strong_order_1=true, eigen_est=nothing)\n\nSROCKEM: ROCK-Stabilized Euler-Maruyama Method\n\nFixed step Euler-Maruyama method with first-order ROCK stabilization for handling stiff problems.\n\nMethod Properties\n\nStrong Order: 1.0 (default) or 0.5 (if strong_order_1=false)\nWeak Order: 1.0 (default) or 0.5 (if strong_order_1=false)\nTime stepping: Fixed step size with ROCK stabilization\nNoise types: 1-dimensional, diagonal, and multi-dimensional noise\nSDE interpretation: Itô only\nStability: ROCK stabilization for moderate stiffness\n\nParameters\n\nstrong_order_1::Bool = true: Use strong/weak order 1.0 (true) or 0.5 (false)\neigen_est: Eigenvalue estimation for stability (automatic if nothing)\n\nWhen to Use\n\nStiff problems where standard EM fails\nWhen ROCK stabilization is preferred over full implicit treatment\nProblems requiring Euler-Maruyama structure with enhanced stability\nMulti-dimensional stiff SDEs\n\nAlgorithm Description\n\nCombines Euler-Maruyama discretization with ROCK stabilization techniques to extend the stability region without requiring linear solves.\n\nReferences\n\nROCK stabilization techniques applied to SDEs\nStabilized Euler methods for stiff problems\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#StochasticDiffEq.SKSROCK","page":"Stabilized Methods (SROCK Family)","title":"StochasticDiffEq.SKSROCK","text":"SKSROCK(;post_processing=false, eigen_est=nothing)\n\nSKSROCK: SK-SROCK Stabilized Method\n\nFixed step stabilized explicit method for stiff Itô problems with enhanced stability domain and optional post-processing.\n\nMethod Properties\n\nStrong Order: 0.5 (up to 2.0 with post-processing)\nWeak Order: 1.0 (up to 2.0 with post-processing)\nTime stepping: Fixed step size with enhanced stability\nNoise types: 1-dimensional, diagonal, and multi-dimensional noise\nSDE interpretation: Itô only\nStability: Better stability domain than SROCK1\n\nParameters\n\npost_processing::Bool = false: Enable post-processing for higher accuracy (experimental)\neigen_est: Eigenvalue estimation for stability (automatic if nothing)\n\nWhen to Use\n\nStiff Itô problems requiring better stability than SROCK1\nErgodic dynamical systems (with post-processing)\nProblems where enhanced stability domain is crucial\nWhen experimenting with post-processing techniques\n\nPost-Processing (Experimental)\n\nCan achieve order 2 accuracy for ergodic systems\nParticularly useful for Brownian dynamics\nCurrently under development - use with caution\n\nAlgorithm Features\n\nEnhanced stability compared to SROCK1\nHandles various noise structures\nOptional post-processing for specialized applications\n\nReferences\n\nSK-SROCK methods for stochastic problems\nPost-processing techniques for ergodic systems\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/stabilized_methods/#StochasticDiffEq.TangXiaoSROCK2","page":"Stabilized Methods (SROCK Family)","title":"StochasticDiffEq.TangXiaoSROCK2","text":"TangXiaoSROCK2(;version_num=5, eigen_est=nothing)\n\nTangXiaoSROCK2: Tang-Xiao Second-Order SROCK Method\n\nFixed step size stabilized explicit method with multiple variants offering different stability domains.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: 2.0\nTime stepping: Fixed step size with extended stability\nNoise types: Various (depends on version)\nSDE interpretation: Itô only\nStability: Version-dependent stability domains\n\nParameters\n\nversion_num::Int = 5: Choose version 1-5 with different stability characteristics\neigen_est: Eigenvalue estimation for stability (automatic if nothing)\n\nWhen to Use\n\nWhen experimenting with different stability domains\nProblems requiring weak order 2.0 with fixed steps\nBenchmarking different ROCK variants\nNote: Currently under development\n\nVersions\n\nVersions 1-5 offer different stability domains\nVersion 5 (default) typically provides good general performance\nChoose version based on problem-specific stability requirements\n\nDevelopment Status\n\nMethod is under active development\nUse with caution in production code\nConsider more established ROCK methods for critical applications\n\nReferences\n\nTang and Xiao, \"Second-order SROCK methods for stochastic problems\"\n\n\n\n\n\n","category":"type"},{"location":"features/dae_initialization/#dae_initialization","page":"DAE Initialization","title":"DAE Initialization","text":"DAE (Differential-Algebraic Equation) problems often require special initialization procedures to ensure that the initial conditions are consistent with the algebraic constraints. The DifferentialEquations.jl ecosystem provides several initialization algorithms to handle this automatically or to verify that your provided initial conditions are already consistent.","category":"section"},{"location":"features/dae_initialization/#The-Initialization-Problem","page":"DAE Initialization","title":"The Initialization Problem","text":"DAEs have the general form:\n\nM fracdudt = f(u p t)\n\nwhere M is a (possibly singular) mass matrix. For the initial conditions u₀ and du₀ to be consistent, they must satisfy:\n\nf(du₀ u₀ p t₀) = 0\n\nfor fully implicit DAEs, or the equivalent constraint for semi-explicit DAEs. Finding consistent initial conditions is a nonlinear problem that must be solved before time integration can begin.","category":"section"},{"location":"features/dae_initialization/#Available-Initialization-Algorithms","page":"DAE Initialization","title":"Available Initialization Algorithms","text":"The initializealg keyword argument to solve controls how initialization is performed. All algorithms are documented with their docstrings:","category":"section"},{"location":"features/dae_initialization/#WARNING:-NoInit-Usage","page":"DAE Initialization","title":"⚠️ WARNING: NoInit Usage","text":"warning: Use NoInit at your own risk\nNoInit() should almost never be used. No algorithm has any guarantee of correctness if NoInit() is used with inconsistent initial conditions. Users should almost always use CheckInit() instead for safety.Important:Any issues opened that are using NoInit() will be immediately closed\nAllowing incorrect initializations is not a supported part of the system\nUsing NoInit() with inconsistent conditions can lead to:\nSolver instability and crashes\nIncorrect results that may appear plausible\nUndefined behavior in the numerical algorithms\nSilent corruption of the solutionWhen to use CheckInit() instead:When you believe your initial conditions are consistent\nWhen you want to skip automatic modification of initial conditions\nWhen you need to verify your manual initializationThe only valid use case for NoInit() is when you are 100% certain your conditions are consistent AND you need to skip the computational cost of verification for performance reasons in production code that has been thoroughly tested.","category":"section"},{"location":"features/dae_initialization/#Algorithm-Selection-Guide","page":"DAE Initialization","title":"Algorithm Selection Guide","text":"Algorithm When to Use Modifies Variables\nDefaultInit() Default choice - automatically selects appropriate method Depends on selection\nCheckInit() When you've computed consistent conditions yourself No (verification only)\nNoInit() ⚠️ AVOID - Only for verified consistent conditions No\nOverrideInit() With ModelingToolkit problems Yes (uses custom problem)\nBrownFullBasicInit() For index-1 DAEs with differential_vars Algebraic variables only\nShampineCollocationInit() For general DAEs without structure information All variables","category":"section"},{"location":"features/dae_initialization/#Examples","page":"DAE Initialization","title":"Examples","text":"","category":"section"},{"location":"features/dae_initialization/#Example-1:-Simple-Pendulum-DAE","page":"DAE Initialization","title":"Example 1: Simple Pendulum DAE","text":"using DifferentialEquations\n\nfunction pendulum!(res, du, u, p, t)\n    x, y, T = u\n    dx, dy, dT = du\n    g, L = p\n\n    res[1] = dx - du[1]\n    res[2] = dy - du[2]\n    res[3] = x^2 + y^2 - L^2  # Algebraic constraint\nend\n\nu0 = [1.0, 0.0, 0.0]  # Initial position\ndu0 = [0.0, 0.0, 0.0]  # Initial velocity (inconsistent!)\np = [9.81, 1.0]  # g, L\ntspan = (0.0, 10.0)\n\nprob = DAEProblem(pendulum!, du0, u0, tspan, p,\n                  differential_vars = [true, true, false])\n\n# BrownFullBasicInit will fix the inconsistent du0\nsol = solve(prob, DFBDF(), initializealg = BrownFullBasicInit())","category":"section"},{"location":"features/dae_initialization/#Example-2:-Checking-Consistency-(Recommended-over-NoInit)","page":"DAE Initialization","title":"Example 2: Checking Consistency (Recommended over NoInit)","text":"# If you've computed consistent conditions yourself\nu0_consistent = [1.0, 0.0, 0.0]\ndu0_consistent = [0.0, -1.0, compute_tension(u0_consistent, p)]\n\nprob2 = DAEProblem(pendulum!, du0_consistent, u0_consistent, tspan, p,\n                   differential_vars = [true, true, false])\n\n# RECOMMENDED: Verify they're consistent with CheckInit\nsol = solve(prob2, DFBDF(), initializealg = CheckInit())\n\n# NOT RECOMMENDED: NoInit skips all checks - use at your own risk!\n# sol = solve(prob2, DFBDF(), initializealg = NoInit())  # ⚠️ DANGEROUS","category":"section"},{"location":"features/dae_initialization/#Example-3:-ModelingToolkit-Integration","page":"DAE Initialization","title":"Example 3: ModelingToolkit Integration","text":"When using ModelingToolkit, initialization information is often included automatically:\n\nusing ModelingToolkit, DifferentialEquations\n\n@variables t x(t) y(t) T(t)\n@parameters g L\nD = Differential(t)\n\neqs = [\n    D(x) ~ -T * x/L,\n    D(y) ~ -T * y/L - g,\n    x^2 + y^2 ~ L^2\n]\n\n@named pendulum = ODESystem(eqs, t, [x, y, T], [g, L])\nsys = structural_simplify(pendulum)\n\n# ModelingToolkit provides initialization_data\nprob = DAEProblem(sys, [x => 1.0, y => 0.0], (0.0, 10.0), [g => 9.81, L => 1.0])\n\n# DefaultInit will use OverrideInit with ModelingToolkit's initialization_data\nsol = solve(prob, DFBDF())  # Automatic initialization!","category":"section"},{"location":"features/dae_initialization/#Algorithm-Specific-Options","page":"DAE Initialization","title":"Algorithm-Specific Options","text":"Both OrdinaryDiffEq and Sundials support the same initialization algorithms through the initializealg parameter:","category":"section"},{"location":"features/dae_initialization/#OrdinaryDiffEq-and-Sundials","page":"DAE Initialization","title":"OrdinaryDiffEq and Sundials","text":"using OrdinaryDiffEq\n# or\nusing Sundials\n\n# Use Brown's algorithm to fix inconsistent conditions\nsol = solve(prob, DFBDF(), initializealg = BrownFullBasicInit())  # OrdinaryDiffEq\nsol = solve(prob, IDA(), initializealg = BrownFullBasicInit())    # Sundials\n\n# Use Shampine's collocation method for general DAEs\nsol = solve(prob, DFBDF(), initializealg = ShampineCollocationInit())  # OrdinaryDiffEq\nsol = solve(prob, IDA(), initializealg = ShampineCollocationInit())    # Sundials\n\n# RECOMMENDED: Verify conditions are consistent\nsol = solve(prob, DFBDF(), initializealg = CheckInit())  # OrdinaryDiffEq\nsol = solve(prob, IDA(), initializealg = CheckInit())    # Sundials\n\n# NOT RECOMMENDED: Skip all initialization checks (dangerous!)\n# sol = solve(prob, DFBDF(), initializealg = NoInit())  # ⚠️ USE AT YOUR OWN RISK\n# sol = solve(prob, IDA(), initializealg = NoInit())    # ⚠️ USE AT YOUR OWN RISK","category":"section"},{"location":"features/dae_initialization/#Troubleshooting","page":"DAE Initialization","title":"Troubleshooting","text":"","category":"section"},{"location":"features/dae_initialization/#Common-Issues-and-Solutions","page":"DAE Initialization","title":"Common Issues and Solutions","text":"\"Initial conditions are not consistent\" error\nEnsure your du0 satisfies the DAE constraints at t0\nTry using BrownFullBasicInit() or ShampineCollocationInit() instead of CheckInit()\nCheck that differential_vars correctly identifies differential vs algebraic variables\nInitialization fails to converge\nRelax tolerances if using extended versions\nTry a different initialization algorithm\nProvide a better initial guess for algebraic variables\nCheck if your DAE is index-1: The system may be higher-index (see below)\nSolver fails immediately after initialization\nThe initialization might have found a consistent but numerically unstable point\nTry tightening initialization tolerances\nCheck problem scaling and consider non-dimensionalization\nDAE is not index-1 (higher-index DAE)\nMany initialization algorithms only work reliably for index-1 DAEs\nTo check if your DAE is index-1: The Jacobian of the algebraic equations with respect to the algebraic variables must be non-singular\nSolution: Use ModelingToolkit.jl to analyze and potentially reduce the index:\nusing ModelingToolkit\n\n# Define your system with ModelingToolkit\n@named sys = ODESystem(eqs, t, vars, params)\n\n# Analyze and reduce the index (structural_simplify handles this in v10+)\nsys_reduced = structural_simplify(sys)\n\n# The reduced system will be index-1 and easier to initialize\nprob = DAEProblem(sys_reduced, [], (0.0, 10.0), params)\nModelingToolkit can automatically detect the index and apply appropriate transformations\nAfter index reduction, standard initialization algorithms will work more reliably","category":"section"},{"location":"features/dae_initialization/#Performance-Tips","page":"DAE Initialization","title":"Performance Tips","text":"Use differential_vars when possible: This helps initialization algorithms understand problem structure\nProvide good initial guesses: Even when using automatic initialization, starting closer to the solution helps\nConsider problem-specific initialization: For complex systems, custom initialization procedures may be more efficient\nUse CheckInit() when appropriate: If you know conditions are consistent, skip unnecessary computation","category":"section"},{"location":"features/dae_initialization/#References","page":"DAE Initialization","title":"References","text":"Brown, P. N., Hindmarsh, A. C., & Petzold, L. R. (1998). Consistent initial condition calculation for differential-algebraic systems. SIAM Journal on Scientific Computing, 19(5), 1495-1512.\nShampine, L. F. (2002). Consistent initial condition for differential-algebraic systems. SIAM Journal on Scientific Computing, 22(6), 2007-2026.","category":"section"},{"location":"features/dae_initialization/#DiffEqBase.DefaultInit","page":"DAE Initialization","title":"DiffEqBase.DefaultInit","text":"struct DefaultInit <: DAEInitializationAlgorithm\n\nThe default initialization algorithm for DAEs. This will use heuristics to determine the most appropriate initialization based on the problem type.\n\nFor Sundials, this will use:\n\nOverrideInit if the problem has initialization_data (typically from ModelingToolkit)\nCheckInit otherwise\n\n\n\n\n\n","category":"type"},{"location":"features/dae_initialization/#SciMLBase.CheckInit","page":"DAE Initialization","title":"SciMLBase.CheckInit","text":"struct CheckInit <: DAEInitializationAlgorithm\n\nAn initialization algorithm that only checks if the initial conditions are consistent with the DAE constraints, without attempting to modify them. If the conditions are not consistent within the solver's tolerance, an error will be thrown.\n\nThis is useful when:\n\nYou have already computed consistent initial conditions\nYou want to verify the consistency of your initial guess\nYou want to ensure no automatic modifications are made to your initial conditions\n\nExample\n\nprob = DAEProblem(f, du0, u0, tspan)\nsol = solve(prob, IDA(), initializealg = CheckInit())\n\n\n\n\n\n","category":"type"},{"location":"features/dae_initialization/#SciMLBase.NoInit","page":"DAE Initialization","title":"SciMLBase.NoInit","text":"struct NoInit <: DAEInitializationAlgorithm\n\nAn initialization algorithm that completely skips the initialization phase. The solver will use the provided initial conditions directly without any consistency checks or modifications.\n\nwarning: Warning\nUsing NoInit() with inconsistent initial conditions will likely cause solver failures or incorrect results. Only use this when you are absolutely certain your initial conditions satisfy all DAE constraints.\n\nThis is useful when:\n\nYou know your initial conditions are already perfectly consistent\nYou want to avoid the computational cost of initialization\nYou are debugging solver issues and want to isolate initialization from integration\n\nExample\n\nprob = DAEProblem(f, du0_consistent, u0_consistent, tspan)\nsol = solve(prob, IDA(), initializealg = NoInit())\n\n\n\n\n\n","category":"type"},{"location":"features/dae_initialization/#SciMLBase.OverrideInit","page":"DAE Initialization","title":"SciMLBase.OverrideInit","text":"struct OverrideInit <: DAEInitializationAlgorithm\n\nAn initialization algorithm that uses a separate initialization problem to find consistent initial conditions. This is typically used with ModelingToolkit.jl which can generate specialized initialization problems based on the model structure.\n\nWhen using OverrideInit, the problem must have initialization_data that contains an initializeprob field with the initialization problem to solve.\n\nThis algorithm is particularly useful for:\n\nHigh-index DAEs that have been index-reduced\nSystems with complex initialization requirements\nModelingToolkit models with custom initialization equations\n\nFields\n\nabstol: Absolute tolerance for the initialization solver\nreltol: Relative tolerance for the initialization solver\nnlsolve: Nonlinear solver to use for initialization\n\nExample\n\n# Typically used automatically with ModelingToolkit\n@named sys = ODESystem(eqs, t, vars, params)\nsys = structural_simplify(sys)\nprob = DAEProblem(sys, [], (0.0, 1.0), [])\n# Will automatically use OverrideInit if initialization_data exists\nsol = solve(prob, IDA())\n\n\n\n\n\n","category":"type"},{"location":"features/dae_initialization/#DiffEqBase.BrownFullBasicInit","page":"DAE Initialization","title":"DiffEqBase.BrownFullBasicInit","text":"struct BrownFullBasicInit{T, F} <: DAEInitializationAlgorithm\n\nThe Brown full basic initialization algorithm for DAEs. This implementation is based on the algorithm described in:\n\nPeter N. Brown, Alan C. Hindmarsh, and Linda R. Petzold, \"Consistent Initial Condition Calculation for Differential-Algebraic Systems\", SIAM Journal on Scientific Computing, Vol. 19, No. 5, pp. 1495-1512, 1998. DOI: https://doi.org/10.1137/S1064827595289996\n\nThis method modifies the algebraic variables and their derivatives to be consistent with the DAE constraints, while keeping the differential variables fixed. It uses Newton's method to solve for consistent initial values.\n\nThis is the default initialization for many DAE solvers when differential_vars is provided, allowing the solver to distinguish between differential and algebraic variables.\n\nParameters\n\nabstol: Absolute tolerance for the nonlinear solver (default: 1e-10)\nnlsolve: Custom nonlinear solver to use (optional)\n\n\n\n\n\n","category":"type"},{"location":"features/dae_initialization/#DiffEqBase.ShampineCollocationInit","page":"DAE Initialization","title":"DiffEqBase.ShampineCollocationInit","text":"struct ShampineCollocationInit{T, F} <: DAEInitializationAlgorithm\n\nThe Shampine collocation initialization algorithm for DAEs. This implementation is based on the algorithm described in:\n\nLawrence F. Shampine, \"Consistent Initial Condition for Differential-Algebraic Systems\", SIAM Journal on Scientific Computing, Vol. 22, No. 6, pp. 2007-2026, 2001. DOI: https://doi.org/10.1137/S1064827599355049\n\nThis method uses collocation on the first two steps to find consistent initial conditions. It modifies both the differential and algebraic variables to satisfy the DAE constraints. This is more general than BrownBasicInit but may be more expensive computationally.\n\nThis method is useful when you need to modify all variables (both differential and algebraic) to achieve consistency, rather than just the algebraic ones.\n\nParameters\n\ninitdt: Initial time step to use in the collocation method (optional)\nnlsolve: Custom nonlinear solver to use (optional)\n\n\n\n\n\n","category":"type"},{"location":"solvers/dae_solve/#Mass-Matrix-and-Fully-Implicit-DAE-Solvers","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Mass Matrix and Fully Implicit DAE Solvers","text":"","category":"section"},{"location":"solvers/dae_solve/#Recommended-Methods","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Recommended Methods","text":"For medium to low accuracy small numbers of DAEs in constant mass matrix form, the  Rosenbrock23 and Rodas4 methods are good choices which will get good efficiency if the mass matrix is constant. Rosenbrock23 is better for low accuracy (error tolerance <1e-4) and Rodas4 is better for high accuracy. Another choice at high accuracy is Rodas5P and RadauIIA5.\n\nNon-constant mass matrices are not directly supported: users are advised to transform their problem through substitution to a DAE with constant mass matrices.\n\nIf the problem cannot be defined in mass matrix form, the recommended method for performance is IDA from the Sundials.jl package if you are solving problems with Float64. If Julia types are required, currently DFBDF is the best method but still needs more optimizations.","category":"section"},{"location":"solvers/dae_solve/#dae_solve_full","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/dae_solve/#Initialization-Schemes","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Initialization Schemes","text":"For all OrdinaryDiffEq.jl methods, an initialization scheme can be set with a common keyword argument initializealg. The choices are:\n\nCheckInit: Check that the provided initial conditions satisfy the equation; if not, error. This avoids the need for nonlinear solution, as well as avoiding changing the provided initial conditions.\nBrownFullBasicInit: For Index-1 DAEs implicit DAEs and semi-explicit DAEs in mass matrix form. Keeps the differential variables constant. Requires du0 when used on a DAEProblem.\nShampineCollocationInit: For Index-1 DAEs implicit DAEs and semi-explicit DAEs in mass matrix form. Changes both the differential and algebraic variables.\nNoInit: Explicitly opts-out of DAE initialization.","category":"section"},{"location":"solvers/dae_solve/#OrdinaryDiffEq.jl-(Implicit-ODE)","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"OrdinaryDiffEq.jl (Implicit ODE)","text":"These methods from OrdinaryDiffEq are for DAEProblem specifications.\n\nDImplicitEuler - 1st order A-L and stiffly stable adaptive implicit Euler\nDABDF2 - 2nd order A-L stable adaptive BDF method.\nDFBDF - A fixed-leading coefficient adaptive-order adaptive-time BDF method, similar to ode15i or IDA in divided differences form.","category":"section"},{"location":"solvers/dae_solve/#OrdinaryDiffEq.jl-(Mass-Matrix)","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"OrdinaryDiffEq.jl (Mass Matrix)","text":"These methods require the DAE to be an ODEProblem in mass matrix form. For extra options for the solvers, see the ODE solver page.\n\nnote: Note\nThe standard Hermite interpolation used for ODE methods in OrdinaryDiffEq.jl falls back to a linear interpolation on the differential variables. If the mass matrix is non-diagonal, the Hermite interpolation does not have a fallback and will error.","category":"section"},{"location":"solvers/dae_solve/#Rosenbrock-Methods","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Rosenbrock Methods","text":"ROS3P - 3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\nRodas3 - 3rd order A-stable and stiffly stable Rosenbrock method.\nRosShamp4- An A-stable 4th order Rosenbrock method.\nVeldd4 - A 4th order D-stable Rosenbrock method.\nVelds4 - A 4th order A-stable Rosenbrock method.\nGRK4T - An efficient 4th order Rosenbrock method.\nGRK4A - An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\nRos4LStab - A 4th order L-stable Rosenbrock method.\nRodas4 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas42 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas4P - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\nRodas4P2 - A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems. It is an improvement of Roadas4P and in case of inexact Jacobians a second order W method.\nRodas5 - A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant.\nRodas5P - A 5th order A-stable stiffly stable Rosenbrock method with a stiff-aware 4th order interpolant. Has improved stability in the adaptive time stepping embedding.","category":"section"},{"location":"solvers/dae_solve/#Rosenbrock-W-Methods","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Rosenbrock-W Methods","text":"Rosenbrock23 - An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\nRosenbrock32 - An Order 3/2 A-Stable Rosenbrock-W method which is good for mildly stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\nRosenbrockW6S4OS - A 4th order L-stable Rosenbrock-W method (fixed step only).\nROS34PW1a - A 4th order L-stable Rosenbrock-W method.\nROS34PW1b - A 4th order L-stable Rosenbrock-W method.\nROS34PW2 - A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\nROS34PW3 - A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.\n\nnote: Note\nRosenbrock23 and Rosenbrock32 have a stiff-aware interpolation but this interpolation is not safe for the algebraic variables. Thus use the interpolation (and therefore saveat) with caution if the default Hermite interpolation is used.","category":"section"},{"location":"solvers/dae_solve/#FIRK-Methods","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"FIRK Methods","text":"RadauIIA5 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.","category":"section"},{"location":"solvers/dae_solve/#SDIRK-Methods","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"SDIRK Methods","text":"ImplicitEuler - Stage order 1. A-B-L-stable. Adaptive timestepping through a divided differences estimate via memory. Strong-stability preserving (SSP).\nImplicitMidpoint - Stage order 1. Symplectic. Good for when symplectic integration is required.\nTrapezoid - A second order A-stable symmetric ESDIRK method. \"Almost symplectic\" without numerical dampening. Also known as Crank-Nicolson when applied to PDEs. Adaptive timestepping via divided differences on the memory. Good for highly stiff equations which are non-oscillatory.","category":"section"},{"location":"solvers/dae_solve/#Multistep-Methods","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Multistep Methods","text":"Quasi-constant stepping is the time stepping strategy which matches the classic GEAR, LSODE,  and ode15s integrators. The variable-coefficient methods match the ideas of the classic EPISODE integrator and early VODE designs. The Fixed Leading Coefficient (FLC) methods match the behavior of the classic VODE and Sundials CVODE integrator.\n\nQNDF1 - An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function (NDF) method. Optional parameter kappa defaults to Shampine's accuracy-optimal -0.1850.\nQBDF1 - An adaptive order 1 L-stable BDF method. This is equivalent to implicit Euler but using the BDF error estimator.\nABDF2 - An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\nQNDF2 - An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\nQBDF2 - An adaptive order 2 L-stable BDF method using quasi-constant timesteps.\nQNDF - An adaptive order quasi-constant timestep NDF method. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients).\nQBDF - An adaptive order quasi-constant timestep BDF method.\nFBDF - A fixed-leading coefficient adaptive-order adaptive-time BDF method, similar to ode15i or CVODE_BDF in divided differences form.","category":"section"},{"location":"solvers/dae_solve/#dae_solve_sundials","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"Sundials.jl","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl:\n\nusing Pkg\nPkg.add(\"Sundials\")\nimport Sundials\n\nIDA: A fixed-leading coefficient fully implicit BDF method. Efficient for large systems.\n\nFor more details on controlling the Sundials.jl solvers, see the Sundials detailed solver API page","category":"section"},{"location":"solvers/dae_solve/#DASKR.jl","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"DASKR.jl","text":"DASKR.jl is not automatically included by DifferentialEquations.jl. To use this algorithm, you will need to install and use the package:\n\nusing Pkg\nPkg.add(\"DASKR\")\nimport DASKR\n\ndaskr - This is a wrapper for the well-known DASKR algorithm.\n\nFor more details on controlling the DASKR.jl solvers, see the DASKR detailed solver API page","category":"section"},{"location":"solvers/dae_solve/#DASSL.jl","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"DASSL.jl","text":"dassl - A native Julia implementation of the DASSL algorithm.","category":"section"},{"location":"solvers/dae_solve/#ODEInterfaceDiffEq.jl","page":"Mass Matrix and Fully Implicit DAE Solvers","title":"ODEInterfaceDiffEq.jl","text":"These methods require the DAE to be an ODEProblem in mass matrix form. For extra options for the solvers, see the ODE solver page.\n\nseulex - Extrapolation-algorithm based on the linear implicit Euler method.\nradau - Implicit Runge-Kutta (Radau IIA) of variable order between 5 and 13.\nradau5 - Implicit Runge-Kutta method (Radau IIA) of order 5.\nrodas - Rosenbrock 4(3) method.","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton","text":"Adams-Bashforth and Adams-Moulton multistep methods for non-stiff differential equations. Note that Runge-Kutta methods generally come out as more efficient in benchmarks, except when the ODE function f is expensive to evaluate or the problem is very smooth. These methods can achieve high accuracy with fewer function evaluations per step than Runge-Kutta methods in those specific cases.","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Key-Properties","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Key Properties","text":"Adams-Bashforth-Moulton methods provide:\n\nReduced function evaluations compared to Runge-Kutta methods\nHigh efficiency for expensive-to-evaluate functions\nMultistep structure using information from previous timesteps\nVariable step and order capabilities for adaptive integration\nPredictor-corrector variants for enhanced accuracy and stability\nGood stability properties for non-stiff problems","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#When-to-Use-Adams-Bashforth-Moulton-Methods","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"When to Use Adams-Bashforth-Moulton Methods","text":"These methods are recommended for:\n\nExpensive function evaluations where minimizing calls to f is critical\nNon-stiff smooth problems with regular solution behavior\nLong-time integration where efficiency over many steps matters\nProblems with expensive Jacobian computations that cannot use implicit methods efficiently\nScientific computing applications with computationally intensive right-hand sides\nSystems where startup cost of multistep methods is amortized over long integration","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Method-Types","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Method Types","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Explicit-Adams-Bashforth-(AB)","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Explicit Adams-Bashforth (AB)","text":"Pure explicit multistep methods using only past information:\n\nLower computational cost per step\nLess stability than predictor-corrector variants\nGood for mildly stiff problems","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Predictor-Corrector-Adams-Bashforth-Moulton-(ABM)","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Predictor-Corrector Adams-Bashforth-Moulton (ABM)","text":"Implicit corrector step for enhanced accuracy:\n\nBetter accuracy than pure explicit methods\nImproved stability properties\nSlightly higher cost but often worth it","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Solver-Selection-Guide","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Primary-recommendation","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Primary recommendation","text":"VCABM: Main recommendation - adaptive order variable-step Adams-Bashforth-Moulton, best overall choice for Adams methods","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Variable-step-predictor-corrector-methods","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Variable-step predictor-corrector methods","text":"VCABM3: Third-order variable-step Adams-Bashforth-Moulton\nVCABM4: Fourth-order variable-step Adams-Bashforth-Moulton\nVCABM5: Fifth-order variable-step Adams-Bashforth-Moulton","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Variable-step-Adams-Bashforth-methods","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Variable-step Adams-Bashforth methods","text":"VCAB3: Third-order variable-step Adams-Bashforth\nVCAB4: Fourth-order variable-step Adams-Bashforth\nVCAB5: Fifth-order variable-step Adams-Bashforth","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Fixed-step-predictor-corrector-methods","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Fixed-step predictor-corrector methods","text":"ABM32: Third-order Adams-Bashforth-Moulton\nABM43: Fourth-order Adams-Bashforth-Moulton\nABM54: Fifth-order Adams-Bashforth-Moulton","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Fixed-step-explicit-methods","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Fixed-step explicit methods","text":"AB3: Third-order Adams-Bashforth\nAB4: Fourth-order Adams-Bashforth\nAB5: Fifth-order Adams-Bashforth","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Performance-Considerations","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Performance Considerations","text":"Most efficient when function evaluation dominates computational cost\nStartup phase requires initial steps from single-step method\nMemory efficient compared to high-order Runge-Kutta methods\nBest for smooth problems - avoid for problems with discontinuities\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqAdamsBashforthMoulton\", \"VCABM\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Full-list-of-solvers","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Explicit-Multistep-Methods","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Explicit Multistep Methods","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#Predictor-Corrector-Methods","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"Predictor-Corrector Methods","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.AB3","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.AB3","text":"AB3(; thread = OrdinaryDiffEq.False())\n\nAdams-Bashforth Explicit Method The 3-step third order multistep method.         Ralston's Second Order Method is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.AB4","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.AB4","text":"AB4(; thread = OrdinaryDiffEq.False())\n\nAdams-Bashforth Explicit Method The 4-step fourth order multistep method.     Runge-Kutta method of order 4 is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.AB5","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.AB5","text":"AB5(; thread = OrdinaryDiffEq.False())\n\nAdams-Bashforth Explicit Method The 5-step fifth order multistep method.     Ralston's 3rd order Runge-Kutta method is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.ABM32","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.ABM32","text":"ABM32(; thread = OrdinaryDiffEq.False())\n\nAdams-Bashforth Explicit Method It is third order method.     In ABM32, AB3 works as predictor and Adams Moulton 2-steps method works as Corrector.     Ralston's Second Order Method is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.ABM43","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.ABM43","text":"ABM43(; thread = OrdinaryDiffEq.False())\n\nAdams-Bashforth Explicit Method It is fourth order method.     In ABM43, AB4 works as predictor and Adams Moulton 3-steps method works as Corrector.     Runge-Kutta method of order 4 is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.ABM54","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.ABM54","text":"ABM54(; thread = OrdinaryDiffEq.False())\n\nAdams-Bashforth Explicit Method It is fifth order method.     In ABM54, AB5 works as predictor and Adams Moulton 4-steps method works as Corrector.     Runge-Kutta method of order 4 is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.VCAB3","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.VCAB3","text":"VCAB3(; thread = OrdinaryDiffEq.False())\n\nAdams explicit Method The 3rd order Adams method.     Bogacki-Shampine 3/2 method is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.VCAB4","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.VCAB4","text":"VCAB4(; thread = OrdinaryDiffEq.False())\n\nAdams explicit Method The 4th order Adams method.     Runge-Kutta 4 is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.VCAB5","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.VCAB5","text":"VCAB5(; thread = OrdinaryDiffEq.False())\n\nAdams explicit Method The 5th order Adams method.     Runge-Kutta 4 is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.VCABM3","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.VCABM3","text":"VCABM3(; thread = OrdinaryDiffEq.False())\n\nAdams explicit Method The 3rd order Adams-Moulton method.     Bogacki-Shampine 3/2 method is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.VCABM4","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.VCABM4","text":"VCABM4(; thread = OrdinaryDiffEq.False())\n\nAdams explicit Method The 4th order Adams-Moulton method.     Runge-Kutta 4 is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.VCABM5","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.VCABM5","text":"VCABM5(; thread = OrdinaryDiffEq.False())\n\nAdams explicit Method The 5th order Adams-Moulton method.     Runge-Kutta 4 is used to calculate starting values.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/AdamsBashforthMoulton/#OrdinaryDiffEqAdamsBashforthMoulton.VCABM","page":"OrdinaryDiffEqAdamsBashforthMoulton","title":"OrdinaryDiffEqAdamsBashforthMoulton.VCABM","text":"VCABM(; thread = OrdinaryDiffEq.False())\n\nadaptive order Adams explicit Method An adaptive order adaptive time Adams Moulton method.     It uses an order adaptivity algorithm is derived from Shampine's DDEABM.\n\nKeyword Arguments\n\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S. P. Norsett, G. Wanner, Solving Ordinary Differential Equations I, Nonstiff Problems. Computational Mathematics (2nd revised ed.), Springer (1996) doi: https://doi.org/10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"api/sundials/#sundials","page":"Sundials.jl","title":"Sundials.jl","text":"This is a wrapper package for importing solvers from Sundials into the SciML interface. Note that these solvers do not come by default, and thus one needs to install the package before using these solvers:\n\nusing Pkg\nPkg.add(\"Sundials\")\nimport Sundials\n\nThese methods can be used independently of the rest of DifferentialEquations.jl.","category":"section"},{"location":"api/sundials/#ODE-Solver-APIs","page":"Sundials.jl","title":"ODE Solver APIs","text":"","category":"section"},{"location":"api/sundials/#DAE-Solver-APIs","page":"Sundials.jl","title":"DAE Solver APIs","text":"","category":"section"},{"location":"api/sundials/#Sundials.CVODE_Adams","page":"Sundials.jl","title":"Sundials.CVODE_Adams","text":"CVODE_Adams(;method=:Functional,linear_solver=:None,\n            jac_upper=0,jac_lower=0,\n            stored_upper = jac_upper + jac_lower,\n            krylov_dim=0,\n            stability_limit_detect=false,\n            max_hnil_warns = 10,\n            max_order = 12,\n            max_error_test_failures = 7,\n            max_nonlinear_iters = 3,\n            max_convergence_failures = 10,\n            prec = nothing, psetup = nothing, prec_side = 0)\n\nCVODE_Adams: CVode Adams-Moulton solver.\n\nMethod Choices\n\nmethod - This is the method for solving the implicit equation. For BDF this defaults to   :Newton while for Adams this defaults to :Functional. These choices match the   recommended pairing in the Sundials.jl manual. However, note that using the :Newton   method may take less iterations but requires more memory than the :Function iteration   approach.\nlinear_solver - This is the linear solver which is used in the :Newton method.\n\nLinear Solver Choices\n\nThe choices for the linear solver are:\n\n:Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jacupper and jaclower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:Diagonal - This method is specialized for diagonal Jacobians.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.\n\nExample:\n\nCVODE_Adams() # Adams method using Newton + Dense solver\nCVODE_Adams(method=:Functional) # Adams method using Functional iterations\nCVODE_Adams(linear_solver=:Band,jac_upper=3,jac_lower=3) # Banded solver with nonzero diagonals 3 up and 3 down\nCVODE_Adams(linear_solver=:BCG) # Biconjugate gradient method\n\nPreconditioners\n\nNote that here prec is a preconditioner function prec(z,r,p,t,y,fy,gamma,delta,lr) where:\n\nz: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance\nlr: a flag for whether lr=1 (left) or lr=2 (right) preconditioning\n\nand psetup is the preconditioner setup function for pre-computing Jacobian information psetup(p, t, u, du, jok, jcurPtr, gamma). Where:\n\np: the parameters\nt: the current independent variable\nu: the current state\ndu: the current f(u,p,t)\njok: a bool indicating whether the Jacobian needs to be updated\njcurPtr: a reference to an Int for whether the Jacobian was updated. jcurPtr[]=true should be set if the Jacobian was updated, and jcurPtr[]=false should be set if the Jacobian was not updated.\ngamma: the gamma of W = M - gamma*J\n\npsetup is optional when prec is set.\n\nAdditional Options\n\nSee the CVODE manual for details on the additional options.\n\n\n\n\n\n","category":"type"},{"location":"api/sundials/#Sundials.CVODE_BDF","page":"Sundials.jl","title":"Sundials.CVODE_BDF","text":"CVODE_BDF(;method=:Newton,linear_solver=:Dense,\n          jac_upper=0,jac_lower=0,\n          stored_upper = jac_upper + jac_lower,\n          non_zero=0,krylov_dim=0,\n          stability_limit_detect=false,\n          max_hnil_warns = 10,\n          max_order = 5,\n          max_error_test_failures = 7,\n          max_nonlinear_iters = 3,\n          max_convergence_failures = 10,\n          prec = nothing, prec_side = 0)\n\nCVODE_BDF: CVode Backward Differentiation Formula (BDF) solver.\n\nMethod Choices\n\nmethod - This is the method for solving the implicit equation. For BDF this defaults to   :Newton while for Adams this defaults to :Functional. These choices match the   recommended pairing in the Sundials.jl manual. However, note that using the :Newton   method may take less iterations but requires more memory than the :Function iteration   approach.\nlinear_solver - This is the linear solver which is used in the :Newton method.\n\nLinear Solver Choices\n\nThe choices for the linear solver are:\n\n:Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jacupper and jaclower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:Diagonal - This method is specialized for diagonal Jacobians.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.\n\nExample:\n\nCVODE_BDF() # BDF method using Newton + Dense solver\nCVODE_BDF(method=:Functional) # BDF method using Functional iterations\nCVODE_BDF(linear_solver=:Band,jac_upper=3,jac_lower=3) # Banded solver with nonzero diagonals 3 up and 3 down\nCVODE_BDF(linear_solver=:BCG) # Biconjugate gradient method\n\nPreconditioners\n\nNote that here prec is a preconditioner function prec(z,r,p,t,y,fy,gamma,delta,lr) where:\n\nz: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance\nlr: a flag for whether lr=1 (left) or lr=2 (right) preconditioning\n\nand psetup is the preconditioner setup function for pre-computing Jacobian information psetup(p, t, u, du, jok, jcurPtr, gamma). Where:\n\np: the parameters\nt: the current independent variable\nu: the current state\ndu: the current f(u,p,t)\njok: a bool indicating whether the Jacobian needs to be updated\njcurPtr: a reference to an Int for whether the Jacobian was updated. jcurPtr[]=true should be set if the Jacobian was updated, and jcurPtr[]=false should be set if the Jacobian was not updated.\ngamma: the gamma of W = M - gamma*J\n\npsetup is optional when prec is set.\n\nAdditional Options\n\nSee the CVODE manual for details on the additional options.\n\n\n\n\n\n","category":"type"},{"location":"api/sundials/#Sundials.ARKODE","page":"Sundials.jl","title":"Sundials.ARKODE","text":"ARKODE(stiffness=Sundials.Implicit();\n      method=:Newton,linear_solver=:Dense,\n      jac_upper=0,jac_lower=0,stored_upper = jac_upper+jac_lower,\n      non_zero=0,krylov_dim=0,\n      max_hnil_warns = 10,\n      max_error_test_failures = 7,\n      max_nonlinear_iters = 3,\n      max_convergence_failures = 10,\n      predictor_method = 0,\n      nonlinear_convergence_coefficient = 0.1,\n      dense_order = 3,\n      order = 4,\n      set_optimal_params = false,\n      crdown = 0.3,\n      dgmax = 0.2,\n      rdiv = 2.3,\n      msbp = 20,\n      adaptivity_method = 0,\n      prec = nothing, psetup = nothing, prec_side = 0\n      )\n\nARKODE: Explicit and ESDIRK Runge-Kutta methods of orders 2-8 depending on choice of options.\n\nTableau Choices\n\nThe main options for ARKODE are the choice between explicit and implicit and the method order, given via:\n\nARKODE(Sundials.Explicit()) # Solve with explicit tableau of default order 4 ARKODE(Sundials.Implicit(),order = 3) # Solve with explicit tableau of order 3\n\nThe order choices for explicit are 2 through 8 and for implicit 3 through 5. Specific methods can also be set through the etable and itable options for explicit and implicit tableaus respectively. The available tableaus are:\n\netable:\n\nHEUNEULER212: 2nd order Heun's method\nBOGACKISHAMPINE423:\nARK324L2SAERK423: explicit portion of Kennedy and Carpenter's 3rd order method\nZONNEVELD53_4: 4th order explicit method\nARK436L2SAERK634: explicit portion of Kennedy and Carpenter's 4th order method\nSAYFYABURUB634: 4th order explicit method\nCASHKARP645: 5th order explicit method\nFEHLBERG64_5: Fehlberg's classic 5th order method\nDORMANDPRINCE745: the classic 5th order Dormand-Prince method\nARK548L2SAERK845: explicit portion of Kennedy and Carpenter's 5th order method\nVERNER85_6: Verner's classic 5th order method\nFEHLBERG137_8: Fehlberg's 8th order method\n\nitable:\n\nSDIRK21_2: An A-B-stable 2nd order SDIRK method\nBILLINGTON33_2: A second order method with a 3rd order error predictor of less stability\nTRBDF233_2: The classic TR-BDF2 method\nKVAERNO42_3: an L-stable 3rd order ESDIRK method\nARK324L2SADIRK423: implicit portion of Kennedy and Carpenter's 3th order method\nCASH52_4: Cash's 4th order L-stable SDIRK method\nCASH53_4: Cash's 2nd 4th order L-stable SDIRK method\nSDIRK53_4: Hairer's 4th order SDIRK method\nKVAERNO53_4: Kvaerno's 4th order ESDIRK method\nARK436L2SADIRK634: implicit portion of Kennedy and Carpenter's 4th order method\nKVAERNO74_5: Kvaerno's 5th order ESDIRK method\nARK548L2SADIRK845: implicit portion of Kennedy and Carpenter's 5th order method\n\nThese can be set for example via:\n\nARKODE(Sundials.Explicit(),etable = Sundials.DORMAND_PRINCE_7_4_5)\nARKODE(Sundials.Implicit(),itable = Sundials.KVAERNO_4_2_3)\n\nMethod Choices\n\nmethod - This is the method for solving the implicit equation. For BDF this defaults to   :Newton while for Adams this defaults to :Functional. These choices match the   recommended pairing in the Sundials.jl manual. However, note that using the :Newton   method may take less iterations but requires more memory than the :Function iteration   approach.\nlinear_solver - This is the linear solver which is used in the :Newton method.\n\nLinear Solver Choices\n\nThe choices for the linear solver are:\n\n:Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jacupper and jaclower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:Diagonal - This method is specialized for diagonal Jacobians.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.\n\nPreconditioners\n\nNote that here prec is a preconditioner function prec(z,r,p,t,y,fy,gamma,delta,lr) where:\n\nz: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance\nlr: a flag for whether lr=1 (left) or lr=2 (right) preconditioning\n\nand psetup is the preconditioner setup function for pre-computing Jacobian information psetup(p, t, u, du, jok, jcurPtr, gamma). Where:\n\np: the parameters\nt: the current independent variable\nu: the current state\ndu: the current f(u,p,t)\njok: a bool indicating whether the Jacobian needs to be updated\njcurPtr: a reference to an Int for whether the Jacobian was updated. jcurPtr[]=true should be set if the Jacobian was updated, and jcurPtr[]=false should be set if the Jacobian was not updated.\ngamma: the gamma of W = M - gamma*J\n\npsetup is optional when prec is set.\n\nAdditional Options\n\nSee the ARKODE manual for details on the additional options.\n\n\n\n\n\n","category":"type"},{"location":"api/sundials/#Sundials.IDA","page":"Sundials.jl","title":"Sundials.IDA","text":"IDA(;linear_solver=:Dense,jac_upper=0,jac_lower=0,krylov_dim=0,\n    max_order = 5,\n    max_error_test_failures = 7,\n    max_nonlinear_iters = 3,\n    nonlinear_convergence_coefficient = 0.33,\n    nonlinear_convergence_coefficient_ic = 0.0033,\n    max_num_steps_ic = 5,\n    max_num_jacs_ic = 4,\n    max_num_iters_ic = 10,\n    max_num_backs_ic = 100,\n    use_linesearch_ic = true,\n    max_convergence_failures = 10,\n    init_all = false,\n    prec = nothing, psetup = nothing)\n\nIDA: This is the IDA method from the Sundials.jl package.\n\nLinear Solvers\n\nNote that the constructors for the Sundials algorithms take a main argument: linearsolver - This is the linear solver which is used in the Newton iterations. The choices are:\n\n:Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jacupper and jaclower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticeable overhead on smaller (<100 ODE) systems.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.\n\nNote that the preconditioner for iterative linear solvers (if supplied) should be a left preconditioner.\n\nExample:\n\nIDA() # Newton + Dense solver\nIDA(linear_solver=:Band,jac_upper=3,jac_lower=3) # Banded solver with nonzero diagonals 3 up and 3 down\nIDA(linear_solver=:BCG) # Biconjugate gradient method\n\nPreconditioners\n\nNote that here prec is a (left) preconditioner function prec(z,r,p,t,y,fy,gamma,delta) where:\n\nz: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance\n\nand psetup is the preconditioner setup function for pre-computing Jacobian information. Where:\n\np: the parameters\nt: the current independent variable\nresid: the current residual\nu: the current state\ndu: the current derivative of the state\ngamma: the gamma of W = M - gamma*J\n\npsetup is optional when prec is set.\n\nAdditional Options\n\nSee the Sundials manual for details on the additional options. The option init_all controls the initial condition consistency routine. If the initial conditions are inconsistent (i.e. they do not satisfy the implicit equation), init_all=false means that the algebraic variables and derivatives will be modified in order to satisfy the DAE. If init_all=true, all initial conditions will be modified to satisfy the DAE.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/ode_example/","page":"-","title":"-","text":"<meta http-equiv=\"Refresh\" content=\"0; url='../getting_started'\" />","category":"section"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK","text":"Methods for semi-linear differential equations.","category":"section"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#Installation","page":"OrdinaryDiffEqExponentialRK","title":"Installation","text":"To be able to access the solvers in OrdinaryDiffEqLinear, you must first install them use the Julia package manager:\n\nusing Pkg\nPkg.add(\"OrdinaryDiffEqExponentialRK\")\n\nThis will only install the solvers listed at the bottom of this page. If you want to explore other solvers for your problem, you will need to install some of the other libraries listed in the navigation bar on the left.","category":"section"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#Example-usage","page":"OrdinaryDiffEqExponentialRK","title":"Example usage","text":"using OrdinaryDiffEqExponentialRK, SciMLOperators\nA = [2.0 -1.0; -1.0 2.0]\nlinnonlin_f1 = MatrixOperator(A)\nlinnonlin_f2 = (du, u, p, t) -> du .= 1.01 .* u\nlinnonlin_fun_iip = SplitFunction(linnonlin_f1, linnonlin_f2)\ntspan = (0.0, 1.0)\nu0 = [0.1, 0.1]\nprob = SplitODEProblem(linnonlin_fun_iip, u0, tspan)\nsol = solve(prob, ETDRK4(), dt = 1 / 4)","category":"section"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#Full-list-of-solvers","page":"OrdinaryDiffEqExponentialRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK.LawsonEuler","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.LawsonEuler","text":"LawsonEuler(; krylov = false,\n              m = 30,\n              iop = 0)\n\nSemilinear ODE solver First order exponential Euler scheme (fixed timestepping)\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK.NorsettEuler","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.NorsettEuler","text":"NorsettEuler(; krylov = false,\n               m = 30,\n               iop = 0)\n\nSemilinear ODE solver First order exponential-RK scheme. Alias: ETD1\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETD2","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETD2","text":"ETD2: Exponential Runge-Kutta Method Second order Exponential Time Differencing method (in development).\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETDRK2","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETDRK2","text":"ETDRK2(; krylov = false,\n         m = 30,\n         iop = 0)\n\nSemilinear ODE solver 2nd order exponential-RK scheme.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETDRK3","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETDRK3","text":"ETDRK3(; krylov = false,\n         m = 30,\n         iop = 0)\n\nSemilinear ODE solver 3rd order exponential-RK scheme.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK.ETDRK4","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.ETDRK4","text":"ETDRK4(; krylov = false,\n         m = 30,\n         iop = 0)\n\nSemilinear ODE solver 4th order exponential-RK scheme (fixed timestepping)\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/ExponentialRK/#OrdinaryDiffEqExponentialRK.HochOst4","page":"OrdinaryDiffEqExponentialRK","title":"OrdinaryDiffEqExponentialRK.HochOst4","text":"HochOst4(; krylov = false,\n           m = 30,\n           iop = 0)\n\nSemilinear ODE solver 4th order exponential-RK scheme with stiff order 4.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nHochbruck, Marlis, and Alexander Ostermann. “Exponential Integrators.” Acta   Numerica 19 (2010): 209–286. doi:10.1017/S0962492910000048.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear","text":"Specialized methods for linear and semi-linear differential equations where the system can be written in matrix form du/dt = A(t,u) * u or du/dt = A(t) * u. These methods exploit the linear structure to provide exact solutions or highly accurate integration.","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Key-Properties","page":"OrdinaryDiffEqLinear","title":"Key Properties","text":"Linear ODE methods provide:\n\nExact solutions for time-independent linear systems\nGeometric integration preserving Lie group structure\nHigh-order Magnus expansions for time-dependent linear systems\nLie group methods for matrix differential equations\nExcellent stability for a wide range of linear systems\nSpecialized algorithms for different types of linear operators","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#When-to-Use-Linear-Methods","page":"OrdinaryDiffEqLinear","title":"When to Use Linear Methods","text":"These methods are essential for:\n\nLinear systems du/dt = A * u with constant or time-dependent matrices\nMatrix differential equations on Lie groups (rotation matrices, etc.)\nQuantum dynamics with Hamiltonian evolution\nLinear oscillators and harmonic systems\nTime-dependent linear systems with periodic or smooth coefficients\nGeometric mechanics requiring preservation of group structure","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Mathematical-Background","page":"OrdinaryDiffEqLinear","title":"Mathematical Background","text":"For linear systems du/dt = A(t) * u, the exact solution is u(t) = exp(∫A(s)ds) * u₀. Linear methods approximate this matrix exponential using various mathematical techniques like Magnus expansions, Lie group integrators, and specialized exponential methods.","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Solver-Selection-Guide","page":"OrdinaryDiffEqLinear","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Time-and-state-independent-(constant-A)","page":"OrdinaryDiffEqLinear","title":"Time and state-independent (constant A)","text":"LinearExponential: Exact solution for du/dt = A * u with constant matrix A","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Time-dependent,-state-independent-(A(t))","page":"OrdinaryDiffEqLinear","title":"Time-dependent, state-independent (A(t))","text":"MagnusMidpoint: Second-order Magnus method\nMagnusLeapfrog: Second-order Magnus leapfrog scheme\nMagnusGauss4: Fourth-order with Gauss quadrature\nMagnusGL4: Fourth-order Gauss-Legendre Magnus method\nMagnusGL6: Sixth-order Gauss-Legendre Magnus method\nMagnusGL8: Eighth-order Gauss-Legendre Magnus method\nMagnusNC6: Sixth-order Newton-Cotes Magnus method\nMagnusNC8: Eighth-order Newton-Cotes Magnus method","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#State-dependent-(A(u))","page":"OrdinaryDiffEqLinear","title":"State-dependent (A(u))","text":"LieEuler: First-order Lie group method\nRKMK2: Second-order Runge-Kutta-Munthe-Kaas method\nRKMK4: Fourth-order Runge-Kutta-Munthe-Kaas method\nLieRK4: Fourth-order Lie Runge-Kutta method\nCG2: Second-order Crouch-Grossman method\nCG4a: Fourth-order Crouch-Grossman method\nCayleyEuler: First-order method using Cayley transformations","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Adaptive-methods","page":"OrdinaryDiffEqLinear","title":"Adaptive methods","text":"MagnusAdapt4: Fourth-order adaptive Magnus method","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Time-and-state-dependent-(A(t,u))","page":"OrdinaryDiffEqLinear","title":"Time and state-dependent (A(t,u))","text":"CG3: Third-order Crouch-Grossman method for most general case","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Method-Selection-Guidelines","page":"OrdinaryDiffEqLinear","title":"Method Selection Guidelines","text":"For constant linear systems: LinearExponential (exact)\nFor time-dependent systems: Magnus methods based on desired order\nFor matrix Lie groups: Lie group methods (RKMK, LieRK4, CG)\nFor high accuracy: Higher-order Magnus methods (GL6, GL8)\nFor adaptive integration: MagnusAdapt4","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Special-Considerations","page":"OrdinaryDiffEqLinear","title":"Special Considerations","text":"These methods require:\n\nProper problem formulation with identified linear structure\nMatrix operator interface for operator-based problems\nUnderstanding of Lie group structure for geometric problems","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Installation","page":"OrdinaryDiffEqLinear","title":"Installation","text":"To be able to access the solvers in OrdinaryDiffEqLinear, you must first install them use the Julia package manager:\n\nusing Pkg\nPkg.add(\"OrdinaryDiffEqLinear\")\n\nThis will only install the solvers listed at the bottom of this page. If you want to explore other solvers for your problem, you will need to install some of the other libraries listed in the navigation bar on the left.","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Example-usage","page":"OrdinaryDiffEqLinear","title":"Example usage","text":"using OrdinaryDiffEqLinear, SciMLOperators\nfunction update_func!(A, u, p, t)\n    A[1, 1] = 0\n    A[2, 1] = sin(u[1])\n    A[1, 2] = -1\n    A[2, 2] = 0\nend\nA0 = ones(2, 2)\nA = MatrixOperator(A0, update_func! = update_func!)\nu0 = ones(2)\ntspan = (0.0, 30.0)\nprob = ODEProblem(A, u0, tspan)\nsol = solve(prob, LieRK4(), dt = 1 / 4)","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Full-list-of-solvers","page":"OrdinaryDiffEqLinear","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Time-and-State-Independent-Solvers","page":"OrdinaryDiffEqLinear","title":"Time and State-Independent Solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Time-Dependent-and-State-Independent-Solvers","page":"OrdinaryDiffEqLinear","title":"Time-Dependent and State-Independent Solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#State-Dependent-Solvers","page":"OrdinaryDiffEqLinear","title":"State-Dependent Solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#Time-and-State-Dependent-Operators","page":"OrdinaryDiffEqLinear","title":"Time and State-Dependent Operators","text":"","category":"section"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.LinearExponential","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.LinearExponential","text":"LinearExponential(; krylov = :off,\n                    m = 10,\n                    iop = 0)\n\nSemilinear ODE solver Exact solution formula for linear, time-independent problems.\n\nKeyword Arguments\n\nkrylov:\n:off: cache the operator beforehand. Requires Matrix(A) method defined for the operator A.\n:simple: uses simple Krylov approximations with fixed subspace size m.\n:adaptive: uses adaptive Krylov approximations with internal timestepping.\nm: Controls the size of Krylov subspace if krylov=:simple, and the initial subspace size if krylov=:adaptive.\niop: If not zero, determines the length of the incomplete orthogonalization procedure       Note that if the linear operator/Jacobian is hermitian,       then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@book{strogatz2018nonlinear,     title={Nonlinear dynamics and chaos: with applications to physics, biology, chemistry, and engineering},     author={Strogatz, Steven H},     year={2018},     publisher={CRC press}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusMidpoint","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusMidpoint","text":"MagnusMidpoint(; krylov = false,\n                 m = 30,\n                 iop = 0)\n\nSemilinear ODE solver Second order Magnus Midpoint method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nhttps://joshuagoings.com/2017/06/15/magnus/\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusLeapfrog","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusLeapfrog","text":"MagnusLeapfrog(; krylov = false,\n                 m = 30,\n                 iop = 0)\n\nSemilinear ODE solver Second order Magnus Leapfrog method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\nhttps://joshuagoings.com/2017/06/15/magnus/\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusGauss4","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusGauss4","text":"MagnusGauss4(; krylov = false,\n               m = 30,\n               iop = 0)\n\nSemilinear ODE solver Fourth order Magnus method approximated using a two stage Gauss quadrature.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{hairer2011solving,   title={Solving differential equations on manifolds},   author={Hairer, Ernst},   journal={Lecture notes},   year={2011} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusNC6","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusNC6","text":"MagnusNC6(; krylov = false,\n            m = 30,\n            iop = 0)\n\nSemilinear ODE solver Sixth order Magnus method approximated using Newton-Cotes quadrature.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{blanes2000improved,   title={Improved high order integrators based on the Magnus expansion},   author={Blanes, Sergio and Casas, Fernando and Ros, Javier},   journal={BIT Numerical Mathematics},   volume={40},   number={3},   pages={434–450},   year={2000},   publisher={Springer} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusGL6","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusGL6","text":"MagnusGL6(; krylov = false,\n            m = 30,\n            iop = 0)\n\nSemilinear ODE solver Sixth order Magnus method approximated using Gauss-Legendre quadrature.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{blanes2000improved,   title={Improved high order integrators based on the Magnus expansion},   author={Blanes, Sergio and Casas, Fernando and Ros, Javier},   journal={BIT Numerical Mathematics},   volume={40},   number={3},   pages={434–450},   year={2000},   publisher={Springer} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusGL8","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusGL8","text":"MagnusGL8(; krylov = false,\n            m = 30,\n            iop = 0)\n\nSemilinear ODE solver Eighth order Magnus method approximated using Newton-Cotes quadrature.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{blanes2000improved,   title={Improved high order integrators based on the Magnus expansion},   author={Blanes, Sergio and Casas, Fernando and Ros, Javier},   journal={BIT Numerical Mathematics},   volume={40},   number={3},   pages={434–450},   year={2000},   publisher={Springer} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusNC8","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusNC8","text":"MagnusNC8(; krylov = false,\n            m = 30,\n            iop = 0)\n\nSemilinear ODE solver Eighth order Magnus method approximated using Gauss-Legendre quadrature.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{blanes2000improved,   title={Improved high order integrators based on the Magnus expansion},   author={Blanes, Sergio and Casas, Fernando and Ros, Javier},   journal={BIT Numerical Mathematics},   volume={40},   number={3},   pages={434–450},   year={2000},   publisher={Springer} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusGL4","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusGL4","text":"MagnusGL4(; krylov = false,\n            m = 30,\n            iop = 0)\n\nSemilinear ODE solver Fourth order Magnus method approximated using Gauss-Legendre quadrature.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{blanes2009magnus,   title={The Magnus expansion and some of its applications},   author={Blanes, Sergio and Casas, Fernando and Oteo, Jose-Angel and Ros, Jos{'e}},   journal={Physics reports},   volume={470},   number={5-6},   pages={151–238},   year={2009},   publisher={Elsevier} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.LieEuler","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.LieEuler","text":"LieEuler(; krylov = false,\n           m = 30,\n           iop = 0)\n\nSemilinear ODE solver description\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{celledoni2014introduction,   title={An introduction to Lie group integrators–basics, new developments and applications},   author={Celledoni, Elena and Marthinsen, H{\u0007a}kon and Owren, Brynjulf},   journal={Journal of Computational Physics},   volume={257},   pages={1040–1061},   year={2014},   publisher={Elsevier} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.RKMK2","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.RKMK2","text":"RKMK2(; krylov = false,\n        m = 30,\n        iop = 0)\n\nSemilinear ODE solver Second order Runge–Kutta–Munthe-Kaas method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{celledoni2014introduction,   title={An introduction to Lie group integrators–basics, new developments and applications},   author={Celledoni, Elena and Marthinsen, H{\u0007a}kon and Owren, Brynjulf},   journal={Journal of Computational Physics},   volume={257},   pages={1040–1061},   year={2014},   publisher={Elsevier} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.RKMK4","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.RKMK4","text":"RKMK4(; krylov = false,\n        m = 30,\n        iop = 0)\n\nSemilinear ODE solver Fourth order Runge–Kutta–Munthe-Kaas method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{celledoni2014introduction,   title={An introduction to Lie group integrators–basics, new developments and applications},   author={Celledoni, Elena and Marthinsen, H{\u0007a}kon and Owren, Brynjulf},   journal={Journal of Computational Physics},   volume={257},   pages={1040–1061},   year={2014},   publisher={Elsevier} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.LieRK4","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.LieRK4","text":"LieRK4(; krylov = false,\n         m = 30,\n         iop = 0)\n\nSemilinear ODE solver Fourth order Lie Runge-Kutta method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{celledoni2014introduction,   title={An introduction to Lie group integrators–basics, new developments and applications},   author={Celledoni, Elena and Marthinsen, H{\u0007a}kon and Owren, Brynjulf},   journal={Journal of Computational Physics},   volume={257},   pages={1040–1061},   year={2014},   publisher={Elsevier} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.CG2","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.CG2","text":"CG2(; krylov = false,\n      m = 30,\n      iop = 0)\n\nSemilinear ODE solver Second order Crouch–Grossman method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{celledoni2014introduction,   title={An introduction to Lie group integrators–basics, new developments and applications},   author={Celledoni, Elena and Marthinsen, H{\u0007a}kon and Owren, Brynjulf},   journal={Journal of Computational Physics},   volume={257},   pages={1040–1061},   year={2014},   publisher={Elsevier} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.CG4a","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.CG4a","text":"CG4a(; krylov = false,\n       m = 30,\n       iop = 0)\n\nSemilinear ODE solver  Fourth order Crouch-Grossman method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{jackiewicz2000construction,   title={Construction of Runge–Kutta methods of Crouch–Grossman type of high order},   author={Jackiewicz, Zdzislaw and Marthinsen, Arne and Owren, Brynjulf},   journal={Advances in Computational Mathematics},   volume={13},   pages={405–415},   year={2000},   publisher={Springer} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.MagnusAdapt4","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.MagnusAdapt4","text":"MagnusAdapt4()\n\nSemilinear ODE solver Fourth Order Adaptive Magnus method.\n\nKeyword Arguments\n\nReferences\n\n@article{li2008adaptive,     title={Adaptive explicit Magnus numerical method for nonlinear dynamical systems},     author={Li, Wen-cheng and Deng, Zi-chen},     journal={Applied Mathematics and Mechanics},     volume={29},     number={9},     pages={1111–1118},     year={2008},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.CayleyEuler","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.CayleyEuler","text":"CayleyEuler()\n\nSemilinear ODE solver First order method using Cayley transformations.\n\nKeyword Arguments\n\nReferences\n\n@article{iserles2000lie,     title={Lie-group methods},     author={Iserles, Arieh and Munthe-Kaas, Hans Z and Norsett, Syvert P and Zanna, Antonella},     journal={Acta numerica},     volume={9},     pages={215–365},     year={2000},     publisher={Cambridge University Press}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/semilinear/Linear/#OrdinaryDiffEqLinear.CG3","page":"OrdinaryDiffEqLinear","title":"OrdinaryDiffEqLinear.CG3","text":"CG3(; krylov = false,\n      m = 30,\n      iop = 0)\n\nSemilinear ODE solver Third order Crouch-Grossman method.\n\nKeyword Arguments\n\nkrylov: Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.   krylov=true is much faster for larger systems and is thus recommended whenever there are >100 ODEs.\nm: Controls the size of Krylov subspace.\niop: If not zero, determines the length of the incomplete orthogonalization procedure (IOP).       Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nReferences\n\n@article{crouch1993numerical,   title={Numerical integration of ordinary differential equations on manifolds},   author={Crouch, Peter E and Grossman, R},   journal={Journal of Nonlinear Science},   volume={3},   pages={1–33},   year={1993},   publisher={Springer} }\n\n\n\n\n\n","category":"type"},{"location":"solvers/nonautonomous_linear_ode/#Non-autonomous-Linear-ODE-/-Lie-Group-ODE-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Non-autonomous linear ODE solvers focus on equations in the general form of\n\nu^prime = A(upt)u\n\nand utilize the Lie group structure of the solution to accelerate the numerical methods and capture certain properties in the solution process. One common simplification is for solvers to require state-independent operators, which implies the form:\n\nu^prime = A(t)u\n\nAnother type of solver is needed when the operators are state-dependent, i.e.\n\nu^prime = A(u)u\n\nOthers specifically require linearity, i.e.\n\nu^prime = Au\n\nwhere A is a constant operator.","category":"section"},{"location":"solvers/nonautonomous_linear_ode/#Recommendations","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Recommendations","text":"It is recommended to always specialize on the properties of the operator as much as possible.","category":"section"},{"location":"solvers/nonautonomous_linear_ode/#Standard-ODE-Integrators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Standard ODE Integrators","text":"The standard ODE integrators will work on Non-autonomous linear ODE problems via an automatic transformation to a first-order ODE. See the ODE solvers page for more details.","category":"section"},{"location":"solvers/nonautonomous_linear_ode/#Specialized-OrdinaryDiffEq.jl-Integrators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Specialized OrdinaryDiffEq.jl Integrators","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a “free” interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).\n\nNote that all of these methods are fixed timestep unless otherwise specified.","category":"section"},{"location":"solvers/nonautonomous_linear_ode/#Exponential-Methods-for-Linear-and-Affine-Problems","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Exponential Methods for Linear and Affine Problems","text":"These methods require that A is constant.\n\nLinearExponential - Exact solution formula for linear, time-independent problems.\n\nOptions:\n\nkrylov - symbol. One of\n:off (default) - cache the operator beforehand. Requires Matrix(A) method defined for the operator A.\n:simple - uses simple Krylov approximations with fixed subspace size m.\n:adaptive - uses adaptive Krylov approximations with internal timestepping.\nm - integer, default: 10. Controls the size of Krylov subspace if krylov=:simple, and the initial subspace size if krylov=:adaptive.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/Jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\n\nimport DifferentialEquations as DE, SciMLOperators\n_A = [2 -1; -3 -5] / 5\nA = SciMLOperators.MatrixOperator(_A)\nprob = DE.ODEProblem(A, [1.0, -1.0], (1.0, 6.0))\nsol = DE.solve(prob, DE.LinearExponential())\n\nnote: Note\nLinearExponential is exact, and thus it uses dt=tspan[2]-tspan[1] by default. The interpolation used is inexact (3rd order Hermite). Thus, values generated by the interpolation (via sol(t) or saveat) will be inexact with increasing error as the size of the time span grows. To counteract this, directly set dt or use tstops instead of saveat. For more information, see this issue.","category":"section"},{"location":"solvers/nonautonomous_linear_ode/#State-Independent-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"State-Independent Solvers","text":"These methods require A is only dependent on the independent variable, i.e. A(t).\n\nMagnusMidpoint - Second order Magnus Midpoint method.\nMagnusLeapfrog- Second order Magnus Leapfrog method.\nMagnusGauss4 - Fourth order Magnus method approximated using a two stage Gauss quadrature.\nMagnusGL4- Fourth order Magnus method approximated using Gauss-Legendre quadrature.\nMagnusNC6- Sixth order Magnus method approximated using Newton-Cotes quadrature.\nMagnusGL6- Sixth order Magnus method approximated using Gauss-Legendre quadrature.\nMagnusNC8- Eighth order Magnus method approximated using Newton-Cotes quadrature.\nMagnusGL8- Eighth order Magnus method approximated using Gauss-Legendre quadrature.\n\nExample:\n\nfunction update_func(A, u, p, t)\n    A[1, 1] = cos(t)\n    A[2, 1] = sin(t)\n    A[1, 2] = -sin(t)\n    A[2, 2] = cos(t)\nend\nA = SciMLOperators.MatrixOperator(ones(2, 2), update_func! = update_func)\nprob = DE.ODEProblem(A, ones(2), (1.0, 6.0))\nsol = DE.solve(prob, DE.MagnusGL6(), dt = 1 / 10)\n\nThe initial values for A are irrelevant in this and similar cases, as the update_func immediately overwrites them. Starting with ones(2,2) is just a convenient way to get a mutable 2x2 matrix.","category":"section"},{"location":"solvers/nonautonomous_linear_ode/#State-Dependent-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"State-Dependent Solvers","text":"These methods can be used when A is dependent on the state variables, i.e. A(u).\n\nCayleyEuler - First order method using Cayley transformations.\nLieEuler - First order Lie Euler method.\nRKMK2 - Second order Runge–Kutta–Munthe-Kaas method.\nRKMK4 - Fourth order Runge–Kutta–Munthe-Kaas method.\nLieRK4 - Fourth order Lie Runge-Kutta method.\nCG2 - Second order Crouch–Grossman method.\nCG4a - Fourth order Crouch-Grossman method.\nMagnusAdapt4 - Fourth Order Adaptive Magnus method.\n\nExample:\n\nfunction update_func(A, u, p, t)\n    A[1, 1] = 0\n    A[2, 1] = sin(u[1])\n    A[1, 2] = -1\n    A[2, 2] = 0\nend\nA = SciMLOperators.MatrixOperator(ones(2, 2), update_func! = update_func)\nprob = DE.ODEProblem(A, ones(2), (0, 30.0))\nsol = DE.solve(prob, DE.LieRK4(), dt = 1 / 4)\n\nThe above example solves a non-stiff Non-Autonomous Linear ODE with a state dependent operator, using the LieRK4 method. Similarly, a stiff Non-Autonomous Linear ODE with state dependent operators can be solved using specialized adaptive algorithms, like MagnusAdapt4.\n\nExample:\n\nfunction update_func(A, u, p, t)\n    A[1, 1] = 0\n    A[2, 1] = 1\n    A[1, 2] = -2 * (1 - cos(u[2]) - u[2] * sin(u[2]))\n    A[2, 2] = 0\nend\nA = SciMLOperators.MatrixOperator(ones(2, 2), update_func! = update_func)\nprob = DE.ODEProblem(A, ones(2), (30, 150.0))\nsol = DE.solve(prob, DE.MagnusAdapt4())","category":"section"},{"location":"solvers/nonautonomous_linear_ode/#Time-and-State-Dependent-Operators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Time and State-Dependent Operators","text":"These methods can be used when A is dependent on both time and state variables, i.e. A(ut)\n\nCG3 - Third order Crouch-Grossman method.\n\n[1]: A description of IOP can be found in this paper.","category":"section"},{"location":"types/discrete_types/#Discrete-Problems","page":"Discrete Problems","title":"Discrete Problems","text":"","category":"section"},{"location":"types/discrete_types/#Solution-Type","page":"Discrete Problems","title":"Solution Type","text":"DiscreteProblem solutions return an ODESolution. For more information, see the ODE problem definition page for the ODESolution docstring.","category":"section"},{"location":"types/discrete_types/#SciMLBase.DiscreteProblem","page":"Discrete Problems","title":"SciMLBase.DiscreteProblem","text":"Defines a discrete dynamical system problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/discrete_types/\n\nMathematical Specification of a Discrete Problem\n\nTo define a Discrete Problem, you simply need to give the function f and the initial condition u_0 which define a function map:\n\nu_n+1 = f(u_npt_n+1)\n\nf should be specified as f(un,p,t) (or in-place as f(unp1,un,p,t)), and u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. u_n+1 only depends on the previous iteration u_n and t_n+1. The default t_n+1 of FunctionMap is t_n = t_0 + n*dt (with dt=1 being the default). For continuous-time Markov chains, this is the time at which the change is occurring.\n\nNote that if the discrete solver is set to have scale_by_time=true, then the problem is interpreted as the map:\n\nu_n+1 = u_n + dt f(u_npt_n+1)\n\nProblem Type\n\nConstructors\n\nDiscreteProblem(f::ODEFunction,u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the specified functions.\nDiscreteProblem{isinplace,specialize}(f,u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the specified functions.\nDiscreteProblem{isinplace,specialize}(u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the identity map.\n\nisinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the DiffEqFunctions page.\n\nFields\n\nf: The function in the map.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.\n\nNote About Timing\n\nNote that if no dt and not tstops is given, it's assumed that dt=1 and thus tspan=(0,n) will solve for n iterations. If in the solver dt is given, then the number of iterations will change. And if tstops is not empty, the solver will revert to the standard behavior of fixed timestep methods, which is \"step to each tstop\".\n\n\n\n\n\n","category":"type"},{"location":"types/discrete_types/#SciMLBase.DiscreteFunction","page":"Discrete Problems","title":"SciMLBase.DiscreteFunction","text":"struct DiscreteFunction{iip, specialize, F, Ta, O, SYS, ID} <: SciMLBase.AbstractDiscreteFunction{iip}\n\nA representation of a discrete dynamical system f, defined by:\n\nu_n+1 = f(upt_n+1)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nDiscreteFunction{iip,specialize}(f;\n                                analytic = __has_analytic(f) ? f.analytic : nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p,t) or du = f(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\n\niip: In-Place vs Out-Of-Place\n\nFor more details on this argument, see the ODEFunction documentation.\n\nspecialize: Controlling Compilation and Specialization\n\nFor more details on this argument, see the ODEFunction documentation.\n\nFields\n\nThe fields of the DiscreteFunction type directly match the names of the inputs.\n\n\n\n\n\n","category":"type"},{"location":"types/ode_types/#ode_prob","page":"ODE Problems","title":"ODE Problems","text":"","category":"section"},{"location":"types/ode_types/#Solution-Type","page":"ODE Problems","title":"Solution Type","text":"","category":"section"},{"location":"types/ode_types/#ODE-Alias-Specifier","page":"ODE Problems","title":"ODE Alias Specifier","text":"","category":"section"},{"location":"types/ode_types/#Example-Problems","page":"ODE Problems","title":"Example Problems","text":"Example problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_ode_linear, you can do something like:\n\n#] add DiffEqProblemLibrary\nimport DiffEqProblemLibrary.ODEProblemLibrary\nimport DifferentialEquations as DE\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = DE.solve(prob)","category":"section"},{"location":"types/ode_types/#SciMLBase.ODEProblem","page":"ODE Problems","title":"SciMLBase.ODEProblem","text":"Defines an ordinary differential equation (ODE) problem. Documentation Page: https://docs.sciml.ai/DiffEqDocs/stable/types/ode_types/\n\nMathematical Specification of an ODE Problem\n\nTo define an ODE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:\n\nM fracdudt = f(upt)\n\nThere are two different ways of specifying f:\n\nf(du,u,p,t): in-place. Memory-efficient when avoiding allocations. Best option for most cases unless mutation is not allowed.\nf(u,p,t): returning du. Less memory-efficient way, particularly suitable when mutation is not allowed (e.g. with certain automatic differentiation packages such as Zygote).\n\nu₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.\n\nFor the mass matrix M, see the documentation of ODEFunction.\n\nProblem Type\n\nConstructors\n\nODEProblem can be constructed by first building an ODEFunction or by simply passing the ODE right-hand side to the constructor. The constructors are:\n\nODEProblem(f::ODEFunction,u0,tspan,p=NullParameters();kwargs...)\nODEProblem{isinplace,specialize}(f,u0,tspan,p=NullParameters();kwargs...) : Defines the ODE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. specialize optionally controls the specialization level. See the specialization levels section of the SciMLBase documentation for more details. The default is AutoSpecialize.\n\nFor more details on the in-place and specialization controls, see the ODEFunction documentation.\n\nParameters are optional, and if not given, then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.\n\nFor specifying Jacobians and mass matrices, see the ODEFunction documentation.\n\nFields\n\nf: The function in the ODE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters.\nkwargs: The keyword arguments passed onto the solves.\n\nExample Problem\n\nusing SciMLBase\nfunction lorenz!(du,u,p,t)\n  du[1] = 10.0(u[2]-u[1])\n  du[2] = u[1]*(28.0-u[3]) - u[2]\n  du[3] = u[1]*u[2] - (8/3)*u[3]\nend\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz!,u0,tspan)\n\n# Test that it worked\nusing OrdinaryDiffEq\nsol = solve(prob,Tsit5())\nusing Plots; plot(sol,vars=(1,2,3))\n\nMore Example Problems\n\nExample problems can be found in DiffEqProblemLibrary.jl.\n\nTo use a sample problem, such as prob_ode_linear, you can do something like:\n\n#] add ODEProblemLibrary\nusing ODEProblemLibrary\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)\n\n\n\n\n\n","category":"type"},{"location":"types/ode_types/#SciMLBase.ODEFunction","page":"ODE Problems","title":"SciMLBase.ODEFunction","text":"struct ODEFunction{iip, specialize, F, TMM, Ta, Tt, TJ, JVP, VJP, JP, SP, TW, TWt, WP, TPJ, O, TCV, SYS, ID<:Union{Nothing, SciMLBase.OverrideInitData}, NLP<:Union{Nothing, SciMLBase.ODENLStepData}} <: SciMLBase.AbstractODEFunction{iip}\n\nA representation of an ODE function f, defined by:\n\nM fracdudt = f(upt)\n\nand all of its related functions, such as the Jacobian of f, its gradient with respect to time, and more. For all cases, u0 is the initial condition, p are the parameters, and t is the independent variable.\n\nConstructor\n\nODEFunction{iip,specialize}(f;\n                           mass_matrix = __has_mass_matrix(f) ? f.mass_matrix : I,\n                           analytic = __has_analytic(f) ? f.analytic : nothing,\n                           tgrad= __has_tgrad(f) ? f.tgrad : nothing,\n                           jac = __has_jac(f) ? f.jac : nothing,\n                           jvp = __has_jvp(f) ? f.jvp : nothing,\n                           vjp = __has_vjp(f) ? f.vjp : nothing,\n                           jac_prototype = __has_jac_prototype(f) ? f.jac_prototype : nothing,\n                           sparsity = __has_sparsity(f) ? f.sparsity : jac_prototype,\n                           paramjac = __has_paramjac(f) ? f.paramjac : nothing,\n                           colorvec = __has_colorvec(f) ? f.colorvec : nothing,\n                           sys = __has_sys(f) ? f.sys : nothing)\n\nNote that only the function f itself is required. This function should be given as f!(du,u,p,t) or du = f(u,p,t). See the section on iip for more details on in-place vs out-of-place handling.\n\nAll of the remaining functions are optional for improving or accelerating the usage of f. These include:\n\nmass_matrix: the mass matrix M represented in the ODE function. Can be used to determine that the equation is actually a differential-algebraic equation (DAE) if M is singular. Note that in this case special solvers are required, see the DAE solver page for more details: https://docs.sciml.ai/DiffEqDocs/stable/solvers/dae_solve/. Must be an AbstractArray or an AbstractSciMLOperator.\nanalytic(u0,p,t): used to pass an analytical solution function for the analytical solution of the ODE. Generally only used for testing and development of the solvers.\ntgrad(dT,u,p,t) or dT=tgrad(u,p,t): returns fracf(upt)t\njac(J,u,p,t) or J=jac(u,p,t): returns fracdfdu\njvp(Jv,v,u,p,t) or Jv=jvp(v,u,p,t): returns the directional derivativefracdfdu v\nvjp(Jv,v,u,p,t) or Jv=vjp(v,u,p,t): returns the adjoint derivativefracdfdu^ v\njac_prototype: a prototype matrix matching the type that matches the Jacobian. For example, if the Jacobian is tridiagonal, then an appropriately sized Tridiagonal matrix can be used as the prototype and integrators will specialize on this structure where possible. Non-structured sparsity patterns should use a SparseMatrixCSC with a correct sparsity pattern for the Jacobian. The default is nothing, which means a dense Jacobian.\nparamjac(pJ,u,p,t): returns the parameter Jacobian fracdfdp.\ncolorvec: a color vector according to the SparseDiffTools.jl definition for the sparsity pattern of the jac_prototype. This specializes the Jacobian construction when using finite differences and automatic differentiation to be computed in an accelerated manner based on the sparsity pattern. Defaults to nothing, which means a color vector will be internally computed on demand when required. The cost of this operation is highly dependent on the sparsity pattern.\n\niip: In-Place vs Out-Of-Place\n\niip is the optional boolean for determining whether a given function is written to be used in-place or out-of-place. In-place functions are f!(du,u,p,t) where the return is ignored, and the result is expected to be mutated into the value of du. Out-of-place functions are du=f(u,p,t).\n\nNormally, this is determined automatically by looking at the method table for f and seeing the maximum number of arguments in available dispatches. For this reason, the constructor ODEFunction(f) generally works (but is type-unstable). However, for type-stability or to enforce correctness, this option is passed via ODEFunction{true}(f).\n\nspecialize: Controlling Compilation and Specialization\n\nThe specialize parameter controls the specialization level of the ODEFunction on the function f. This allows for a trade-off between compile and run time performance. The available specialization levels are:\n\nSciMLBase.AutoSpecialize: this form performs a lazy function wrapping on the functions of the ODE in order to stop recompilation of the ODE solver, but allow for the prob.f to stay unwrapped for normal usage. This is the default specialization level and strikes a balance in compile time vs runtime performance.\nSciMLBase.FullSpecialize: this form fully specializes the ODEFunction on the constituent functions that make its fields. As such, each ODEFunction in this form is uniquely typed, requiring re-specialization and compilation for each new ODE definition. This form has the highest compile-time at the cost of being the most optimal in runtime. This form should be preferred for long-running calculations (such as within optimization loops) and for benchmarking.\nSciMLBase.NoSpecialize: this form fully unspecializes the function types in the ODEFunction definition by using an Any type declaration. As a result, it can result in reduced runtime performance, but is the form that induces the least compile-time.\nSciMLBase.FunctionWrapperSpecialize: this is an eager function wrapping form. It is unsafe with many solvers, and thus is mostly used for development testing.\n\nFor more details, see the specialization levels section of the SciMLBase documentation.\n\nFields\n\nThe fields of the ODEFunction type directly match the names of the inputs.\n\nMore Details on Jacobians\n\nThe following example creates an inplace ODEFunction whose Jacobian is a Diagonal:\n\nusing LinearAlgebra\nf = (du,u,p,t) -> du .= t .* u\njac = (J,u,p,t) -> (J[1,1] = t; J[2,2] = t; J)\njp = Diagonal(zeros(2))\nfun = ODEFunction(f; jac=jac, jac_prototype=jp)\n\nNote that the integrators will always make a deep copy of fun.jac_prototype, so there's no worry of aliasing.\n\nIn general, the Jacobian prototype can be anything that has mul! defined, in particular sparse matrices or custom lazy types that support mul!. A special case is when the jac_prototype is a AbstractSciMLOperator, in which case you do not need to supply jac as it is automatically set to update_coefficients!. Refer to the AbstractSciMLOperators documentation for more information on setting up time/parameter dependent operators.\n\nExamples\n\nDeclaring Explicit Jacobians for ODEs\n\nThe most standard case, declaring a function for a Jacobian is done by overloading the function f(du,u,p,t) with an in-place updating function for the Jacobian: f_jac(J,u,p,t) where the value type is used for dispatch. For example, take the Lotka-Volterra model:\n\nfunction f(du,u,p,t)\n  du[1] = 2.0 * u[1] - 1.2 * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\n\nTo declare the Jacobian, we simply add the dispatch:\n\nfunction f_jac(J,u,p,t)\n  J[1,1] = 2.0 - 1.2 * u[2]\n  J[1,2] = -1.2 * u[1]\n  J[2,1] = 1 * u[2]\n  J[2,2] = -3 + u[1]\n  nothing\nend\n\nThen we can supply the Jacobian with our ODE as:\n\nff = ODEFunction(f;jac=f_jac)\n\nand use this in an ODEProblem:\n\nprob = ODEProblem(ff,ones(2),(0.0,10.0))\n\nSymbolically Generating the Functions\n\nSee the modelingtoolkitize function from ModelingToolkit.jl for automatically symbolically generating the Jacobian and more from the numerically-defined functions.\n\n\n\n\n\n","category":"type"},{"location":"types/ode_types/#SciMLBase.ODESolution","page":"ODE Problems","title":"SciMLBase.ODESolution","text":"struct ODESolution{T, N, uType, uType2, DType, tType, rateType, discType, P, A, IType, S, AC<:Union{Nothing, Vector{Int64}}, R, O, V} <: SciMLBase.AbstractODESolution{T, N, uType}\n\nRepresentation of the solution to an ordinary differential equation defined by an ODEProblem.\n\nDESolution Interface\n\nFor more information on interacting with DESolution types, check out the Solution Handling page of the DifferentialEquations.jl documentation.\n\nhttps://docs.sciml.ai/DiffEqDocs/stable/basics/solution/\n\nFields\n\nu: the representation of the ODE solution. Given as an array of solutions, where u[i] corresponds to the solution at time t[i]. It is recommended in most cases one does not access sol.u directly and instead use the array interface described in the Solution Handling page of the DifferentialEquations.jl documentation.\nt: the time points corresponding to the saved values of the ODE solution.\nprob: the original ODEProblem that was solved.\nalg: the algorithm type used by the solver.\nstats: statistics of the solver, such as the number of function evaluations required, number of Jacobians computed, and more.\nretcode: the return code from the solver. Used to determine whether the solver solved successfully, whether it terminated early due to a user-defined callback, or whether it exited due to an error. For more details, see the return code documentation.\n\n\n\n\n\n","category":"type"},{"location":"types/ode_types/#SciMLBase.ODEAliasSpecifier","page":"ODE Problems","title":"SciMLBase.ODEAliasSpecifier","text":"ODEAliasSpecifier(;alias_p = nothing, alias_f = nothing, alias_u0 = false, alias_du0 = false, alias_tstops = false, alias = nothing)\n\nHolds information on what variables to alias when solving an ODE. Conforms to the AbstractAliasSpecifier interface. \n\nWhen a keyword argument is nothing, the default behaviour of the solver is used.\n\nKeywords\n\nalias_p::Union{Bool, Nothing}\nalias_f::Union{Bool, Nothing}\nalias_u0::Union{Bool, Nothing}: alias the u0 array. Defaults to false .\nalias_du0::Union{Bool, Nothing}: alias the du0 array for DAEs. Defaults to false.\nalias_tstops::Union{Bool, Nothing}: alias the tstops array\nalias::Union{Bool, Nothing}: sets all fields of the ODEAliasSpecifier to alias\n\n\n\n\n\n","category":"type"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_linear","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_linear","text":"Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0=frac12, α=101, and solution\n\nu(t) = u_0 e^αt\n\nwith Float64s. The parameter is α\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_2Dlinear","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_2Dlinear","text":"4×2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers, α=101, and solution\n\nu(t) = u_0 e^αt\n\nwith Float64s\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_bigfloatlinear","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_bigfloatlinear","text":"Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0=frac12, α=101, and solution\n\nu(t) = u_0 e^αt\n\nwith BigFloats\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_bigfloat2Dlinear","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_bigfloat2Dlinear","text":"4×2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers, α=101, and solution\n\nu(t) = u_0 e^αt\n\nwith BigFloats\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_large2Dlinear","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_large2Dlinear","text":"100×100 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers, α=101, and solution\n\nu(t) = u_0 e^αt\n\nwith Float64s\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_2Dlinear_notinplace","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_2Dlinear_notinplace","text":"4×2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers, α=101, and solution\n\nu(t) = u_0 e^αt\n\non Float64. Purposefully not in-place as a test.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_lotkavolterra","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_lotkavolterra","text":"Lotka-Volterra Equations (Non-stiff)\n\nbeginalign*\nfracdxdt = ax - bxy \nfracdydt = -cy + dxy \nendalign*\n\nwith initial condition x=y=1\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_fitzhughnagumo","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_fitzhughnagumo","text":"Fitzhugh-Nagumo (Non-stiff)\n\nbeginalign*\nfracdvdt = v - fracv^33 - w + I_est \nτ fracdwdt = v + a -bw\nendalign*\n\nwith initial condition v=w=1\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_threebody","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_threebody","text":"The ThreeBody problem as written by Hairer: (Non-stiff)\n\nbeginalign*\nfracdy₁dt = y₁ + 2fracdy₂dt - barμfracy₁+μD₁ - μfracy₁-barμD₂ \nfracdy₂dt = y₂ - 2fracdy₁dt - barμfracy₂D₁ - μfracy₂D₂\nendalign*\n\nbeginalign*\nD₁ = left((y₁+μ)^2 + y₂^2right)^32 \nD₂ = left((y₁-barμ)^2 + y₂^2right)^32 \nμ = 0012277471 \nbarμ = 1-μ\nendalign*\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129\n\nUsually solved on t₀ = 00 and T = 170652165601579625588917206249 Periodic with that setup.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_pleiades","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_pleiades","text":"Pleiades Problem (Non-stiff)\n\nbeginalign*\nfracd^2xᵢdt^2 = sum_ji mⱼ(xⱼ-xᵢ)rᵢⱼ \nfracd^2yᵢdt^2 = sum_ji mⱼ(yⱼ-yᵢ)rᵢⱼ\nendalign*\n\nwhere\n\nrᵢⱼ = left((xᵢ-xⱼ)^2 + (yᵢ-yⱼ)^2right)^32\n\nand initial conditions are\n\nbeginalign*\nx₁(0) =  3  y₁(0) =  3 \nx₂(0) =  3  y₂(0) = -3 \nx₃(0) = -1  y₃(0) =  2 \nx₄(0) = -3  y₄(0) =  0 \nx₅(0) =  2  y₅(0) =  0 \nx₆(0) = -2  y₆(0) = -4 \nx₇(0) =  2  y₇(0) =  4\nendalign*\n\nand with fracdxᵢ(0)dt = fracdyᵢ(0)dt = 0 except for\n\nbeginalign*\nfracdx₆(0)dt = 175 \nfracdx₇(0)dt = -15 \nfracdy₄(0)dt = -125 \nfracdy₅(0)dt = 1\nendalign*\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 244\n\nUsually solved from 0 to 3.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_vanderpol","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_vanderpol","text":"Van der Pol Equations\n\nbeginalign*\nfracdxdt = y \nfracdydt = μ left(left(1-x^2right) y - xright)\nendalign*\n\nwith μ=10 and u_0=sqrt3 0 (where u1 = x, u2 = y)\n\nNon-stiff parameters.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_vanderpol_stiff","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_vanderpol_stiff","text":"Van der Pol Equations\n\nbeginalign*\nfracdxdt = y \nfracdydt = μ left(left(1 - x^2right) y - xright)\nendalign*\n\nwith μ=10^6 and u_0=sqrt3 0 (where u1 = x, u2 = y)\n\nStiff parameters.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_rober","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_rober","text":"The Robertson biochemical reactions: (Stiff)\n\nbeginalign*\nfracdy₁dt = -k₁y₁ + k₃y₂y₃ \nfracdy₂dt =  k₁y₁ - k₂y₂^2 - k₃y₂y₃ \nfracdy₃dt =  k₂y₂^2\nendalign*\n\nwhere k₁=004, k₂=310^7, k₃=10^4. For details, see:\n\nHairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129\n\nUsually solved on 010^11\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_rigidbody","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_rigidbody","text":"Rigid Body Equations (Non-stiff)\n\nbeginalign*\nfracdy₁dt = I₁y₂y₃ \nfracdy₂dt = I₂y₁y₃ \nfracdy₃dt = I₃y₁y₂\nendalign*\n\nwith I₁=-2, I₂=125, and I₃=-12.\n\nThe initial condition is y=100009.\n\nFrom Solving Differential Equations in R by Karline Soetaert\n\nor Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 244\n\nUsually solved from 0 to 20.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_hires","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_hires","text":"Hires Problem (Stiff)\n\nIt is in the form of\n\nfracdydt = f(y)\n\nwith\n\n y(0)=y_0 quad y in ℝ^8 quad 0  t  3218122\n\nwhere f is defined by\n\nf(y) = beginpmatrix\n171y_1   + 043y_2 + 832y_3  + 00007y_4           \n 171y_1    875y_2                                  \n1003y_3  + 043y_4 + 0035y_5                       \n 832y_2   + 171y_3  112y_4                        \n1745y_5  + 043y_6 + 043y_7                        \n280y_6y_8 + 069y_4 + 171y_5   043y_6   + 069y_7 \n 280y_6y_8  181y_7                                  \n280y_6y_8 + 181y_7\nendpmatrix\n\nReference: demohires.pdf Notebook: Hires.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_orego","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_orego","text":"Orego Problem (Stiff)\n\nIt is in the form of fracdydt=f(y) quad y(0)=y_0 with\n\ny in ℝ^3 quad 0  t  360\n\nwhere f is defined by\n\nf(y) = beginpmatrix\ns(y_2 - y_1 (1 - q y_1 - y_2)) \n(y_3 - y_2 (1 + y_1))  s \nw (y_1 - y_3)\nendpmatrix\n\nwhere s=7727, w=0161 and q=837510^-6.\n\nReference: demoorego.pdf Notebook: Orego.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_pollution","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_pollution","text":"Pollution Problem (Stiff)\n\nThis IVP is a stiff system of 20 non-linear Ordinary Differential Equations. It is in the form of\n\nfracdydt=f(y)\n\nwith\n\ny(0)=y_0 quad y in ℝ^20 quad 0  t  60\n\nwhere f is defined by\n\nf(y) = beginpmatrix\n-sum_jin110142324 r_j + sum_jin23911122225 r_j \n-r_2 - r_3 - r_9 - r_12 + r_1 + r_21 \n-r_15 + r_1 + r_17 + r_19 + r_22 \n-r_2 - r_16 - r_17 - r_23 + r_15 \n-r_3 + 2r_4 + r_6 + r_7 + r_13 + r_20 \n-r_6 - r_8 - r_14 - r_20 + r_3 + 2r_18 \n-r_4 - r_5 - r_6 + r_13 \nr_4 + r_5 + r_6 + r_7 \n-r_7 - r_8 \n-r_12 + r_7 + r_9 \n-r_9 - r_10 + r_8 + r_11 \nr_9 \n-r_11 + r_10 \n-r_13 + r_12 \nr_14 \n-r_18 - r_19 + r_16 \n-r_20 \nr_20 \n-r_21 - r_22 - r_24 + r_23 + r_25 \n-r_25 + r_24\nendpmatrix\n\nwith the initial condition of\n\ny0 = (0 02 0 004 0 0 01 03 001 0 0 0 0 0 0 0 0007 0 0 0)^T\n\nAnalytical Jacobian is included.\n\nReference: pollu.pdf Notebook: Pollution.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_nonlinchem","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_nonlinchem","text":"Nonlinear system of reactions with an analytical solution\n\nbeginalign*\nfracdy_1dt = -y_1 \nfracdy_2dt = y_1 - y_2^2 \nfracdy_3dt = y_2^2\nendalign*\n\nwith initial condition y=100 on a time span of t in (020)\n\nFrom\n\nLiu, L. C., Tian, B., Xue, Y. S., Wang, M., & Liu, W. J. (2012). Analytic solution  for a nonlinear chemistry system of ordinary differential equations. Nonlinear  Dynamics, 68(1-2), 17-21.\n\nThe analytical solution is implemented, allowing easy testing of ODE solvers.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_brusselator_1d","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_brusselator_1d","text":"1D Brusselator\n\nbeginalign*\nfracut = A - (B+1) u + u^2 v + α frac^2 ux^2 \nfracvt =      B    u - u^2 v + α frac^2 ux^2\nendalign*\n\nand the initial conditions are\n\nbeginalign*\nu(x0) = 1 + sin(2πx) \nv(x0) = 3\nendalign*\n\nwith the boundary condition\n\nbeginalign*\nu(0t) = u(1t) = 1 \nv(0t) = v(1t) = 3\nendalign*\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations II - Stiff and Differential-Algebraic Problems Page 6\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_brusselator_2d","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_brusselator_2d","text":"2D Brusselator\n\nbeginalign*\nfracut = 1 + u^2v - 44u + αleft(frac^2 ux^2 + frac^2 uy^2right) + f(x y t) \nfracvt =     34u - u^2v + αleft(frac^2 ux^2 + frac^2 uy^2right)\nendalign*\n\nwhere\n\nf(x y t) = begincases\n    5  textif  (x-03)^2+(y-06)^2  01^2 text and  t  11 \n    0  textelse\nendcases\n\nand the initial conditions are\n\nbeginalign*\nu(x y 0) = 22  y(1-y)^32 \nv(x y 0) = 27  x(1-x)^32\nendalign*\n\nwith the periodic boundary condition\n\nbeginalign*\nu(x+1yt) = u(xyt) \nu(xy+1t) = u(xyt)\nendalign*\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations II - Stiff and Differential-Algebraic Problems Page 152\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#ODEProblemLibrary.prob_ode_filament","page":"ODE Problems","title":"ODEProblemLibrary.prob_ode_filament","text":"Filament PDE Discretization\n\nNotebook: Filament.ipynb\n\nIn this problem is a real-world biological model from a paper entitled Magnetic dipole with a flexible tail as a self-propelling microdevice. It is a system of PDEs representing a Kirchhoff model of an elastic rod, where the equations of motion are given by the Rouse approximation with free boundary conditions.\n\n\n\n\n\n","category":"constant"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#OrdinaryDiffEqHighOrderRK","page":"OrdinaryDiffEqHighOrderRK","title":"OrdinaryDiffEqHighOrderRK","text":"High-order explicit Runge-Kutta methods for non-stiff differential equations requiring high accuracy. These methods provide alternatives to the Verner methods, though OrdinaryDiffEqVerner methods generally perform better at low tolerances and should be preferred in most cases.","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#Key-Properties","page":"OrdinaryDiffEqHighOrderRK","title":"Key Properties","text":"High-order RK methods provide:\n\nHigh-order accuracy (7th and 8th order) for precise integration\nSpecialized coefficients for specific problem types\nDense output capabilities for some methods\nAlternative approaches to the more commonly used Verner methods","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#When-to-Use-High-Order-RK-Methods","page":"OrdinaryDiffEqHighOrderRK","title":"When to Use High-Order RK Methods","text":"These methods are recommended when:\n\nVerner methods are not suitable for specific problem characteristics\nSpecialized properties are needed (e.g., phase-fitted methods for oscillatory problems)\nResearch or comparison purposes require different high-order method families\nSpecific applications benefit from particular method properties","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#Solver-Selection-Guide","page":"OrdinaryDiffEqHighOrderRK","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#General-high-order-integration","page":"OrdinaryDiffEqHighOrderRK","title":"General high-order integration","text":"Use Vern7 or Vern8 instead - they are generally more efficient","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#Specialized-cases-where-these-methods-may-be-preferred","page":"OrdinaryDiffEqHighOrderRK","title":"Specialized cases where these methods may be preferred","text":"TanYam7: Seventh-order Tanaka-Yamashita method\nTsitPap8: Eighth-order Tsitouras-Papakostas method\nDP8: Eighth-order Dormand-Prince method (Hairer's 8/5/3 implementation)\nPFRK87: Phase-fitted eighth-order method for oscillatory problems","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#Performance-Notes","page":"OrdinaryDiffEqHighOrderRK","title":"Performance Notes","text":"Verner methods are generally more efficient for most high-accuracy applications\nThese methods are provided for specialized use cases and research purposes\nConsider problem-specific properties when choosing between different high-order families\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqHighOrderRK\", \"DP8\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#Full-list-of-solvers","page":"OrdinaryDiffEqHighOrderRK","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#OrdinaryDiffEqHighOrderRK.TanYam7","page":"OrdinaryDiffEqHighOrderRK","title":"OrdinaryDiffEqHighOrderRK.TanYam7","text":"TanYam7(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n          thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Tanaka-Yamashita 7 Runge-Kutta method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nTanaka M., Muramatsu S., Yamashita S., (1992), On the Optimization of Some Nine-Stage     Seventh-order Runge-Kutta Method, Information Processing Society of Japan,     33 (12), pp. 1512-1526.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#OrdinaryDiffEqHighOrderRK.TsitPap8","page":"OrdinaryDiffEqHighOrderRK","title":"OrdinaryDiffEqHighOrderRK.TsitPap8","text":"TsitPap8(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n           thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Tsitouras-Papakostas 8/7 Runge-Kutta method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\n@article{tsitouras1999cheap, title={Cheap error estimation for Runge–Kutta methods}, author={Tsitouras, Ch and Papakostas, SN}, journal={SIAM Journal on Scientific Computing}, volume={20}, number={6}, pages={2067–2088}, year={1999}, publisher={SIAM}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#OrdinaryDiffEqHighOrderRK.DP8","page":"OrdinaryDiffEqHighOrderRK","title":"OrdinaryDiffEqHighOrderRK.DP8","text":"DP8(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n      thread = OrdinaryDiffEq.False())\n\nExplicit Runge-Kutta Method.  Hairer's 8/5/3 adaption of the Dormand-Prince Runge-Kutta method.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I.     Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics,     Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/HighOrderRK/#OrdinaryDiffEqHighOrderRK.PFRK87","page":"OrdinaryDiffEqHighOrderRK","title":"OrdinaryDiffEqHighOrderRK.PFRK87","text":"PFRK87(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n         thread = OrdinaryDiffEq.False(),\n         omega = 0.0)\n\nExplicit Runge-Kutta Method.  Phase-fitted Runge-Kutta of 8th order.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nomega: a periodicity phase estimate,              when accurate this method results in zero numerical dissipation.\n\nReferences\n\n@article{tsitouras2017phase, title={Phase-fitted Runge–Kutta pairs of orders 8 (7)}, author={Tsitouras, Ch and Famelis, I Th and Simos, TE}, journal={Journal of Computational and Applied Mathematics}, volume={321}, pages={226–231}, year={2017}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"solvers/sdde_solve/#SDDE-Solvers","page":"SDDE Solvers","title":"SDDE Solvers","text":"solve(prob::AbstractSDDEProblem, alg; kwargs)\n\nSolves the SDDE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"section"},{"location":"solvers/sdde_solve/#Recommended-Methods","page":"SDDE Solvers","title":"Recommended Methods","text":"The recommended method for SDDE problems are the SDE algorithms. On SDEs you simply reuse the same algorithm as the SDE solver, and StochasticDelayDiffEq.jl will convert it to an SDDE solver. The recommendations for SDDE solvers match those of SDEs, except that only up to strong order 1 is recommended. Also note that order 1 is currently only attainable if there is no delay term in the diffusion function g: delays in the drift function f are compatible with first order convergence. Theoretical issues with higher order methods (1.5+) on SDDEs is currently unknown.\n\nNote that adaptive time stepping utilizes the same rejection sampling with memory technique as SDEs, but no proof of convergence is known for SDDEs.","category":"section"},{"location":"solvers/sdde_solve/#Example","page":"SDDE Solvers","title":"Example","text":"function hayes_modelf(du, u, h, p, t)\n    τ, a, b, c, α, β, γ = p\n    du .= a .* u .+ b .* h(p, t - τ) .+ c\nend\nfunction hayes_modelg(du, u, h, p, t)\n    τ, a, b, c, α, β, γ = p\n    du .= α .* u .+ γ\nend\nh(p, t) = (ones(1) .+ t);\ntspan = (0.0, 10.0)\n\npmul = [1.0, -4.0, -2.0, 10.0, -1.3, -1.2, 1.1]\npadd = [1.0, -4.0, -2.0, 10.0, -0.0, -0.0, 0.1]\n\nprob = SDDEProblem(hayes_modelf, hayes_modelg, [1.0], h, tspan, pmul;\n    constant_lags = (pmul[1],));\nsol = solve(prob, RKMil())","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Solving-the-heat-equation-with-diffusion-implicit-time-stepping","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Solving the heat equation with diffusion-implicit time-stepping","text":"In this tutorial, we'll be solving the heat equation:\n\n_t T = α ²T + β sin(γ z)\n\nwith boundary conditions: T(z=a) = T_textbottom and T(z=b) = T_texttop. We'll solve these equations numerically using Finite Difference Method on cell faces. The same exercise could easily be done on cell centers.","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Code-loading-and-parameters","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Code loading and parameters","text":"First, we'll use / import some packages:\n\nimport Plots\nimport LinearAlgebra\nimport DiffEqBase\nimport OrdinaryDiffEqBDF as BDF\nimport SciMLBase, SciMLOperators\n\nNext, we'll define some global problem parameters:\n\na, b, n = 0, 1, 10               # zmin, zmax, number of cells\nn̂_min, n̂_max = -1, 1            # Outward facing unit vectors\nα = 100;                        # thermal diffusivity, larger means more stiff\nβ, γ = 10000, π;                # source term coefficients\nΔt = 1000;                      # timestep size\nN_t = 10;                       # number of timesteps to take\nFT = Float64;                   # float type\nΔz = FT(b - a) / FT(n)\nΔz² = Δz^2;\n∇²_op = [1 / Δz², -2 / Δz², 1 / Δz²]; # interior Laplacian operator\n∇T_bottom = 10;                 # Temperature gradient at the top\nT_top = 1;                      # Temperature at the bottom\nS(z) = β * sin(γ * z)               # source term, (sin for easy integration)\nzf = range(a, b, length = n + 1);   # coordinates on cell faces","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Derivation-of-analytic-solution","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Derivation of analytic solution","text":"Here, we'll derive the analytic solution:\n\nbeginalign*\nfrac²T²z = -fracS(z)α = -fracβ sin(γ z)α \nfracTz = fracβ cos(γ z)γ α+c_1 \nT(z) = fracβ sin(γ z)γ^2 α+c_1 z+c_2 qquad text(generic solution)\nendalign*\n\nApply bottom boundary condition:\n\nbeginalign*\nfracTz(a) = fracβ cos(γ a)γ α + c_1 = T_bottom \nc_1 = T_textbottom - fracβ cos(γ a)γ α\nendalign*\n\nApply top boundary condition:\n\nbeginalign*\nT(b) = fracβ sin(γ b)γ^2 α + c_1 b+c_2 = T_texttop \nc_2 = T_texttop - left(fracβ sin(γ b)γ^2 α+c_1 bright)\nendalign*\n\nAnd now let's define this in a Julia function:\n\nfunction T_analytic(z) # Analytic steady state solution\n    c1 = ∇T_bottom - β * cos(γ * a) / (γ * α)\n    c2 = T_top - (β * sin(γ * b) / (γ^2 * α) + c1 * b)\n    return β * sin(γ * z) / (γ^2 * α) + c1 * z + c2\nend","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Derive-the-temporal-discretization","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Derive the temporal discretization","text":"Here, we'll derive the matrix form of the temporal discretization we wish to use (diffusion-implicit and explicit Euler):\n\nbeginalign*\n_t T = α ²T + S \n(T^n+1-T^n) = Δt (α  ²T^n+1 + S) \n(T^n+1 - Δt α ²T^n+1) = T^n + Δt S \n(I - Δt α ²) T^n+1 = T^n + Δt S\nendalign*\n\nNote that, since the ² reaches to boundary points, we'll need to modify the stencils to account for boundary conditions.","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Derive-the-finite-difference-stencil","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Derive the finite difference stencil","text":"For the interior domain, a central and second-order finite difference stencil is simply:\n\nbeginalign*\n²f = fracf_i-1 -2f_i + f_i+1Δz² qquad textor \n² = leftfrac1Δz² frac-2Δz² frac1Δz²right \nendalign*\n\nAt the boundaries, we need to modify the stencil to account for Dirichlet and Neumann BCs. Using the following index denotation:\n\ni first interior index\nb boundary index\ng ghost index\n\nthe Dirichlet boundary stencil & source:\n\nbeginalign*\n_t T = α fracTi-1+Tb-2 TiΔz² + S \n_t T = α fracTi-1-2 TiΔz² + S + α fracTbΔz²\nendalign*\n\nand Neumann boundary stencil & source:\n\nbeginalign*\nT_textbottom n̂ = fracTg - Ti2Δz qquad    n̂ = -11  z_textminz_textmax \nTi + 2 Δz T_textbottom n̂ = Tg \n_t T = α fracfrac(Ti + 2 Δz T_textbottom n̂) - TbΔz - fracTb - TiΔzΔz + S \n_t T = α fracfracTi - TbΔz - fracTb - TiΔzΔz + S + α 2 Δz fracT_textbottomΔz² \n_t T = α frac2 Ti - 2 TbΔz² + S + 2α fracT_textbottom n̂Δz\nendalign*","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Define-the-discrete-diffusion-operator","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Define the discrete diffusion operator","text":"# Initialize interior and boundary stencils:\n∇² = LinearAlgebra.Tridiagonal(ones(FT, n) .* ∇²_op[1],\n    ones(FT, n + 1) .* ∇²_op[2],\n    ones(FT, n) .* ∇²_op[3]);\n\n# Modify boundary stencil to account for BCs\n\n∇².d[1] = -2 / Δz²\n∇².du[1] = +2 / Δz²\n\n# Modify boundary stencil to account for BCs\n∇².du[n] = 0  # modified stencil\n∇².d[n + 1] = 0 # to ensure `∂_t T = 0` at `z=zmax`\n∇².dl[n] = 0  # to ensure `∂_t T = 0` at `z=zmax`\nD = α .* ∇²","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Define-boundary-source","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Define boundary source","text":"Here, we'll compute the boundary source left(fracα TbΔz²right)\n\nAT_b = zeros(FT, n + 1);\nAT_b[1] = α * 2 / Δz * ∇T_bottom * n̂_min;\nAT_b[end - 1] = α * T_top / Δz²;","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Set-initial-condition","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Set initial condition","text":"Let's just initialize the solution to 1, and also set the top boundary condition:\n\nT = zeros(FT, n + 1);\nT .= 1;\nT[n + 1] = T_top; # set top BC","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Define-right-hand-side-sources","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Define right-hand side sources","text":"Here, we define the right-hand side (RHS) sources:\n\nfunction rhs!(dT, T, params, t)\n    n = params.n\n    i = 1:n # interior domain\n    dT[i] .= S.(zf[i]) .+ AT_b[i]\n    return dT\nend;\n\nNext, we'll package up parameters needed in the RHS function, define the ODE problem, and solve.\n\nparams = (; n)\n\ntspan = (FT(0), N_t * FT(Δt))\n\nprob = SciMLBase.SplitODEProblem(SciMLOperators.MatrixOperator(D),\n    rhs!,\n    T,\n    tspan,\n    params)\nalg = BDF.IMEXEuler()\nprintln(\"Solving...\")\nsol = SciMLBase.solve(prob,\n    alg,\n    dt = Δt,\n    saveat = range(FT(0), N_t * FT(Δt), length = 5),\n    progress = true,\n    progress_message = (dt, u, p, t) -> t);","category":"section"},{"location":"examples/diffusion_implicit_heat_equation/#Visualizing-results","page":"Solving the heat equation with diffusion-implicit time-stepping","title":"Visualizing results","text":"Now, let's visualize the results of the solution and error:\n\nT_end = sol.u[end]\n\np1 = Plots.plot(zf, T_analytic.(zf), label = \"analytic\", markershape = :circle,\n    markersize = 6)\np1 = Plots.plot!(p1, zf, T_end, label = \"numerical\", markershape = :diamond)\np1 = Plots.plot!(p1, title = \"T ∈ cell faces\")\n\np2 = Plots.plot(zf, abs.(T_end .- T_analytic.(zf)), label = \"error\", markershape = :circle,\n    markersize = 6)\np2 = Plots.plot!(p2, title = \"T ∈ cell faces\")\n\nPlots.plot(p1, p2)","category":"section"},{"location":"getting_started/#ode_example","page":"Getting Started with Differential Equations in Julia","title":"Getting Started with Differential Equations in Julia","text":"This tutorial will introduce you to the functionality for solving ODEs. Additionally, a video tutorial walks through this material.","category":"section"},{"location":"getting_started/#Example-1-:-Solving-Scalar-Equations","page":"Getting Started with Differential Equations in Julia","title":"Example 1 : Solving Scalar Equations","text":"In this example, we will solve the equation\n\nfracdudt = f(upt)\n\non the time interval tin01 where f(upt)=αu. Here, u is the current state variable, p is our parameter variable (containing things like a reaction rate or the constant of gravity), and t is the current time.\n\n(In our example, we know by calculus that the solution to this equation is u(t)=u₀exp(αt), but we will use DifferentialEquations.jl to solve this problem numerically, which is essential for problems where a symbolic solution is not known.)\n\nThe general workflow is to define a problem, solve the problem, and then analyze the solution. The full code for solving this problem is:\n\nimport DifferentialEquations as DE\nf(u, p, t) = 1.01 * u\nu0 = 1 / 2\ntspan = (0.0, 1.0)\nprob = DE.ODEProblem(f, u0, tspan)\nsol = DE.solve(prob, DE.Tsit5(), reltol = 1e-8, abstol = 1e-8)\n\nimport Plots\nPlots.plot(sol, linewidth = 5, title = \"Solution to the linear ODE with a thick line\",\n    xaxis = \"Time (t)\", yaxis = \"u(t) (in μm)\", label = \"My Thick Line!\") # legend=false\nPlots.plot!(sol.t, t -> 0.5 * exp(1.01t), lw = 3, ls = :dash, label = \"True Solution!\")\n\nwhere the pieces are described below.","category":"section"},{"location":"getting_started/#Step-1:-Defining-a-Problem","page":"Getting Started with Differential Equations in Julia","title":"Step 1: Defining a Problem","text":"To solve this numerically, we define a problem type by giving it the equation, the initial condition, and the timespan to solve over:\n\nimport DifferentialEquations as DE\nf(u, p, t) = 1.01 * u\nu0 = 1 / 2\ntspan = (0.0, 1.0)\nprob = DE.ODEProblem(f, u0, tspan)\n\nNote that DifferentialEquations.jl will choose the types for the problem based on the types used to define the problem type. For our example, notice that u0 is a Float64, and therefore this will solve with the dependent variables being Float64. Since tspan = (0.0,1.0) is a tuple of Float64's, the independent variables will be solved using Float64's (note that the start time and end time must match types). You can use this to choose to solve with arbitrary precision numbers, unitful numbers, etc. Please see the notebook tutorials for more examples.\n\nThe problem types include many other features, including the ability to define mass matrices and hold callbacks for events. Each problem type has a page which details its constructor and the available fields. For ODEs, the appropriate page is here. In addition, a user can specify additional functions to be associated with the function in order to speed up the solvers. These are detailed at the performance overloads page.","category":"section"},{"location":"getting_started/#Step-2:-Solving-a-Problem","page":"Getting Started with Differential Equations in Julia","title":"Step 2: Solving a Problem","text":"","category":"section"},{"location":"getting_started/#Controlling-the-Solvers","page":"Getting Started with Differential Equations in Julia","title":"Controlling the Solvers","text":"After defining a problem, you solve it using solve.\n\nsol = DE.solve(prob)\n\nThis gives us an object sol which contains the solution. Looking at the solution object:\n\ntypeof(sol)\n\nThe solution object contains the time points and corresponding solution values:\n\n@info \"Solution contains $(length(sol.t)) time points from t=$(sol.t[1]) to t=$(sol.t[end])\"\n\nThe solvers can be controlled using the available options are described on the Common Solver Options manual page. For example, we can lower the relative tolerance (in order to get a more correct result, at the cost of more timesteps) by using the command reltol:\n\nsol = DE.solve(prob, reltol = 1e-6);\nnothing # hide\n\nThere are many controls for handling outputs. For example, we can choose to have the solver save every 0.1 time points by setting saveat=0.1. Chaining this with the tolerance choice looks like:\n\nsol = DE.solve(prob, reltol = 1e-6, saveat = 0.1);\nnothing # hide\n\nMore generally, saveat can be any collection of time points to save at. Note that this uses interpolations to keep the timestep unconstrained to speed up the solution. In addition, if we only care about the endpoint, we can turn off intermediate saving in general:\n\nsol = DE.solve(prob, reltol = 1e-6, save_everystep = false);\nnothing # hide\n\nwhich will only save the final time point.","category":"section"},{"location":"getting_started/#Choosing-a-Solver-Algorithm","page":"Getting Started with Differential Equations in Julia","title":"Choosing a Solver Algorithm","text":"DifferentialEquations.jl has a method for choosing the default solver algorithm, which will find an efficient method to solve your problem. To help users receive the right algorithm, DifferentialEquations.jl offers a method for choosing algorithms through hints. This default chooser utilizes the precision of the number types and the keyword arguments (such as the tolerances) to select an algorithm. Additionally, one can provide alg_hints to help choose good defaults using properties of the problem and necessary features for the solution. For example, if we have a stiff problem where we need high accuracy, but don't know the best stiff algorithm for this problem, we can use:\n\nsol = DE.solve(prob, alg_hints = [:stiff], reltol = 1e-8, abstol = 1e-8);\nnothing # hide\n\nYou can also explicitly choose the algorithm to use. DifferentialEquations.jl offers a much wider variety of solver algorithms than traditional differential equations libraries. Many of these algorithms are from recent research and have been shown to be more efficient than the “standard” algorithms. For example, we can choose a 5th order Tsitouras method:\n\nsol = DE.solve(prob, DE.Tsit5());\nnothing # hide\n\nNote that the solver controls can be combined with the algorithm choice. Thus we can for example solve the problem using DE.Tsit5() with a lower tolerance via:\n\nsol = DE.solve(prob, DE.Tsit5(), reltol = 1e-8, abstol = 1e-8);\nnothing # hide\n\nIn DifferentialEquations.jl, some good “go-to” choices for ODEs are:\n\nAutoTsit5(Rosenbrock23()) handles both stiff and non-stiff equations. This is a good algorithm to use if you know nothing about the equation.\nAutoVern7(Rodas5()) handles both stiff and non-stiff equations in a way that's efficient for high accuracy.\nDE.Tsit5() for standard non-stiff. This is the first algorithm to try in most cases.\nBS3() for fast low accuracy non-stiff.\nVern7() for high accuracy non-stiff.\nRodas4() or Rodas5() for small stiff equations with Julia-defined types, events, etc.\nKenCarp4() or TRBDF2() for medium-sized (100-2000 ODEs) stiff equations\nRadauIIA5() for really high accuracy stiff equations\nQNDF() for large stiff equations\n\nFor a comprehensive list of the available algorithms and detailed recommendations, please see the solver documentation. Every problem type has an associated page detailing all the solvers associated with the problem.","category":"section"},{"location":"getting_started/#Step-3:-Analyzing-the-Solution","page":"Getting Started with Differential Equations in Julia","title":"Step 3: Analyzing the Solution","text":"","category":"section"},{"location":"getting_started/#Handling-the-Solution-Type","page":"Getting Started with Differential Equations in Julia","title":"Handling the Solution Type","text":"The result of solve is a solution object. We can access the 5th value of the solution with:\n\nsol[5]\n\nor get the time of the 8th timestep by:\n\nsol.t[8]\n\nConvenience features are also included. We can build an array using a comprehension over the solution tuples via:\n\n[t + u for (u, t) in DE.tuples(sol)]\n\nor more generally\n\n[t + 2u for (u, t) in zip(sol.u, sol.t)]\n\nallows one to use more parts of the solution type. The object that is returned by default acts as a continuous solution via an interpolation. We can access the interpolated values by treating sol as a function, for example:\n\nsol(0.45) # The value of the solution at t=0.45\n\nNote the difference between these: indexing with [i] is the value at the ith step, while (t) is an interpolation at time t!\n\nIf in the solver dense=true (this is the default unless saveat is used), then this interpolation is a high order interpolation and thus usually matches the error of the solution time points. The interpolations associated with each solver is detailed at the solver algorithm page. If dense=false (unless specifically set, this only occurs when save_everystep=false or saveat is used) then this defaults to giving a linear interpolation.\n\nFor more details on handling the output, see the solution handling page.","category":"section"},{"location":"getting_started/#Plotting-Solutions","page":"Getting Started with Differential Equations in Julia","title":"Plotting Solutions","text":"While one can directly plot solution time points using the tools given above, convenience commands are defined by recipes for Plots.jl. To plot the solution object, simply call plot:\n\n#]add Plots # You need to install Plots.jl before your first time using it!\nimport Plots\n#plotly() # You can optionally choose a plotting backend\nPlots.plot(sol)\n\nThe plot function can be formatted using the attributes available in Plots.jl. Additional DiffEq-specific controls are documented at the plotting page.\n\nFor example, from the Plots.jl attribute page, we see that the line width can be set via the argument linewidth. Additionally, a title can be set with title. Thus we add these to our plot command to get the correct output, fix up some axis labels, and change the legend (note we can disable the legend with legend=false) to get a nice-looking plot:\n\nPlots.plot(sol, linewidth = 5, title = \"Solution to the linear ODE with a thick line\",\n    xaxis = \"Time (t)\", yaxis = \"u(t) (in μm)\", label = \"My Thick Line!\") # legend=false\n\nWe can then add to the plot using the plot! command:\n\nPlots.plot!(sol.t, t -> 0.5 * exp(1.01t), lw = 3, ls = :dash, label = \"True Solution!\")","category":"section"},{"location":"getting_started/#Example-2:-Solving-Systems-of-Equations","page":"Getting Started with Differential Equations in Julia","title":"Example 2: Solving Systems of Equations","text":"In this example, we will solve the Lorenz equations:\n\nbeginaligned\nfracdxdt = σ(y-x) \nfracdydt = x(ρ-z) - y \nfracdzdt = xy - βz \nendaligned\n\nDefining your ODE function to be in-place updating can have performance benefits. What this means is that, instead of writing a function which outputs its solution, you write a function which updates a vector that is designated to hold the solution. By doing this, DifferentialEquations.jl's solver packages are able to reduce the amount of array allocations and achieve better performance.\n\nThe way we do this is we simply write the output to the 1st input of the function. For example, our Lorenz equation problem would be defined by the function:\n\nfunction lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\nand then we can use this function in a problem:\n\nimport DifferentialEquations as DE\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 100.0)\nprob = DE.ODEProblem(lorenz!, u0, tspan)\nsol = DE.solve(prob)\n@info \"Solution has $(length(sol.t)) timesteps\" # hide\nnothing # hide\n\nUsing the plot recipe tools defined on the plotting page, we can choose to do a 3D phase space plot between the different variables:\n\nimport Plots\nPlots.plot(sol, idxs = (1, 2, 3))\n\nNote that the default plot for multidimensional systems is an overlay of each timeseries. We can plot the timeseries of just the second component using the variable choices interface once more:\n\nPlots.plot(sol, idxs = (0, 2))\n\nNote that \"variable 0\" corresponds to the independent variable (\"time\").","category":"section"},{"location":"getting_started/#Defining-Parameterized-Functions","page":"Getting Started with Differential Equations in Julia","title":"Defining Parameterized Functions","text":"Often, you want to explicitly have parameters associated with your differential equations. This can be used by things like parameter estimation routines. In this case, you use the p values via the syntax:\n\nfunction parameterized_lorenz!(du, u, p, t)\n    du[1] = p[1] * (u[2] - u[1])\n    du[2] = u[1] * (p[2] - u[3]) - u[2]\n    du[3] = u[1] * u[2] - p[3] * u[3]\nend\n\nand then we add the parameters to the ODEProblem:\n\nu0 = [1.0, 0.0, 0.0]\ntspan = (0.0, 1.0)\np = [10.0, 28.0, 8 / 3]\nprob = DE.ODEProblem(parameterized_lorenz!, u0, tspan, p)\n\nWe can make our functions look nicer by doing a few tricks. For example:\n\nfunction parameterized_lorenz!(du, u, p, t)\n    x, y, z = u\n    σ, ρ, β = p\n    du[1] = dx = σ * (y - x)\n    du[2] = dy = x * (ρ - z) - y\n    du[3] = dz = x * y - β * z\nend\n\nNote that the type for the parameters p can be anything: you can use arrays, static arrays, named tuples, etc. to enclose your parameters in a way that is sensible for your problem.\n\nSince the parameters exist within the function, functions defined in this manner can also be used for sensitivity analysis, parameter estimation routines, and bifurcation plotting. This makes DifferentialEquations.jl a full-stop solution for differential equation analysis which also achieves high performance.","category":"section"},{"location":"getting_started/#Example-3:-Solving-Nonhomogeneous-Equations-using-Parameterized-Functions","page":"Getting Started with Differential Equations in Julia","title":"Example 3: Solving Nonhomogeneous Equations using Parameterized Functions","text":"Parameterized functions can also be used for building nonhomogeneous ordinary differential equations (these are also referred to as ODEs with nonzero right-hand sides). They are frequently used as models for dynamical systems with external (in general time-varying) inputs. As an example, consider a model of a pendulum consisting of a slender rod of length l and mass m:\n\nbeginaligned\nfracmathrmdtheta(t)mathrmdt = omega(t)\nfracmathrmdomega(t)mathrmdt = - frac32fracglsintheta(t) + frac3ml^2M(t)\nendaligned\n\nwhere θ and ω are the angular deviation of the pendulum from the vertical (hanging) orientation and the angular rate, respectively, M is an external torque (developed, say, by a wind or a motor), and finally, g stands for gravitational acceleration.\n\nimport DifferentialEquations as DE\nimport Plots\n\nl = 1.0                             # length [m]\nm = 1.0                             # mass [kg]\ng = 9.81                            # gravitational acceleration [m/s²]\n\nfunction pendulum!(du, u, p, t)\n    du[1] = u[2]                    # θ'(t) = ω(t)\n    du[2] = -3g / (2l) * sin(u[1]) + 3 / (m * l^2) * p(t) # ω'(t) = -3g/(2l) sin θ(t) + 3/(ml^2)M(t)\nend\n\nθ₀ = 0.01                           # initial angular deflection [rad]\nω₀ = 0.0                            # initial angular velocity [rad/s]\nu₀ = [θ₀, ω₀]                       # initial state vector\ntspan = (0.0, 10.0)                  # time interval\n\nM = t -> 0.1sin(t)                    # external torque [Nm]\n\nprob = DE.ODEProblem(pendulum!, u₀, tspan, M)\nsol = DE.solve(prob)\n\nPlots.plot(\n    sol, linewidth = 2, xaxis = \"t\", label = [\"θ [rad]\" \"ω [rad/s]\"], layout = (2, 1))\n\nNote how the external time-varying torque M is introduced as a parameter in the pendulum! function. Indeed, as a general principle the parameters can be any type; here we specify M as time-varying by representing it by a function, which is expressed by appending the dependence on time (t) to the name of the parameter.\n\nNote also that, in contrast with the time-varying parameter, the (vector of) state variables u, which is generally also time-varying, is always used without the explicit dependence on time (t).","category":"section"},{"location":"getting_started/#ode_other_types","page":"Getting Started with Differential Equations in Julia","title":"Example 4: Using Other Types for Systems of Equations","text":"DifferentialEquations.jl can handle many different dependent variable types (generally, anything with a linear index should work!). So instead of solving a vector equation, let's let u be a matrix! To do this, we simply need to have u0 be a matrix, and define f such that it takes in a matrix and outputs a matrix. We can define a matrix of linear ODEs as follows:\n\nimport DifferentialEquations as DE\nimport Plots\nA = [1.0 0 0 -5\n     4 -2 4 -3\n     -4 0 0 1\n     5 -2 2 3]\nu0 = rand(4, 2)\ntspan = (0.0, 1.0)\nf(u, p, t) = A * u\nprob = DE.ODEProblem(f, u0, tspan)\n\nHere our ODE is on a 4×2 matrix, and the ODE is the linear system defined by multiplication by A. To solve the ODE, we do the same steps as before.\n\nsol = DE.solve(prob)\nPlots.plot(sol)\n\nWe can instead use the in-place form by using Julia's in-place matrix multiplication function mul!:\n\nimport LinearAlgebra\nf(du, u, p, t) = LinearAlgebra.mul!(du, A, u)\n\nAdditionally, we can use non-traditional array types as well. For example, StaticArrays.jl offers immutable arrays which are stack-allocated, meaning that their usage does not require any (slow) heap-allocations that arrays normally have. This means that they can be used to solve the same problem as above, with the only change being the type for the initial condition and constants:\n\nimport StaticArrays\nA = StaticArrays.@SMatrix [1.0 0.0 0.0 -5.0\n                           4.0 -2.0 4.0 -3.0\n                           -4.0 0.0 0.0 1.0\n                           5.0 -2.0 2.0 3.0]\nu0 = StaticArrays.@SMatrix rand(4, 2)\ntspan = (0.0, 1.0)\nf2(u, p, t) = A * u\nprob = DE.ODEProblem(f2, u0, tspan)\nsol = DE.solve(prob)\nPlots.plot(sol)\n\nNote that the analysis tools generalize over to systems of equations as well.\n\nsol[4]\n\nstill returns the solution at the fourth timestep. It also indexes into the array as well. The last value is the timestep, and the beginning values are for the component. This means\n\nsol[5, 3]\n\nis the value of the 5th component (by linear indexing) at the 3rd timepoint, or\n\nsol[2, 1, :]\n\nis the timeseries for the component, which is the 2nd row and 1 column.","category":"section"},{"location":"getting_started/#Going-Beyond-ODEs:-How-to-Use-the-Documentation","page":"Getting Started with Differential Equations in Julia","title":"Going Beyond ODEs: How to Use the Documentation","text":"Not everything can be covered in the tutorials. Instead, this tutorial will end by pointing you in the directions for the next steps.","category":"section"},{"location":"getting_started/#Common-API-for-Defining,-Solving,-and-Plotting","page":"Getting Started with Differential Equations in Julia","title":"Common API for Defining, Solving, and Plotting","text":"One feature of DifferentialEquations.jl is that this pattern for solving equations is conserved across the different types of differential equations. Every equation has a problem type, a solution type, and the same solution handling (+ plotting) setup. Thus the solver and plotting commands in the Basics section applies to all sorts of equations, like stochastic differential equations and delay differential equations. Each of these different problem types are defined in the Problem Types section of the docs. Every associated solver algorithm is detailed in the Solver Algorithms section, sorted by problem type. The same steps for ODEs can then be used for the analysis of the solution.","category":"section"},{"location":"getting_started/#Additional-Features-and-Analysis-Tools","page":"Getting Started with Differential Equations in Julia","title":"Additional Features and Analysis Tools","text":"In many cases, the common workflow only starts with solving the differential equation. Many common setups have built-in solutions in DifferentialEquations.jl. For example, check out the features for:\n\nHandling, parallelizing, and analyzing large Ensemble experiments\nSaving the output to tabular formats like DataFrames and CSVs\nEvent handling\nParameter estimation (inverse problems)\nQuantification of numerical uncertainty and error\n\nMany more are defined in the relevant sections of the docs. Please explore the rest of the documentation, including tutorials for getting started with other types of equations. In addition, to get help, please either file an issue at the main repository or come have an informal discussion at the Julia Zulip chatroom.","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner","text":"Verner methods are high-order explicit Runge-Kutta methods designed for high-accuracy integration of non-stiff differential equations. These are the preferred solvers when very low tolerances are required.","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#Key-Properties","page":"OrdinaryDiffEqVerner","title":"Key Properties","text":"Verner methods provide:\n\nHigh-order accuracy (6th through 9th order) for precise integration\nExcellent efficiency at low tolerances (1e-8 to 1e-15)\nRobust error estimation with embedded error control\nDense output capability with high-quality interpolation","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#When-to-Use-Verner-Methods","page":"OrdinaryDiffEqVerner","title":"When to Use Verner Methods","text":"Verner methods are recommended for:\n\nHigh-accuracy requirements with tolerances between 1e-8 and 1e-15\nSmooth non-stiff problems where high precision is critical\nLong-time integration where error accumulation must be minimized\nProblems requiring dense output with high interpolation accuracy\nOrbit computation, molecular dynamics, and other precision-critical applications","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#Solver-Selection-Guide","page":"OrdinaryDiffEqVerner","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#Medium-low-tolerance-(1e-6-to-1e-8)","page":"OrdinaryDiffEqVerner","title":"Medium-low tolerance (1e-6 to 1e-8)","text":"Vern6: Sixth-order method, good balance of efficiency and accuracy\nAutoVern6: Automatic switching version for mixed stiffness","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#Low-tolerance-(1e-8-to-1e-12)-with-Float64","page":"OrdinaryDiffEqVerner","title":"Low tolerance (1e-8 to 1e-12) with Float64","text":"Vern7: Seventh-order method, excellent for most high-precision needs\nVern8: Eighth-order method, best efficiency at very low tolerances\nAutoVern7, AutoVern8: Automatic switching versions","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#Very-low-tolerance-(1e-12)","page":"OrdinaryDiffEqVerner","title":"Very low tolerance (<1e-12)","text":"Vern9: Ninth-order method for extreme precision requirements\nRecommended with BigFloat for tolerances below 1e-15\nAutoVern9: Automatic switching version for mixed problems","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#Performance-Notes","page":"OrdinaryDiffEqVerner","title":"Performance Notes","text":"Vern6: Most efficient for tolerances around 1e-6 to 1e-8\nVern7: Sweet spot for tolerances around 1e-8 to 1e-10\nVern8: Best for tolerances around 1e-10 to 1e-12\nVern9: For tolerances below 1e-12, especially with arbitrary precision\n\nThe Auto* variants automatically switch to stiff solvers when stiffness is detected, making them robust for problems of unknown character.\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqVerner\", \"Vern6\")","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#Full-list-of-solvers","page":"OrdinaryDiffEqVerner","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.Vern6","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.Vern6","text":"Vern6(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False(),\n        lazy = true)\n\nExplicit Runge-Kutta Method.  Verner's most efficient 6/5 method (lazy 6th order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nlazy: determines if the lazy interpolant is used.\n\nReferences\n\n@article{verner2010numerically,     title={Numerically optimal Runge–Kutta pairs with interpolants},     author={Verner, James H},     journal={Numerical Algorithms},     volume={53},     number={2-3},     pages={383–396},     year={2010},     publisher={Springer}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.Vern7","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.Vern7","text":"Vern7(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False(),\n        lazy = true)\n\nExplicit Runge-Kutta Method.  Verner's most efficient 7/6 method (lazy 7th order interpolant). Good for problems requiring high accuracy. Slightly more computationally expensive than Tsit5. Performance best when parameter vector remains unchanged. Recommended for high-accuracy non-stiff problems.\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nlazy: determines if the lazy interpolant is used.\n\nReferences\n\n@article{verner2010numerically,     title={Numerically optimal Runge–Kutta pairs with interpolants},     author={Verner, James H},     journal={Numerical Algorithms},     volume={53},     number={2-3},     pages={383–396},     year={2010},     publisher={Springer}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.Vern8","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.Vern8","text":"Vern8(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False(),\n        lazy = true)\n\nExplicit Runge-Kutta Method.  Verner's most efficient 8/7 method (lazy 8th order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nlazy: determines if the lazy interpolant is used.\n\nReferences\n\n@article{verner2010numerically,     title={Numerically optimal Runge–Kutta pairs with interpolants},     author={Verner, James H},     journal={Numerical Algorithms},     volume={53},     number={2-3},     pages={383–396},     year={2010},     publisher={Springer}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.Vern9","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.Vern9","text":"Vern9(; stage_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        step_limiter! = OrdinaryDiffEq.trivial_limiter!,\n        thread = OrdinaryDiffEq.False(),\n        lazy = true)\n\nExplicit Runge-Kutta Method.  Verner's most efficient 9/8 method (lazy 9th order interpolant).\n\nKeyword Arguments\n\nstage_limiter!: function of the form limiter!(u, integrator, p, t)\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nthread: determines whether internal broadcasting on appropriate CPU arrays should be serial (thread = OrdinaryDiffEq.False()) or use multiple threads (thread = OrdinaryDiffEq.True()) when Julia is started with multiple threads.\nlazy: determines if the lazy interpolant is used.\n\nReferences\n\n@article{verner2010numerically,     title={Numerically optimal Runge–Kutta pairs with interpolants},     author={Verner, James H},     journal={Numerical Algorithms},     volume={53},     number={2-3},     pages={383–396},     year={2010},     publisher={Springer}     }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.AutoVern6","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.AutoVern6","text":"Automatic switching algorithm that can switch between the (non-stiff) Vern6() and stiff_alg.\n\nAutoVern6(stiff_alg; kwargs...)\n\nThis method is equivalent to AutoAlgSwitch(Vern6(), stiff_alg; kwargs...). To gain access to stiff algorithms you might have to install additional libraries, such as OrdinaryDiffEqRosenbrock.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.AutoVern7","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.AutoVern7","text":"Automatic switching algorithm that can switch between the (non-stiff) Vern7() and stiff_alg.\n\nAutoVern7(stiff_alg; kwargs...)\n\nThis method is equivalent to AutoAlgSwitch(Vern7(), stiff_alg; kwargs...). To gain access to stiff algorithms you might have to install additional libraries, such as OrdinaryDiffEqRosenbrock.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.AutoVern8","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.AutoVern8","text":"Automatic switching algorithm that can switch between the (non-stiff) Vern8() and stiff_alg.\n\nAutoVern8(stiff_alg; kwargs...)\n\nThis method is equivalent to AutoAlgSwitch(Vern8(), stiff_alg; kwargs...). To gain access to stiff algorithms you might have to install additional libraries, such as OrdinaryDiffEqRosenbrock.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/explicit/Verner/#OrdinaryDiffEqVerner.AutoVern9","page":"OrdinaryDiffEqVerner","title":"OrdinaryDiffEqVerner.AutoVern9","text":"Automatic switching algorithm that can switch between the (non-stiff) Vern9() and stiff_alg.\n\nAutoVern9(stiff_alg; kwargs...)\n\nThis method is equivalent to AutoAlgSwitch(Vern9(), stiff_alg; kwargs...). To gain access to stiff algorithms you might have to install additional libraries, such as OrdinaryDiffEqRosenbrock.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/misc/#Miscellaneous-Solvers","page":"Miscellaneous Solvers","title":"Miscellaneous Solvers","text":"These are solvers that do not fall clearly into any of the major categories.","category":"section"},{"location":"api/ordinarydiffeq/misc/#OrdinaryDiffEqLowOrderRK.SplitEuler","page":"Miscellaneous Solvers","title":"OrdinaryDiffEqLowOrderRK.SplitEuler","text":"SplitEuler()\n\nSplit Method. 1st order fully explicit method for testing split accuracy\n\nKeyword Arguments\n\nReferences\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/misc/#OrdinaryDiffEqExplicitRK.ExplicitRK","page":"Miscellaneous Solvers","title":"OrdinaryDiffEqExplicitRK.ExplicitRK","text":"ExplicitRK(; tableau = ODE_DEFAULT_TABLEAU)\n\nA generic explicit Runge-Kutta method that allows you to define a custom tableau. The default tableau is Dormand-Prince 4/5. This solver is primarily for research purposes or when you need a specific tableau not already implemented.\n\nParameters\n\ntableau: A DiffEqBase.ExplicitRKTableau object defining the Runge-Kutta tableau.\n\nFor most applications, prefer the named methods like DP5(), Tsit5(), etc.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN","text":"Runge-Kutta-Nyström (RKN) methods for solving second-order differential equations of the form d²u/dt² = f(u, du/dt, t). These methods are specifically designed for second-order ODEs and can be more efficient than converting to first-order systems when the problem has this natural structure.","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Key-Properties","page":"OrdinaryDiffEqRKN","title":"Key Properties","text":"RKN methods provide:\n\nDirect integration of second-order ODEs without conversion to first-order systems\nEfficient handling of problems where velocity dependence is minimal or absent\nSpecialized variants for different types of second-order problems\nHigh-order accuracy with methods up to 12th order available\nTrigonometrically-fitted variants for oscillatory problems\nImproved efficiency for second-order problems compared to first-order conversions","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#When-to-Use-RKN-Methods","page":"OrdinaryDiffEqRKN","title":"When to Use RKN Methods","text":"These methods are recommended for:\n\nSecond-order differential equations with natural d²u/dt² = f(u, du/dt, t) structure\nClassical mechanics problems (Newton's equations of motion)\nOscillatory second-order systems (springs, pendulums, wave equations)\nProblems where velocity dependence is weak or absent\nCelestial mechanics and orbital dynamics\nVibration analysis and structural dynamics\nWave propagation problems in their natural second-order form","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Problem-Types","page":"OrdinaryDiffEqRKN","title":"Problem Types","text":"","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Velocity-independent-problems:-du/dt-f(u,-t)","page":"OrdinaryDiffEqRKN","title":"Velocity-independent problems: d²u/dt² = f(u, t)","text":"When the acceleration depends only on position (and possibly time):\n\nMore efficient specialized methods available\nClassical examples: gravitational problems, springs with F = -kx","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Velocity-dependent-problems:-du/dt-f(u,-du/dt,-t)","page":"OrdinaryDiffEqRKN","title":"Velocity-dependent problems: d²u/dt² = f(u, du/dt, t)","text":"When acceleration depends on both position and velocity:\n\nGeneral RKN methods handle the full dependence\nExamples: damped oscillators, air resistance problems","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Solver-Selection-Guide","page":"OrdinaryDiffEqRKN","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#General-purpose-RKN-methods","page":"OrdinaryDiffEqRKN","title":"General-purpose RKN methods","text":"Nystrom4: Fourth-order method for general second-order ODEs\nIRKN3, IRKN4: Improved Runge-Kutta-Nyström methods","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Velocity-independent-specialized-methods","page":"OrdinaryDiffEqRKN","title":"Velocity-independent specialized methods","text":"Nystrom4VelocityIndependent: Fourth-order for d²u/dt² = f(u, t)\nNystrom5VelocityIndependent: Fifth-order for d²u/dt² = f(u, t)","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Velocity-dependent-methods","page":"OrdinaryDiffEqRKN","title":"Velocity-dependent methods","text":"FineRKN4: Fourth-order allowing velocity dependence\nFineRKN5: Fifth-order allowing velocity dependence","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#High-order-Dormand-Prince-variants","page":"OrdinaryDiffEqRKN","title":"High-order Dormand-Prince variants","text":"DPRKN4: Fourth-order Dormand-Prince RKN\nDPRKN5: Fifth-order Dormand-Prince RKN\nDPRKN6: Sixth-order with free interpolant\nDPRKN6FM: Sixth-order with optimized error coefficients\nDPRKN8: Eighth-order for high accuracy\nDPRKN12: Twelfth-order for extreme precision","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Trigonometrically-fitted-methods-for-oscillatory-problems","page":"OrdinaryDiffEqRKN","title":"Trigonometrically-fitted methods for oscillatory problems","text":"ERKN4: Embedded 4(3) pair for periodic problems\nERKN5: Embedded 5(4) pair for periodic problems\nERKN7: Higher-order embedded pair for oscillatory systems","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Specialized-methods","page":"OrdinaryDiffEqRKN","title":"Specialized methods","text":"RKN4: Fourth-order for linear inhomogeneous second-order IVPs","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Advantages-Over-First-Order-Conversion","page":"OrdinaryDiffEqRKN","title":"Advantages Over First-Order Conversion","text":"Natural problem structure: Preserves the physical meaning of the equations\nSpecialized optimizations: Methods can exploit second-order structure\n\nTo be able to access the solvers in OrdinaryDiffEqRKN, you must first install them use the Julia package manager:\n\nusing Pkg\nPkg.add(\"OrdinaryDiffEqRKN\")\n\nThis will only install the solvers listed at the bottom of this page. If you want to explore other solvers for your problem, you will need to install some of the other libraries listed in the navigation bar on the left.","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Example-usage","page":"OrdinaryDiffEqRKN","title":"Example usage","text":"using OrdinaryDiffEqRKN\nfunction HH_acceleration!(dv, v, u, p, t)\n    x, y = u\n    dx, dy = dv\n    dv[1] = -x - 2 * x * y\n    dv[2] = y^2 - y - x^2\nend\ninitial_positions = [0.0, 0.1]\ninitial_velocities = [0.5, 0.0]\ntspan = (0.0, 1.0)\nprob = SecondOrderODEProblem(HH_acceleration!, initial_velocities, initial_positions, tspan)\nsol = solve(prob, Nystrom4(), dt = 1 / 10)","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#Full-list-of-solvers","page":"OrdinaryDiffEqRKN","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.IRKN3","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.IRKN3","text":"IRKN3()\n\nImproved Runge-Kutta-Nyström method Method of order three, which minimizes the amount of evaluated functions in each step. Fixed time steps only. Second order ODE should not depend on the first derivative.\n\nKeyword Arguments\n\nReferences\n\n@article{rabiei2012numerical,     title={Numerical Solution of Second-Order Ordinary Differential Equations by Improved Runge-Kutta Nystrom Method},     author={Rabiei, Faranak and Ismail, Fudziah and Norazak, S and Emadi, Saeid},     publisher={Citeseer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.IRKN4","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.IRKN4","text":"IRKN4()\n\nImproved Runge-Kutta-Nyström method Improves Runge-Kutta-Nyström method of order four,     which minimizes the amount of evaluated functions in each step.     Fixed time steps only.     Second order ODE should not be dependent on the first derivative.     Recommended for smooth problems with expensive functions to evaluate.\n\nKeyword Arguments\n\nReferences\n\n@article{rabiei2012numerical,     title={Numerical Solution of Second-Order Ordinary Differential Equations by Improved Runge-Kutta Nystrom Method},     author={Rabiei, Faranak and Ismail, Fudziah and Norazak, S and Emadi, Saeid},     publisher={Citeseer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.Nystrom4","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.Nystrom4","text":"Nystrom4()\n\nImproved Runge-Kutta-Nyström method 4th order explicit Runge-Kutta-Nyström method. Allows acceleration to depend on velocity.\n\nKeyword Arguments\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I.     Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics,     Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.Nystrom4VelocityIndependent","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.Nystrom4VelocityIndependent","text":"Nystrom4VelocityIndependent()\n\nImproved Runge-Kutta-Nyström method A 4th order explicit method. Used directly on second order ODEs, where the acceleration is independent from velocity (ODE Problem is not dependent on the first derivative).\n\nKeyword Arguments\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I.     Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics,     Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.Nystrom5VelocityIndependent","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.Nystrom5VelocityIndependent","text":"Nystrom5VelocityIndependent()\n\nImproved Runge-Kutta-Nyström method A 5th order explicit method. Used directly on second order ODEs, where the acceleration is independent from velocity (ODE Problem is not dependent on the first derivative).\n\nKeyword Arguments\n\nReferences\n\nE. Hairer, S.P. Norsett, G. Wanner, (1993) Solving Ordinary Differential Equations I.     Nonstiff Problems. 2nd Edition. Springer Series in Computational Mathematics,     Springer-Verlag.\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.FineRKN4","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.FineRKN4","text":"FineRKN4()\n\nImproved Runge-Kutta-Nyström method A 4th order explicit method which can be applied directly to second order ODEs. In particular, this method allows the acceleration equation to depend on the velocity.\n\nKeyword Arguments\n\nReferences\n\n@article{fine1987low,     title={Low order practical {R}unge-{K}utta-{N}ystr{\"o}m methods},     author={Fine, Jerry Michael},     journal={Computing},     volume={38},     number={4},     pages={281–297},     year={1987},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.FineRKN5","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.FineRKN5","text":"FineRKN5()\n\nImproved Runge-Kutta-Nyström method A 5th order explicit method which can be applied directly to second order ODEs. In particular, this method allows the acceleration equation to depend on the velocity.\n\nKeyword Arguments\n\nReferences\n\n@article{fine1987low,     title={Low order practical {R}unge-{K}utta-{N}ystr{\"o}m methods},     author={Fine, Jerry Michael},     journal={Computing},     volume={38},     number={4},     pages={281–297},     year={1987},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.DPRKN4","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.DPRKN4","text":"DPRKN4()\n\nImproved Runge-Kutta-Nyström method 4th order explicit method.     The second order ODE should not depend on the first derivative.\n\nKeyword Arguments\n\nReferences\n\n@article{Dormand1987FamiliesOR,     title={Families of Runge-Kutta-Nystrom Formulae},     author={J. R. Dormand and Moawwad E. A. El-Mikkawy and P. J. Prince},     journal={Ima Journal of Numerical Analysis},     year={1987},     volume={7},     pages={235-250}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.DPRKN5","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.DPRKN5","text":"DPRKN5()\n\nImproved Runge-Kutta-Nyström method 5th order explicit method.     The second order ODE should not depend on the first derivative.\n\nKeyword Arguments\n\nReferences\n\n@article{Bettis1973ARN,     title={A Runge-Kutta Nystrom algorithm},     author={Dale G. Bettis},     journal={Celestial mechanics},     year={1973},     volume={8},     pages={229-233},     publisher={Springer}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.DPRKN6","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.DPRKN6","text":"DPRKN6()\n\nImproved Runge-Kutta-Nyström method 6th order explicit method. The second order ODE should not depend on the first derivative. Free 6th order interpolant\n\nKeyword Arguments\n\nReferences\n\n@article{Dormand1987FamiliesOR,     title={Families of Runge-Kutta-Nystrom Formulae},     author={J. R. Dormand and Moawwad E. A. El-Mikkawy and P. J. Prince},     journal={Ima Journal of Numerical Analysis},     year={1987},     volume={7},     pages={235-250}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.DPRKN6FM","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.DPRKN6FM","text":"DPRKN6FM()\n\nImproved Runge-Kutta-Nyström method 6th order explicit method.     The second order ODE should not depend on the first derivative.     Compared to DPRKN6, this method has smaller truncation error coefficients     which leads to performance gain when only the main solution points are considered.\n\nKeyword Arguments\n\nReferences\n\n@article{Dormand1987FamiliesOR,     title={Families of Runge-Kutta-Nystrom Formulae},     author={J. R. Dormand and Moawwad E. A. El-Mikkawy and P. J. Prince},     journal={Ima Journal of Numerical Analysis},     year={1987},     volume={7},     pages={235-250}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.DPRKN8","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.DPRKN8","text":"DPRKN8()\n\nImproved Runge-Kutta-Nyström method 8th order explicit method.     The second order ODE should not depend on the first derivative.     Not as efficient as DPRKN12 when high accuracy is needed,     however this solver is competitive with DPRKN6 at lax tolerances and,     depending on the problem, might be a good option between performance and accuracy.\n\nKeyword Arguments\n\nReferences\n\n@article{dormand1987high,     title={High-order embedded Runge-Kutta-Nystrom formulae},     author={Dormand, JR and El-Mikkawy, MEA and Prince, PJ},     journal={IMA Journal of Numerical Analysis},     volume={7},     number={4},     pages={423–430},     year={1987},     publisher={Oxford University Press}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.DPRKN12","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.DPRKN12","text":"DPRKN12()\n\nImproved Runge-Kutta-Nyström method 12th order explicit method.     The second order ODE should not depend on the first derivative.     Most efficient when high accuracy is needed.\n\nKeyword Arguments\n\nReferences\n\n@article{dormand1987high,     title={High-order embedded Runge-Kutta-Nystrom formulae},     author={Dormand, JR and El-Mikkawy, MEA and Prince, PJ},     journal={IMA Journal of Numerical Analysis},     volume={7},     number={4},     pages={423–430},     year={1987},     publisher={Oxford University Press}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.ERKN4","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.ERKN4","text":"ERKN4()\n\nImproved Runge-Kutta-Nyström method Embedded 4(3) pair of explicit methods.     Integrates the periodic properties of the harmonic oscillator exactly.     The second order ODE should not depend on the first derivative.     Uses adaptive step size control. This method is extra efficient on periodic problems.\n\nKeyword Arguments\n\nReferences\n\n@article{demba2017embedded,     title={An Embedded 4 (3) Pair of Explicit Trigonometrically-Fitted Runge-Kutta-Nystr{\"o}m Method for Solving Periodic Initial Value Problems},     author={Demba, MA and Senu, N and Ismail, F},     journal={Applied Mathematical Sciences},     volume={11},     number={17},     pages={819–838},     year={2017}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.ERKN5","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.ERKN5","text":"ERKN5()\n\nImproved Runge-Kutta-Nyström method Embedded 5(4) pair of explicit methods.     Integrates the periodic properties of the harmonic oscillator exactly.     The second order ODE should not depend on the first derivative.     Uses adaptive step size control. This method is extra efficient on periodic problems.\n\nKeyword Arguments\n\nReferences\n\n@article{demba20165,     title={A 5 (4) Embedded Pair of Explicit Trigonometrically-Fitted Runge–Kutta–Nystr{\"o}m Methods for the Numerical Solution of Oscillatory Initial Value     Problems},     author={Demba, Musa A and Senu, Norazak and Ismail, Fudziah},     journal={Mathematical and Computational Applications},     volume={21},     number={4},     pages={46},     year={2016},     publisher={Multidisciplinary Digital Publishing Institute}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.ERKN7","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.ERKN7","text":"ERKN7()\n\nImproved Runge-Kutta-Nyström method Embedded pair of explicit methods.     Integrates the periodic properties of the harmonic oscillator exactly.     The second order ODE should not depend on the first derivative.     Uses adaptive step size control. This method is extra efficient on periodic problems.\n\nKeyword Arguments\n\nReferences\n\n@article{SimosOnHO,     title={On high order Runge-Kutta-Nystr{\"o}m pairs},     author={Theodore E. Simos and Ch. Tsitouras},     journal={J. Comput. Appl. Math.},     volume={400},     pages={113753}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/dynamicalodeexplicit/RKN/#OrdinaryDiffEqRKN.RKN4","page":"OrdinaryDiffEqRKN","title":"OrdinaryDiffEqRKN.RKN4","text":"RKN4()\n\nImproved Runge-Kutta-Nyström method 3 stage fourth order method to solve second order linear inhomogeneous IVPs. Does not include an adaptive method. Solves for for d-dimensional differential systems of second order linear inhomogeneous equations.\n\nwarning: Warning\n\n\nThis method is only fourth order for these systems, the method is second order otherwise!\n\nKeyword Arguments\n\nReferences\n\n@article{MONTIJANO2024115533,     title = {Explicit Runge–Kutta–Nyström methods for the numerical solution of second order linear inhomogeneous IVPs},     author = {J.I. Montijano and L. Rández and M. Calvo},     journal = {Journal of Computational and Applied Mathematics},     volume = {438},     pages = {115533},     year = {2024},}\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Implicit-Methods-for-Stiff-SDEs","page":"Implicit Methods for Stiff SDEs","title":"Implicit Methods for Stiff SDEs","text":"When SDEs have stiff drift terms, explicit methods may require impractically small time steps for stability. Implicit methods treat the drift term implicitly while keeping the diffusion explicit, providing excellent stability properties.","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Recommended-Stiff-Methods","page":"Implicit Methods for Stiff SDEs","title":"Recommended Stiff Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#SKenCarp-Stochastic-KenCarp-(Highly-Recommended-for-Stiff-Additive-Noise)","page":"Implicit Methods for Stiff SDEs","title":"SKenCarp - Stochastic KenCarp (Highly Recommended for Stiff Additive Noise)","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Basic-Implicit-Methods","page":"Implicit Methods for Stiff SDEs","title":"Basic Implicit Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#ImplicitEM-Implicit-Euler-Maruyama","page":"Implicit Methods for Stiff SDEs","title":"ImplicitEM - Implicit Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#ImplicitEulerHeun-Implicit-Euler-Heun-(Stratonovich)","page":"Implicit Methods for Stiff SDEs","title":"ImplicitEulerHeun - Implicit Euler-Heun (Stratonovich)","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#ImplicitRKMil-Implicit-Runge-Kutta-Milstein","page":"Implicit Methods for Stiff SDEs","title":"ImplicitRKMil - Implicit Runge-Kutta Milstein","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Split-Step-Implicit-Methods","page":"Implicit Methods for Stiff SDEs","title":"Split-Step Implicit Methods","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#ISSEM-Implicit-Split-Step-Euler-Maruyama","page":"Implicit Methods for Stiff SDEs","title":"ISSEM - Implicit Split-Step Euler-Maruyama","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#ISSEulerHeun-Implicit-Split-Step-Euler-Heun","page":"Implicit Methods for Stiff SDEs","title":"ISSEulerHeun - Implicit Split-Step Euler-Heun","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Method-Selection-Guide","page":"Implicit Methods for Stiff SDEs","title":"Method Selection Guide","text":"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Problem-Classification:","page":"Implicit Methods for Stiff SDEs","title":"Problem Classification:","text":"Mildly stiff drift: ImplicitEM, ImplicitRKMil\nStiff additive noise: SKenCarp (highly recommended)\nFully stiff (including diffusion): ISSEM, ISSEulerHeun\nStratonovich problems: ImplicitEulerHeun, ISSEulerHeun","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Performance-Ranking-for-Stiff-Problems:","page":"Implicit Methods for Stiff SDEs","title":"Performance Ranking for Stiff Problems:","text":"SKenCarp - Best for stiff problems with additive noise\nISSEM/ISSEulerHeun - For fully stiff problems\nImplicitRKMil - Higher order for mildly stiff problems\nImplicitEM - Robust fallback option","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Understanding-Stiffness-in-SDEs","page":"Implicit Methods for Stiff SDEs","title":"Understanding Stiffness in SDEs","text":"Drift Stiffness: Large negative eigenvalues in the drift term f(u,t) requiring small time steps for explicit stability.\n\nDiffusion Stiffness: Large coefficients in the diffusion term g(u,t) causing stability issues.\n\nDetection: If explicit methods require very small dt or produce unstable solutions, try implicit methods.","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Configuration-Options","page":"Implicit Methods for Stiff SDEs","title":"Configuration Options","text":"All implicit methods share common configuration parameters:\n\n# Linear solver options\nImplicitEM(linsolve = KrylovJL_GMRES())\n\n# Nonlinear solver options  \nImplicitEM(nlsolve = NLNewton(max_iter = 20))\n\n# Jacobian computation\nImplicitEM(autodiff = true, chunk_size = 4)\n\n# Theta method parameter\nImplicitEM(theta = 0.5)  # Trapezoidal rule","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Symplectic-Methods","page":"Implicit Methods for Stiff SDEs","title":"Symplectic Methods","text":"For Hamiltonian SDEs, use symplectic variants:\n\nSImplicitMidpoint()  # Symplectic implicit midpoint\nSTrapezoid()        # Symplectic trapezoidal rule","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#Performance-Tips","page":"Implicit Methods for Stiff SDEs","title":"Performance Tips","text":"Jacobian: Provide analytical Jacobian when possible\nLinear solver: Choose appropriate solver for problem structure\nPreconditioning: Use preconditioners for large systems\nTheta parameter: θ=0.5 often provides good accuracy/stability balance","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#References","page":"Implicit Methods for Stiff SDEs","title":"References","text":"Standard implicit ODE methods adapted to SDEs\nMilstein, G.N., \"Numerical Integration of Stochastic Differential Equations\"","category":"section"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#StochasticDiffEq.SKenCarp","page":"Implicit Methods for Stiff SDEs","title":"StochasticDiffEq.SKenCarp","text":"SKenCarp(;chunk_size=0, autodiff=true, diff_type=Val{:central}, \n         standardtag=Val{true}(), concrete_jac=nothing, precs=DEFAULT_PRECS,\n         linsolve=nothing, nlsolve=NLNewton(), smooth_est=true, \n         extrapolant=:min_correct, new_jac_conv_bound=1e-3, \n         controller=:Predictive, ode_error_est=true)\n\nSKenCarp: Stochastic KenCarp Method (Stiff) - Highly Recommended for Stiff Problems\n\nAdaptive L-stable drift-implicit method with strong order 1.5. Highly recommended for stiff problems with additive noise.\n\nMethod Properties\n\nStrong Order: 1.5 (for additive noise)\nWeak Order: 2.0\nTime stepping: Adaptive\nNoise types: Additive noise (diagonal, non-diagonal, and scalar)\nSDE interpretation: Both Itô and Stratonovich\nStability: L-stable (excellent for stiff problems)\nImplicit: Drift-implicit (handles stiffness in drift term)\n\nWhen to Use\n\nHighly recommended for stiff additive noise problems\nWhen the drift term f(u,p,t) is stiff\nFor problems requiring high accuracy with stiff dynamics\nWhen implicit treatment of the drift is necessary for stability\nBest choice for stiff problems with additive noise structure\n\nAlgorithm Description\n\nSKenCarp applies implicit treatment to the drift term while keeping the diffusion explicit. This provides excellent stability for stiff SDEs with additive noise.\n\nStiffness and Stability\n\nL-stable: Excellent for stiff problems\nHandles large negative eigenvalues in the drift term\nMaintains accuracy while providing stability\n\nConfiguration Options\n\nLinear solver options via linsolve parameter\nNonlinear solver options via nlsolve parameter\nJacobian computation control via autodiff and related parameters\nStep size control via controller parameter\n\nReferences\n\nBased on KenCarp methods from OrdinaryDiffEq.jl\nAdapted for stochastic problems with additive noise\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#StochasticDiffEq.ImplicitEM","page":"Implicit Methods for Stiff SDEs","title":"StochasticDiffEq.ImplicitEM","text":"ImplicitEM(;chunk_size=0, autodiff=true, diff_type=Val{:central},\n           standardtag=Val{true}(), concrete_jac=nothing, precs=DEFAULT_PRECS,\n           linsolve=nothing, nlsolve=NLNewton(), extrapolant=:constant,\n           theta=1, symplectic=false, new_jac_conv_bound=1e-3, \n           controller=:Predictive)\n\nImplicitEM: Implicit Euler-Maruyama Method (Stiff)\n\nDrift-implicit version of the Euler-Maruyama method with theta-method treatment of the drift term.\n\nMethod Properties\n\nStrong Order: 0.5 (Itô sense)\nWeak Order: 1.0\nTime stepping: Adaptive (1.0/1.5 heuristic)\nNoise types: All forms (non-diagonal, scalar, colored noise)\nSDE interpretation: Itô\nImplicit treatment: Drift term only (diffusion remains explicit)\n\nParameters\n\ntheta::Real = 1: Implicitness parameter (0=explicit, 1=fully implicit, 0.5=trapezoidal)\nsymplectic::Bool = false: When true and theta=0.5, uses symplectic implicit midpoint\nLinear/nonlinear solver options via linsolve and nlsolve\n\nWhen to Use\n\nFor mildly stiff SDEs where drift term causes stability issues\nWhen explicit methods require very small time steps\nAs a robust fallback for difficult problems\nWhen all noise types need to be supported\n\nTheta Method Variants\n\ntheta = 0: Explicit Euler (not recommended, use EM instead)\ntheta = 0.5: Trapezoidal rule (second order accurate for deterministic part)\ntheta = 1: Backward Euler (default, maximum stability)\n\nSymplectic Option\n\nWhen symplectic=true and theta=0.5, the method preserves the symplectic structure in distribution for appropriate problems.\n\nAlgorithm Description\n\nTreats the SDE du = f(u,t)dt + g(u,t)dW using:\n\nu_{n+1} = u_n + theta*f(u_{n+1},t_{n+1})*dt + (1-theta)*f(u_n,t_n)*dt + g(u_n,t_n)*dW_n\n\nReferences\n\nStandard implicit methods adapted for SDEs\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#StochasticDiffEq.ImplicitEulerHeun","page":"Implicit Methods for Stiff SDEs","title":"StochasticDiffEq.ImplicitEulerHeun","text":"ImplicitEulerHeun(;chunk_size=0, autodiff=true, diff_type=Val{:central},\n                  standardtag=Val{true}(), concrete_jac=nothing, precs=DEFAULT_PRECS,\n                  linsolve=nothing, nlsolve=NLNewton(), extrapolant=:constant,\n                  theta=1, symplectic=false, new_jac_conv_bound=1e-3, \n                  controller=:Predictive)\n\nImplicitEulerHeun: Implicit Euler-Heun Method (Stiff)\n\nDrift-implicit version of the Euler-Heun method for Stratonovich SDEs with stiff drift terms.\n\nMethod Properties\n\nStrong Order: 0.5 (Stratonovich sense)\nWeak Order: 1.0\nTime stepping: Adaptive (1.0/1.5 heuristic)\nNoise types: All forms (non-diagonal, scalar, colored noise)\nSDE interpretation: Stratonovich\nImplicit treatment: Drift term only (diffusion remains explicit)\n\nParameters\n\ntheta::Real = 1: Implicitness parameter (0=explicit, 1=fully implicit, 0.5=trapezoidal)\nsymplectic::Bool = false: When true and theta=1, uses symplectic implicit midpoint\nLinear/nonlinear solver options via linsolve and nlsolve\n\nWhen to Use\n\nStiff Stratonovich SDEs where drift term causes stability issues\nWhen working in Stratonovich interpretation with stiff dynamics\nAlternative to ImplicitEM for Stratonovich problems\nWhen all noise types need to be supported in Stratonovich form\n\nTheta Method Variants\n\ntheta = 0.5: Trapezoidal rule (default, good accuracy/stability balance)\ntheta = 1: Backward Euler (maximum stability)\n\nSymplectic Option\n\nWhen symplectic=true and theta=1, preserves symplectic structure in distribution for appropriate Stratonovich problems.\n\nReferences\n\nImplicit methods for stiff SDEs in Stratonovich interpretation\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#StochasticDiffEq.ImplicitRKMil","page":"Implicit Methods for Stiff SDEs","title":"StochasticDiffEq.ImplicitRKMil","text":"ImplicitRKMil(;chunk_size=0, autodiff=true, diff_type=Val{:central},\n              standardtag=Val{true}(), concrete_jac=nothing, precs=DEFAULT_PRECS,\n              linsolve=nothing, nlsolve=NLNewton(), extrapolant=:constant,\n              theta=1, symplectic=false, new_jac_conv_bound=1e-3, \n              controller=:Predictive, interpretation=AlgorithmInterpretation.Ito)\n\nImplicitRKMil: Implicit Runge-Kutta Milstein Method (Stiff)\n\nDrift-implicit Runge-Kutta Milstein method achieving order 1.0 for stiff problems with diagonal/scalar noise.\n\nMethod Properties\n\nStrong Order: 1.0\nWeak Order: Depends on tableau\nTime stepping: Adaptive (1.5/2.0 heuristic)\nNoise types: Diagonal and scalar noise only\nSDE interpretation: Configurable (Itô or Stratonovich)\nImplicit treatment: Drift term only (diffusion remains explicit)\n\nParameters\n\ntheta::Real = 1: Implicitness parameter (0.5=trapezoidal, 1=backward Euler)\nsymplectic::Bool = false: When true and theta=0.5, uses symplectic implicit midpoint\ninterpretation: Choose AlgorithmInterpretation.Ito (default) or AlgorithmInterpretation.Stratonovich\nLinear/nonlinear solver options via linsolve and nlsolve\n\nWhen to Use\n\nStiff problems requiring higher accuracy than ImplicitEM\nWhen strong order 1.0 is needed with implicit stability\nDiagonal or scalar noise problems with stiff drift\nAlternative to SKenCarp for non-additive noise\n\nRestrictions\n\nOnly works with diagonal or scalar noise\nFor non-diagonal noise, use ISSEM/ISSEulerHeun\nFor additive noise, prefer SKenCarp\n\nAlgorithm Features\n\nHigher order accuracy than ImplicitEM\nMilstein correction for improved strong convergence\nConfigurable interpretation (Itô/Stratonovich)\n\nReferences\n\nImplicit Milstein methods for stiff SDEs\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#StochasticDiffEq.ISSEM","page":"Implicit Methods for Stiff SDEs","title":"StochasticDiffEq.ISSEM","text":"ISSEM(;chunk_size=0, autodiff=true, diff_type=Val{:central},\n      standardtag=Val{true}(), concrete_jac=nothing, precs=DEFAULT_PRECS,\n      linsolve=nothing, nlsolve=NLNewton(), extrapolant=:constant,\n      theta=1, symplectic=false, new_jac_conv_bound=1e-3, \n      controller=:Predictive)\n\nISSEM: Implicit Split-Step Euler-Maruyama Method (Stiff)\n\nFully implicit split-step method for handling stiffness in both drift and diffusion terms.\n\nMethod Properties\n\nStrong Order: 0.5 (Itô sense)\nWeak Order: 1.0\nTime stepping: Adaptive (1.0/1.5 heuristic)\nNoise types: All forms (non-diagonal, scalar, colored noise)\nSDE interpretation: Itô\nImplicit treatment: Both drift and diffusion terms (fully implicit)\n\nParameters\n\ntheta::Real = 1: Implicitness parameter for drift term\nsymplectic::Bool = false: When true and theta=0.5, uses symplectic implicit midpoint\nLinear/nonlinear solver options via linsolve and nlsolve\n\nWhen to Use\n\nRecommended for stiff Itô problems with large noise terms\nWhen both drift and diffusion cause stability issues\nProblems where ImplicitEM and ImplicitRKMil are insufficient\nFully stiff SDEs requiring implicit treatment of everything\n\nAlgorithm Description\n\nApplies implicit treatment to both drift and diffusion using split-step approach:\n\nStep 1: Handle drift implicitly\nStep 2: Handle diffusion implicitly\n\nFully Implicit Features\n\nCan handle stiffness in both drift and diffusion\nMore expensive than drift-only implicit methods\nMost robust for extremely stiff problems\nRequires solving nonlinear systems for both terms\n\nReferences\n\nSplit-step implicit methods for fully stiff SDEs\n\n\n\n\n\n","category":"type"},{"location":"api/stochasticdiffeq/stiff/implicit_methods/#StochasticDiffEq.ISSEulerHeun","page":"Implicit Methods for Stiff SDEs","title":"StochasticDiffEq.ISSEulerHeun","text":"ISSEulerHeun(;chunk_size=0, autodiff=true, diff_type=Val{:central},\n             standardtag=Val{true}(), concrete_jac=nothing, precs=DEFAULT_PRECS,\n             linsolve=nothing, nlsolve=NLNewton(), extrapolant=:constant,\n             theta=1, symplectic=false, new_jac_conv_bound=1e-3, \n             controller=:Predictive)\n\nISSEulerHeun: Implicit Split-Step Euler-Heun Method (Stiff)\n\nFully implicit split-step method for Stratonovich SDEs with stiffness in both drift and diffusion terms.\n\nMethod Properties\n\nStrong Order: 0.5 (Stratonovich sense)\nWeak Order: 1.0\nTime stepping: Adaptive (1.0/1.5 heuristic)\nNoise types: All forms (non-diagonal, scalar, colored noise)\nSDE interpretation: Stratonovich\nImplicit treatment: Both drift and diffusion terms (fully implicit)\n\nParameters\n\ntheta::Real = 1: Implicitness parameter for drift term\nsymplectic::Bool = false: When true and theta=0.5, uses symplectic implicit midpoint\nLinear/nonlinear solver options via linsolve and nlsolve\n\nWhen to Use\n\nRecommended for stiff Stratonovich problems with large noise terms\nWhen both drift and diffusion cause stability issues in Stratonovich form\nStratonovich problems where ImplicitEulerHeun is insufficient\nFully stiff Stratonovich SDEs\n\nAlgorithm Description\n\nStratonovich analog of ISSEM with fully implicit treatment of both drift and diffusion terms using split-step approach.\n\nFully Implicit Features\n\nHandles stiffness in both drift and diffusion for Stratonovich SDEs\nMost expensive but most robust for Stratonovich stiff problems\nRequires solving nonlinear systems for both drift and diffusion\n\nReferences\n\nSplit-step implicit methods for fully stiff Stratonovich SDEs\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#OrdinaryDiffEqBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF","text":"IMEX BDF (Implicit-Explicit Backward Differentiation Formula) methods for stiff differential equations that can be split into stiff and non-stiff components. These methods apply implicit BDF schemes to the stiff part while treating the non-stiff part explicitly, providing efficient handling of problems with mixed stiffness characteristics.","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Key-Properties","page":"OrdinaryDiffEqBDF","title":"Key Properties","text":"IMEX BDF methods provide:\n\nImplicit-explicit splitting for mixed stiffness problems\nBDF stability for the stiff component with A-stable and L-stable behavior\nExplicit treatment of non-stiff terms avoiding unnecessary computational cost\nHigh-order accuracy up to 4th order for both components\nEfficient for large systems where full implicit treatment is expensive\nNatural for operator splitting problems","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#When-to-Use-IMEX-BDF-Methods","page":"OrdinaryDiffEqBDF","title":"When to Use IMEX BDF Methods","text":"These methods are recommended for:\n\nReaction-diffusion systems where reaction terms are stiff and diffusion is moderate\nConvection-diffusion problems with stiff source terms and explicit convection\nParabolic PDEs where diffusion operators are naturally split from other terms\nProblems with natural stiffness separation where some terms require implicit treatment\nLarge-scale systems where full implicit methods are computationally prohibitive\nApplications requiring operator splitting methodology","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Mathematical-Background","page":"OrdinaryDiffEqBDF","title":"Mathematical Background","text":"IMEX BDF methods split the ODE system du/dt = f(u,t) into: du/dt = f₁(u,t) + f₂(u,t)\n\nwhere:\n\nf₁(u,t) contains stiff terms (treated implicitly with BDF)\nf₂(u,t) contains non-stiff terms (treated explicitly)\n\nThis splitting must be chosen carefully to ensure both stability and efficiency.","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Problem-Splitting-Requirements","page":"OrdinaryDiffEqBDF","title":"Problem Splitting Requirements","text":"These methods require a SplitODEProblem formulation where:\n\nFirst function f₁ should contain stiff, implicit terms\nSecond function f₂ should contain non-stiff, explicit terms\nSplitting strategy significantly affects method performance\nStiffness characteristics should align with implicit/explicit treatment","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Solver-Selection-Guide","page":"OrdinaryDiffEqBDF","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#IMEX-Multistep-Methods","page":"OrdinaryDiffEqBDF","title":"IMEX Multistep Methods","text":"SBDF2: Recommended - Second-order IMEX BDF method, good balance of accuracy and stability\nSBDF3: Third-order method for higher accuracy requirements\nSBDF4: Fourth-order method for maximum accuracy in IMEX BDF family\nSBDF: Adaptive order method (experimental)","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#IMEX-SDIRK-Methods","page":"OrdinaryDiffEqBDF","title":"IMEX SDIRK Methods","text":"IMEXEuler: First-order method for simple problems or debugging\nIMEXEulerARK: Alternative first-order formulation","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Performance-Guidelines","page":"OrdinaryDiffEqBDF","title":"Performance Guidelines","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#When-IMEX-BDF-methods-excel","page":"OrdinaryDiffEqBDF","title":"When IMEX BDF methods excel","text":"Natural stiffness separation where splitting is obvious\nLarge systems where full implicit treatment is expensive\nParabolic PDEs with natural operator splitting\nReaction-diffusion problems with well-separated timescales\nProblems where implicit component has efficient linear algebra","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Splitting-strategy-considerations","page":"OrdinaryDiffEqBDF","title":"Splitting strategy considerations","text":"Identify stiff vs non-stiff terms based on eigenvalue analysis\nLinear stiff terms work well in implicit component\nNonlinear non-stiff terms are suitable for explicit treatment\nTest different splittings to optimize performance","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Alternative-Approaches","page":"OrdinaryDiffEqBDF","title":"Alternative Approaches","text":"Consider these alternatives:\n\nFull implicit methods (BDF, SDIRK) if splitting is unclear or ineffective\nStandard IMEX Runge-Kutta methods for different accuracy/efficiency trade-offs\nExponential integrators for linear stiff problems with nonlinear non-stiff terms\nRosenbrock methods for moderately stiff problems without natural splitting","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Usage-Considerations","page":"OrdinaryDiffEqBDF","title":"Usage Considerations","text":"Careful splitting design is crucial for method effectiveness\nStability analysis should verify that explicit treatment doesn't introduce instabilities\nTimestep restrictions may apply to the explicit component\nLinear algebra efficiency in the implicit component affects overall performance\n\nimex_first_steps = evalfile(\"./common_imex_first_steps.jl\")\nimex_first_steps(\"OrdinaryDiffEqBDF\", \"SBDF2\")","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#Full-list-of-solvers","page":"OrdinaryDiffEqBDF","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#IMEX-Multistep","page":"OrdinaryDiffEqBDF","title":"IMEX Multistep","text":"","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#IMEX-SDIRK","page":"OrdinaryDiffEqBDF","title":"IMEX SDIRK","text":"Note that Implicit Euler is the 1st order BDF method, and is thus implemented here using the same machinery.","category":"section"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#OrdinaryDiffEqBDF.SBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.SBDF","text":"SBDF(; chunk_size = Val{0}(),\n       autodiff = AutoForwardDiff(),\n       standardtag = Val{true}(),\n       concrete_jac = nothing,\n       linsolve = nothing,\n       precs = DEFAULT_PRECS,\n       κ = nothing,\n       tol = nothing,\n       nlsolve = NLNewton(),\n       extrapolant = :linear,\n       ark = false,\n       order)\n\nMultistep Method. Implicit-explicit (IMEX) method designed for SplitODEFunction equations, which reduce the size of the implicit handling to a subset of the equations. It's similar to the additive Runge-Kutta methods in splitting mode, like KenCarp4, but instead using a multistep BDF approach\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify SBDF(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nextrapolant: TBD\nark: TBD\n\nReferences\n\n@article{ascher1995implicit, title={Implicit-explicit methods for time-dependent partial differential equations}, author={Ascher, Uri M and Ruuth, Steven J and Wetton, Brian TR}, journal={SIAM Journal on Numerical Analysis}, volume={32}, number={3}, pages={797–823}, year={1995}, publisher={SIAM}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#OrdinaryDiffEqBDF.SBDF2","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.SBDF2","text":"SBDF2(;kwargs...)\n\nThe two-step version of the IMEX multistep methods of\n\nUri M. Ascher, Steven J. Ruuth, Brian T. R. Wetton. Implicit-Explicit Methods for Time-Dependent Partial Differential Equations. Society for Industrial and Applied Mathematics. Journal on Numerical Analysis, 32(3), pp 797-823, 1995. doi: https://doi.org/10.1137/0732037\n\nSee also SBDF.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#OrdinaryDiffEqBDF.SBDF3","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.SBDF3","text":"SBDF3(;kwargs...)\n\nThe three-step version of the IMEX multistep methods of\n\nUri M. Ascher, Steven J. Ruuth, Brian T. R. Wetton. Implicit-Explicit Methods for Time-Dependent Partial Differential Equations. Society for Industrial and Applied Mathematics. Journal on Numerical Analysis, 32(3), pp 797-823, 1995. doi: https://doi.org/10.1137/0732037\n\nSee also SBDF.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#OrdinaryDiffEqBDF.SBDF4","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.SBDF4","text":"SBDF4(;kwargs...)\n\nThe four-step version of the IMEX multistep methods of\n\nUri M. Ascher, Steven J. Ruuth, Brian T. R. Wetton. Implicit-Explicit Methods for Time-Dependent Partial Differential Equations. Society for Industrial and Applied Mathematics. Journal on Numerical Analysis, 32(3), pp 797-823, 1995. doi: https://doi.org/10.1137/0732037\n\nSee also SBDF.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#OrdinaryDiffEqBDF.IMEXEuler","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.IMEXEuler","text":"IMEXEuler(;kwargs...)\n\nThe one-step version of the IMEX multistep methods of\n\nUri M. Ascher, Steven J. Ruuth, Brian T. R. Wetton. Implicit-Explicit Methods for Time-Dependent Partial Differential Equations. Society for Industrial and Applied Mathematics. Journal on Numerical Analysis, 32(3), pp 797-823, 1995. doi: https://doi.org/10.1137/0732037\n\nWhen applied to a SplitODEProblem of the form\n\nu'(t) = f1(u) + f2(u)\n\nThe default IMEXEuler() method uses an update of the form\n\nunew = uold + dt * (f1(unew) + f2(uold))\n\nSee also SBDF, IMEXEulerARK.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/imex/IMEXBDF/#OrdinaryDiffEqBDF.IMEXEulerARK","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.IMEXEulerARK","text":"IMEXEulerARK(;kwargs...)\n\nThe one-step version of the IMEX multistep methods of\n\nUri M. Ascher, Steven J. Ruuth, Brian T. R. Wetton. Implicit-Explicit Methods for Time-Dependent Partial Differential Equations. Society for Industrial and Applied Mathematics. Journal on Numerical Analysis, 32(3), pp 797-823, 1995. doi: https://doi.org/10.1137/0732037\n\nWhen applied to a SplitODEProblem of the form\n\nu'(t) = f1(u) + f2(u)\n\nA classical additive Runge-Kutta method in the sense of Araújo, Murua, Sanz-Serna (1997) consisting of the implicit and the explicit Euler method given by\n\ny1   = uold + dt * f1(y1)\nunew = uold + dt * (f1(unew) + f2(y1))\n\nSee also SBDF, IMEXEuler.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF","text":"Backward Differentiation Formula (BDF) methods are multistep implicit methods specifically designed for solving large stiff systems of differential equations. They are the preferred choice for very large systems (>1000 equations) where other implicit methods become computationally expensive.","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#Key-Properties","page":"OrdinaryDiffEqBDF","title":"Key Properties","text":"BDF methods offer:\n\nExcellent efficiency for large systems (>1000 ODEs)\nL-stable behavior for orders 1 and 2 only\nAdaptive order and stepsize control for optimal performance\nAlpha-stability for higher orders (but less stable than L-stable methods for problems with large complex eigenvalues)","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#When-to-Use-BDF-Methods","page":"OrdinaryDiffEqBDF","title":"When to Use BDF Methods","text":"BDF methods are recommended for:\n\nLarge stiff systems with more than 1000 equations\nVery stiff problems where other implicit methods struggle\nLong-time integration of stiff systems\nParabolic PDEs after spatial discretization\nReaction-diffusion systems and chemical kinetics\nCircuit simulation and other engineering applications with large stiff systems","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#Solver-Selection-Guide","page":"OrdinaryDiffEqBDF","title":"Solver Selection Guide","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#Recommended-methods","page":"OrdinaryDiffEqBDF","title":"Recommended methods","text":"QNDF: Adaptive order quasi-constant timestep BDF, best general choice for large systems\nFBDF: Fixed-leading coefficient BDF, often more efficient than QNDF","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#Performance-Characteristics","page":"OrdinaryDiffEqBDF","title":"Performance Characteristics","text":"Most efficient for systems with >1000 equations\nOutperform Runge-Kutta methods on very large stiff systems\nMemory efficient due to multistep structure\nExcel at very low tolerances (1e-9 and below)\nParticularly effective for problems arising from PDE discretizations","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#Comparison-with-Other-Methods","page":"OrdinaryDiffEqBDF","title":"Comparison with Other Methods","text":"Choose BDF methods over:\n\nRosenbrock methods: When system size > 1000 equations\nSDIRK methods: For very large stiff systems where RK methods become expensive\nExplicit methods: For any stiff problem\n\nChoose other methods over BDF when:\n\nSystem size < 100: Rosenbrock or SDIRK methods often more efficient\nProblems with large complex eigenvalues: Rosenbrock and L-stable SDIRK methods are more stable due to BDF methods only being alpha-stable\nModerate stiffness: SDIRK methods may be more robust\nNon-stiff problems: Use explicit methods like Tsit5\n\nfirst_steps = evalfile(\"./common_first_steps.jl\")\nfirst_steps(\"OrdinaryDiffEqBDF\", \"QNDF\")","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#Full-list-of-solvers","page":"OrdinaryDiffEqBDF","title":"Full list of solvers","text":"","category":"section"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.ABDF2","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.ABDF2","text":"ABDF2(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        κ = nothing,\n        tol = nothing,\n        nlsolve = NLNewton(),\n        smooth_est = true,\n        extrapolant = :linear,\n        controller = :Standard,\n        step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify ABDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nsmooth_est: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\nE. Alberdi Celayaa, J. J. Anza Aguirrezabalab, P. Chatzipantelidisc. Implementation of an Adaptive BDF2 Formula and Comparison with The MATLAB Ode15s. Procedia Computer Science, 29, pp 1014-1026, 2014. doi: https://doi.org/10.1016/j.procs.2014.05.091\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.QNDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QNDF","text":"QNDF(; chunk_size = Val{0}(),\n       autodiff = AutoForwardDiff(),\n       standardtag = Val{true}(),\n       concrete_jac = nothing,\n       linsolve = nothing,\n       precs = DEFAULT_PRECS,\n       κ = nothing,\n       tol = nothing,\n       nlsolve = NLNewton(),\n       extrapolant = :linear,\n       kappa =  promote(-0.1850, -1 // 9, -0.0823, -0.0415, 0),\n       controller = :Standard,\n       step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order quasi-constant timestep NDF method. Similar to MATLAB's ode15s. Uses Shampine's accuracy-optimal coefficients. Performance improves with larger, more complex ODEs. Good for medium to highly stiff problems. Recommended for large systems (>1000 ODEs).\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify QNDF(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nextrapolant: TBD\nkappa: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{shampine1997matlab, title={The matlab ode suite}, author={Shampine, Lawrence F and Reichelt, Mark W}, journal={SIAM journal on scientific computing}, volume={18}, number={1}, pages={1–22}, year={1997}, publisher={SIAM} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.QNDF1","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QNDF1","text":"QNDF1(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        nlsolve = NLNewton(),\n        extrapolant = :linear,\n        kappa = -0.1850,\n        controller = :Standard,\n        step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify QNDF1(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\nkappa: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{shampine1997matlab, title={The matlab ode suite}, author={Shampine, Lawrence F and Reichelt, Mark W}, journal={SIAM journal on scientific computing}, volume={18}, number={1}, pages={1–22}, year={1997}, publisher={SIAM} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.QNDF2","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QNDF2","text":"QNDF2(; chunk_size = Val{0}(),\n        autodiff = AutoForwardDiff(),\n        standardtag = Val{true}(),\n        concrete_jac = nothing,\n        linsolve = nothing,\n        precs = DEFAULT_PRECS,\n        nlsolve = NLNewton(),\n        extrapolant = :linear,\n        kappa =  -1 // 9,\n        controller = :Standard,\n        step_limiter! = trivial_limiter!)\n\nMultistep Method. An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify QNDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\nkappa: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\n\nReferences\n\n@article{shampine1997matlab, title={The matlab ode suite}, author={Shampine, Lawrence F and Reichelt, Mark W}, journal={SIAM journal on scientific computing}, volume={18}, number={1}, pages={1–22}, year={1997}, publisher={SIAM} }\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.QBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QBDF","text":"QBDF: Multistep Method\n\nAn alias of QNDF with κ=0.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.QBDF1","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QBDF1","text":"QBDF1: Multistep Method\n\nAn alias of QNDF1 with κ=0.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.QBDF2","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.QBDF2","text":"QBDF2: Multistep Method\n\nAn alias of QNDF2 with κ=0.\n\n\n\n\n\n","category":"function"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.MEBDF2","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.MEBDF2","text":"MEBDF2(; chunk_size = Val{0}(),\n         autodiff = AutoForwardDiff(),\n         standardtag = Val{true}(),\n         concrete_jac = nothing,\n         linsolve = nothing,\n         precs = DEFAULT_PRECS,\n         nlsolve = NLNewton(),\n         extrapolant = :constant)\n\nMultistep Method. The second order Modified Extended BDF method,     which has improved stability properties over the standard BDF.     Fixed timestep only.\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify MEBDF2(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- nlsolve: TBD\nextrapolant: TBD\n\nReferences\n\n@article{cash2000modified, title={Modified extended backward differentiation formulae for the numerical solution of stiff initial value problems in ODEs and DAEs}, author={Cash, JR}, journal={Journal of Computational and Applied Mathematics}, volume={125}, number={1-2}, pages={117–130}, year={2000}, publisher={Elsevier}}\n\n\n\n\n\n","category":"type"},{"location":"api/ordinarydiffeq/implicit/BDF/#OrdinaryDiffEqBDF.FBDF","page":"OrdinaryDiffEqBDF","title":"OrdinaryDiffEqBDF.FBDF","text":"FBDF(; chunk_size = Val{0}(),\n       autodiff = AutoForwardDiff(),\n       standardtag = Val{true}(),\n       concrete_jac = nothing,\n       linsolve = nothing,\n       precs = DEFAULT_PRECS,\n       κ = nothing,\n       tol = nothing,\n       nlsolve = NLNewton(),\n       extrapolant = :linear,\n       controller = :Standard,\n       step_limiter! = trivial_limiter!,\n       max_order::Val{MO} = Val{5}())\n\nMultistep Method. An adaptive order quasi-constant timestep NDF method. Fixed leading coefficient BDF. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients).\n\nKeyword Arguments\n\nautodiff: Uses ADTypes.jl    to specify whether to use automatic differentiation via   ForwardDiff.jl or finite   differencing via FiniteDiff.jl.    Defaults to AutoForwardDiff() for automatic differentiation, which by default uses   chunksize = 0, and thus uses the internal ForwardDiff.jl algorithm for the choice.   To use FiniteDiff.jl, the AutoFiniteDiff() ADType can be used, which has a keyword argument   fdtype with default value Val{:forward}(), and alternatives Val{:central}() and Val{:complex}().\nstandardtag: Specifies whether to use package-specific tags instead of the   ForwardDiff default function-specific tags. For more information, see   this blog post.   Defaults to Val{true}().\nconcrete_jac: Specifies whether a Jacobian should be constructed. Defaults to   nothing, which means it will be chosen true/false depending on circumstances   of the solver, such as whether a Krylov subspace method is used for linsolve.\nlinsolve: Any LinearSolve.jl compatible linear solver. For example, to use KLU.jl, specify FBDF(linsolve = KLUFactorization()).  When nothing is passed, uses DefaultLinearSolver.\nprecs: Any LinearSolve.jl-compatible preconditioner can be used as a left or right preconditioner. Preconditioners are specified by the Pl,Pr = precs(W,du,u,p,t,newW,Plprev,Prprev,solverdata) function where the arguments are defined as:\nW: the current Jacobian of the nonlinear system. Specified as either   I - gamma J or Igamma - J depending on the algorithm. This will   commonly be a WOperator type defined by OrdinaryDiffEq.jl. It is a lazy   representation of the operator. Users can construct the W-matrix on demand   by calling convert(AbstractMatrix,W) to receive an AbstractMatrix matching   the jac_prototype.\ndu: the current ODE derivative\nu: the current ODE state\np: the ODE parameters\nt: the current ODE time\nnewW: a Bool which specifies whether the W matrix has been updated since   the last call to precs. It is recommended that this is checked to only   update the preconditioner when newW == true.\nPlprev: the previous Pl.\nPrprev: the previous Pr.\nsolverdata: Optional extra data the solvers can give to the precs function.   Solver-dependent and subject to change.\nThe return is a tuple (Pl,Pr) of the LinearSolve.jl-compatible preconditioners. To specify one-sided preconditioning, simply return nothing for the preconditioner which is not used. Additionally, precs must supply the dispatch:\nPl, Pr = precs(W, du, u, p, t, ::Nothing, ::Nothing, ::Nothing, solverdata)\nwhich is used in the solver setup phase to construct the integrator type with the preconditioners (Pl,Pr). The default is precs=DEFAULT_PRECS where the default preconditioner function is defined as:\nDEFAULT_PRECS(W, du, u, p, t, newW, Plprev, Prprev, solverdata) = nothing, nothing\n/n- κ: TBD\ntol: TBD\nnlsolve: TBD\nextrapolant: TBD\ncontroller: TBD\nstep_limiter!: function of the form limiter!(u, integrator, p, t)\nmax_order: TBD\n\nReferences\n\n@article{shampine2002solving, title={Solving 0= F (t, y (t), y′(t)) in Matlab}, author={Shampine, Lawrence F}, year={2002}, publisher={Walter de Gruyter GmbH \\& Co. KG}}\n\n\n\n\n\n","category":"type"},{"location":"basics/overview/#Overview-of-DifferentialEquations.jl","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"The general workflow for using the package is as follows:\n\nDefine a problem\nSolve the problem\nAnalyze the output","category":"section"},{"location":"basics/overview/#Defining-Problems","page":"Overview of DifferentialEquations.jl","title":"Defining Problems","text":"Problems are specified via a type interface. The problem types are designed to contain the necessary information to fully define their associated differential equation. Each problem type has a page explaining their problem type and the special features associated with them. For example, an ordinary differential equation is defined by\n\nfracdudt = f(upt)\n\nover some time interval tspan with some initial condition u0, and therefore the ODEProblem is defined by those components:\n\nprob = ODEProblem(f, u0, tspan)\nprob = ODEProblem(f, u0, tspan, p)\n\nNote that the number types in the solution will match the types you designate in the problem. For example, if one uses Rational{BigInt} for specifying the timespan and BigFloat for specifying the initial condition, then the solution will solve using Rational{BigInt} for the timesteps and BigFloat for the independent variables. A wide variety of number types are compatible with the solvers such as complex numbers, unitful numbers (via Unitful.jl), decimals (via DecFP), dual numbers, and many more which may not have been tested yet (thanks to the power of multiple dispatch!). For information on type-compatibility, please see the solver pages for the specific problems.","category":"section"},{"location":"basics/overview/#Solving-the-Problems","page":"Overview of DifferentialEquations.jl","title":"Solving the Problems","text":"Each type of differential equation has its own problem type which allow the solvers to dispatch to the right methods. The common interface for calling the solvers is:\n\nsol = solve(prob, alg; kwargs)\n\nInto the command, one passes the differential equation problem that they defined, prob, optionally chooses an algorithm alg (a default is given if not chosen), and changes the properties of the solver using keyword arguments. The common arguments which are accepted by most methods are defined in the common solver options manual page. The solver returns a solution object sol which holds all the details for the solution.","category":"section"},{"location":"basics/overview/#Analyzing-the-Solution","page":"Overview of DifferentialEquations.jl","title":"Analyzing the Solution","text":"With the solution object, you do the analysis as you please! The solution type has a common interface, which makes handling the solution similar between the different types of differential equations. Tools such as interpolations are seamlessly built into the solution interface to make analysis easy. This interface is described in the solution handling manual page.\n\nPlotting functionality is provided by a recipe to Plots.jl. To use plot solutions, simply call the plot(sol) and the plotter will generate appropriate plots. If save_everystep was used, the plotters can generate animations of the solutions to evolution equations using the animate(sol) command. Plots can be customized using all the keyword arguments provided by Plots.jl. Please see Plots.jl's documentation for more information.","category":"section"},{"location":"basics/overview/#Add-on-Tools","page":"Overview of DifferentialEquations.jl","title":"Add-on Tools","text":"One of the most compelling features of DifferentialEquations.jl is that the common solver interface allows one to build tools which are “algorithm and problem agnostic”. For example, one of the provided tools allows for performing parameter estimation on ODEProblems. Since the solve interface is the same for the different algorithms, one can use any of the associated solving algorithms. This modular structure allows one to mix and match overarching analysis tools with specialized algorithms to one's problem, leading to high performance with a large feature base. Isn't that the promise of Julia just being fulfilled?","category":"section"},{"location":"basics/overview/#Development-and-Testing-Tools","page":"Overview of DifferentialEquations.jl","title":"Development and Testing Tools","text":"Lastly, one unique feature of DifferentialEquations.jl is the existence of algorithm development and testing functionality. This suite was designed by researchers in the field of numerical differential equations to both try out new ideas and distribute finalized results to large audiences. The tools for algorithm development allow for easy convergence testing, benchmarking, and higher order analysis (stability plotting, etc.). This is one of the reasons why DifferentialEquations.jl contains many algorithms which are unique and the results of recent publications! Please check out the developer documentation for more information on using the development tools.\n\nNote that DifferentialEquations.jl allows for distributed development, meaning that algorithms which “plug-into the ecosystem” don't have to be a part of the major packages. If you are interested in adding your work to the ecosystem, checkout the developer documentation for more information.","category":"section"},{"location":"examples/classical_physics/#Classical-Physics-Models","page":"Classical Physics Models","title":"Classical Physics Models","text":"If you're getting some cold feet to jump in to DiffEq land, here are some handcrafted differential equations mini problems to hold your hand along the beginning of your journey.","category":"section"},{"location":"examples/classical_physics/#First-order-linear-ODE","page":"Classical Physics Models","title":"First order linear ODE","text":"","category":"section"},{"location":"examples/classical_physics/#Radioactive-Decay-of-Carbon-14","page":"Classical Physics Models","title":"Radioactive Decay of Carbon-14","text":"fracdudt = f(tu) = - λ u\n\nThe Radioactive decay problem is the first order linear ODE problem of an exponential with a negative coefficient, which represents the half-life of the process in question. Should the coefficient be positive, this would represent a population growth equation. λ is known as the decay rate, and can be related to the half-life as λ = ln(2)t_12.\n\nimport OrdinaryDiffEq as ODE, Plots\nPlots.gr()\n\n#Half-life of Carbon-14 is 5,730 years.\nt½ = 5.730\n\n#Setup\nu₀ = 1.0\ntspan = (0.0, 30.0)\n\n#Define the problem\nradioactivedecay(u, p, t) = -log(2) / t½ * u\n\n#Pass to solver\nprob = ODE.ODEProblem(radioactivedecay, u₀, tspan)\nsol = ODE.solve(prob, ODE.Tsit5())\n\n#Plot\nPlots.plot(sol, linewidth = 2, title = \"Carbon-14 half-life\",\n    xaxis = \"Time in thousands of years\", yaxis = \"Ratio left\",\n    label = \"Numerical Solution\")\nPlots.plot!(sol.t, t -> 2^(-t / t½), lw = 3, ls = :dash, label = \"Analytical Solution\")","category":"section"},{"location":"examples/classical_physics/#Second-Order-Linear-ODE","page":"Classical Physics Models","title":"Second Order Linear ODE","text":"","category":"section"},{"location":"examples/classical_physics/#Simple-Harmonic-Oscillator","page":"Classical Physics Models","title":"Simple Harmonic Oscillator","text":"Another classical example is the harmonic oscillator, given by:\n\nddotx + omega^2 x = 0\n\nwith the known analytical solution\n\nbeginalign*\nx(t) = Acos(omega t - phi) \nv(t) = -Aomegasin(omega t - phi)\nendalign*\n\nwhere\n\nA = sqrtc_1 + c_2 qquadtextandqquad tan phi = fracc_2c_1\n\nwith c_1, c_2 constants determined by the initial conditions such that c_1 is the initial position and omega c_2 is the initial velocity.\n\nInstead of transforming this to a system of ODEs to solve with ODEProblem, we can use SecondOrderODEProblem as follows.\n\n# Simple Harmonic Oscillator Problem\nimport OrdinaryDiffEq as ODE, Plots\n\n#Parameters\nω = 1\n\n#Initial Conditions\nx₀ = [0.0]\ndx₀ = [π / 2]\ntspan = (0.0, 2π)\n\nϕ = atan((dx₀[1] / ω) / x₀[1])\nA = √(x₀[1]^2 + dx₀[1]^2)\n\n#Define the problem\nfunction harmonicoscillator(ddu, du, u, ω, t)\n    ddu .= -ω^2 * u\nend\n\n#Pass to solvers\nprob = ODE.SecondOrderODEProblem(harmonicoscillator, dx₀, x₀, tspan, ω)\nsol = ODE.solve(prob, ODE.DPRKN6())\n\n#Plot\nPlots.plot(sol, idxs = [2, 1], linewidth = 2, title = \"Simple Harmonic Oscillator\",\n    xaxis = \"Time\", yaxis = \"Elongation\", label = [\"x\" \"dx\"])\nPlots.plot!(t -> A * cos(ω * t - ϕ), lw = 3, ls = :dash, label = \"Analytical Solution x\")\nPlots.plot!(t -> -A * ω * sin(ω * t - ϕ), lw = 3, ls = :dash, label = \"Analytical Solution dx\")\n\nNote that the order of the variables (and initial conditions) is dx, x. Thus, if we want the first series to be x, we have to flip the order with vars=[2,1].","category":"section"},{"location":"examples/classical_physics/#Second-Order-Non-linear-ODE","page":"Classical Physics Models","title":"Second Order Non-linear ODE","text":"","category":"section"},{"location":"examples/classical_physics/#Simple-Pendulum","page":"Classical Physics Models","title":"Simple Pendulum","text":"We will start by solving the pendulum problem. In the physics class, we often solve this problem by small angle approximation, i.e. sin(θ) approx θ, because otherwise, we get an elliptic integral which doesn't have an analytic solution. The linearized form is\n\nddotθ + fracgL θ = 0\n\nBut we have numerical ODE solvers! Why not solve the real pendulum?\n\nddotθ + fracgL sin(θ) = 0\n\nNotice that now we have a second order ODE. In order to use the same method as above, we need to transform it into a system of first order ODEs by employing the notation ω(t) = dotθ.\n\nbeginalign*\ndotθ = ω \ndotω = - fracgL sin(θ)\nendalign*\n\n# Simple Pendulum Problem\nimport OrdinaryDiffEq as ODE, Plots\n\n#Constants\nconst g = 9.81\nL = 1.0\n\n#Initial Conditions\nu₀ = [0, π / 2]\ntspan = (0.0, 6.3)\n\n#Define the problem\nfunction simplependulum(du, u, p, t)\n    θ, ω = u\n    du[1] = ω\n    du[2] = -(g / L) * sin(θ)\nend\n\n#Pass to solvers\nprob = ODE.ODEProblem(simplependulum, u₀, tspan)\nsol = ODE.solve(prob, ODE.Tsit5())\n\n#Plot\nPlots.plot(sol, linewidth = 2, title = \"Simple Pendulum Problem\", xaxis = \"Time\",\n    yaxis = \"Height\", label = [\"\\\\theta\" \"d\\\\theta\"])\n\nSo now we know that behaviour of the position versus time. However, it will be useful to us to look at the phase space of the pendulum, i.e., and representation of all possible states of the system in question (the pendulum) by looking at its velocity and position. Phase space analysis is ubiquitous in the analysis of dynamical systems, and thus we will provide a few facilities for it.\n\np = Plots.plot(sol, vars = (1, 2), xlims = (-9, 9), title = \"Phase Space Plot\",\n    xaxis = \"Angular position\", yaxis = \"Angular velocity\", leg = false)\nfunction phase_plot(prob, u0, p, tspan = 2pi)\n    _prob = ODE.ODEProblem(prob.f, u0, (0.0, tspan))\n    sol = ODE.solve(_prob, ODE.Vern9()) # Use Vern9 solver for higher accuracy\n    Plots.plot!(p, sol, idxs = (1, 2))\nend\nfor i in (-4pi):(pi / 2):(4π)\n    for j in (-4pi):(pi / 2):(4π)\n        phase_plot(prob, [j, i], p)\n    end\nend\nPlots.plot(p, xlims = (-9, 9))","category":"section"},{"location":"examples/classical_physics/#Double-Pendulum","page":"Classical Physics Models","title":"Double Pendulum","text":"A more complicated example is given by the double pendulum. The equations governing its motion are given by the following (taken from this Stack Overflow question)\n\nfracddt\nbeginpmatrix alpha  l_alpha  beta  l_beta endpmatrix\n=\nbeginpmatrix\n2fracl_alpha - (1+cosbeta)l_beta3-cos 2beta \n-2sinalpha - sin(alpha + beta) \n2frac-(1+cosbeta)l_alpha + (3+2cosbeta)l_beta3-cos2beta\n-sin(alpha+beta) - 2sin(beta)frac(l_alpha-l_beta)l_beta3-cos2beta + 2sin(2beta)fracl_alpha^2-2(1+cosbeta)l_alpha l_beta + (3+2cosbeta)l_beta^2(3-cos2beta)^2\nendpmatrix\n\n#Double Pendulum Problem\nimport OrdinaryDiffEq as ODE, Plots\n\n#Constants and setup\nconst m₁, m₂, L₁, L₂ = 1, 2, 1, 2\ninitial = [0, π / 3, 0, 3pi / 5]\ntspan = (0.0, 50.0)\n\n#Convenience function for transforming from polar to Cartesian coordinates\nfunction polar2cart(sol; dt = 0.02, l1 = L₁, l2 = L₂, vars = (2, 4))\n    u = sol.t[1]:dt:sol.t[end]\n\n    p1 = l1 * map(x -> x[vars[1]], sol.(u))\n    p2 = l2 * map(y -> y[vars[2]], sol.(u))\n\n    x1 = l1 * sin.(p1)\n    y1 = l1 * -cos.(p1)\n    (u, (x1 + l2 * sin.(p2),\n         y1 - l2 * cos.(p2)))\nend\n\n#Define the Problem\nfunction double_pendulum(xdot, x, p, t)\n    xdot[1] = x[2]\n    xdot[2] = -((g * (2 * m₁ + m₂) * sin(x[1]) +\n                 m₂ * (g * sin(x[1] - 2 * x[3]) +\n                  2 * (L₂ * x[4]^2 + L₁ * x[2]^2 * cos(x[1] - x[3])) * sin(x[1] - x[3]))) /\n                (2 * L₁ * (m₁ + m₂ - m₂ * cos(x[1] - x[3])^2)))\n    xdot[3] = x[4]\n    xdot[4] = (((m₁ + m₂) * (L₁ * x[2]^2 + g * cos(x[1])) +\n                L₂ * m₂ * x[4]^2 * cos(x[1] - x[3])) * sin(x[1] - x[3])) /\n              (L₂ * (m₁ + m₂ - m₂ * cos(x[1] - x[3])^2))\nend\n\n#Pass to Solvers\ndouble_pendulum_problem = ODE.ODEProblem(double_pendulum, initial, tspan)\nsol = ODE.solve(double_pendulum_problem, ODE.Vern7(), abstol = 1e-10, dt = 0.05);\n\n#Obtain coordinates in Cartesian Geometry\nts, ps = polar2cart(sol, l1 = L₁, l2 = L₂, dt = 0.01)\nPlots.plot(ps...)","category":"section"},{"location":"examples/classical_physics/#Poincaré-section","page":"Classical Physics Models","title":"Poincaré section","text":"In this case, the phase space is 4 dimensional, and it cannot be easily visualized. Instead of looking at the full phase space, we can look at Poincaré sections, which are sections through a higher-dimensional phase space diagram. This helps to understand the dynamics of interactions and is wonderfully pretty.\n\nThe Poincaré section in this is given by the collection of (βl_β) when α=0 and fracdαdt0.\n\n#Constants and setup\nimport OrdinaryDiffEq as ODE\ninitial2 = [0.01, 0.005, 0.01, 0.01]\ntspan2 = (0.0, 500.0)\n\n#Define the problem\nfunction double_pendulum_hamiltonian(udot, u, p, t)\n    α, lα, β, lβ = u\n    udot .= [2(lα - (1 + cos(β))lβ) / (3 - cos(2β)),\n        -2sin(α) - sin(α + β),\n        2(-(1 + cos(β))lα + (3 + 2cos(β))lβ) / (3 - cos(2β)),\n        -sin(α + β) - 2sin(β) * (((lα - lβ)lβ) / (3 - cos(2β))) +\n        2sin(2β) * ((lα^2 - 2(1 + cos(β))lα * lβ + (3 + 2cos(β))lβ^2) / (3 - cos(2β))^2)]\nend\n\n# Construct a ContiunousCallback\ncondition(u, t, integrator) = u[1]\naffect!(integrator) = nothing\ncb = ODE.ContinuousCallback(condition, affect!, nothing,\n    save_positions = (true, false))\n\n# Construct Problem\npoincare = ODE.ODEProblem(double_pendulum_hamiltonian, initial2, tspan2)\nsol2 = ODE.solve(poincare, ODE.Vern9(), save_everystep = false, save_start = false,\n    save_end = false, callback = cb, abstol = 1e-16, reltol = 1e-16)\n\nfunction poincare_map(prob, u₀, p; callback = cb)\n    _prob = ODE.ODEProblem(prob.f, u₀, prob.tspan)\n    sol = ODE.solve(_prob, ODE.Vern9(), save_everystep = false, save_start = false,\n        save_end = false, callback = cb, abstol = 1e-16, reltol = 1e-16)\n    Plots.scatter!(p, sol, idxs = (3, 4), markersize = 3, msw = 0)\nend\n\nlβrange = -0.02:0.0025:0.02\np = Plots.scatter(sol2, idxs = (3, 4), leg = false, markersize = 3, msw = 0)\nfor lβ in lβrange\n    poincare_map(poincare, [0.01, 0.01, 0.01, lβ], p)\nend\nPlots.plot(p, xlabel = \"\\\\beta\", ylabel = \"l_\\\\beta\", ylims = (0, 0.03))","category":"section"},{"location":"examples/classical_physics/#Hénon-Heiles-System","page":"Classical Physics Models","title":"Hénon-Heiles System","text":"The Hénon-Heiles potential occurs when non-linear motion of a star around a galactic center, with the motion restricted to a plane.\n\nbeginalign*\nfracd^2xdt^2=-fracpartial Vpartial x\nfracd^2ydt^2=-fracpartial Vpartial y\nendalign*\n\nwhere\n\nV(xy) = frac 12 (x^2+y^2) + λ left(x^2 y - fracy^33right)\n\nWe pick λ=1 in this case, so\n\nV(xy) = frac12 left(x^2 + y^2 + 2 x^2 y - frac23 y^3right)\n\nThen the total energy of the system can be expressed by\n\nE = T+V = V(xy) + frac12 left(dotx^2+doty^2right)\n\nThe total energy should conserve as this system evolves.\n\nimport OrdinaryDiffEq as ODE, Plots\n\n#Setup\ninitial = [0.0, 0.1, 0.5, 0]\ntspan = (0, 100.0)\n\n#Remember, V is the potential of the system and T is the Total Kinetic Energy, thus E will\n#the total energy of the system.\nV(x, y) = 1 // 2 * (x^2 + y^2 + 2x^2 * y - 2 // 3 * y^3)\nE(x, y, dx, dy) = V(x, y) + 1 // 2 * (dx^2 + dy^2);\n\n#Define the function\nfunction Hénon_Heiles(du, u, p, t)\n    x, y, dx, dy = u\n    du[1] = dx\n    du[2] = dy\n    du[3] = -x - 2x * y\n    du[4] = y^2 - y - x^2\nend\n\n#Pass to solvers\nprob = ODE.ODEProblem(Hénon_Heiles, initial, tspan)\nsol = ODE.solve(prob, ODE.Vern9(), abstol = 1e-16, reltol = 1e-16);\n\n# Plot the orbit\nPlots.plot(sol, idxs = (1, 2), title = \"The orbit of the Hénon-Heiles system\", xaxis = \"x\",\n    yaxis = \"y\", leg = false)\n\n#Optional Sanity check - what do you think this returns and why?\n@show sol.retcode\n\n#Plot -\nPlots.plot(sol, idxs = (1, 3), title = \"Phase space for the Hénon-Heiles system\",\n    xaxis = \"Position\", yaxis = \"Velocity\")\nPlots.plot!(sol, idxs = (2, 4), leg = false)\n\n#We map the Total energies during the time intervals of the solution (sol.u here) to a new vector\n#pass it to the plotter a bit more conveniently\nenergy = map(x -> E(x...), sol.u)\n\n#We use @show here to easily spot erratic behavior in our system by seeing if the loss in energy was too great.\n@show ΔE = energy[1] - energy[end]\n\n#Plot\nPlots.plot(sol.t, energy .- energy[1], title = \"Change in Energy over Time\",\n    xaxis = \"Time in iterations\", yaxis = \"Change in Energy\")","category":"section"},{"location":"examples/classical_physics/#Symplectic-Integration","page":"Classical Physics Models","title":"Symplectic Integration","text":"To prevent energy drift, we can instead use a symplectic integrator. We can directly define and solve the SecondOrderODEProblem:\n\nfunction HH_acceleration!(dv, v, u, p, t)\n    x, y = u\n    dx, dy = dv\n    dv[1] = -x - 2x * y\n    dv[2] = y^2 - y - x^2\nend\ninitial_positions = [0.0, 0.1]\ninitial_velocities = [0.5, 0.0]\nprob = ODE.SecondOrderODEProblem(HH_acceleration!, initial_velocities, initial_positions, tspan)\nsol2 = ODE.solve(prob, ODE.KahanLi8(), dt = 1 / 10);\n\nNotice that we get the same results:\n\n# Plot the orbit\nPlots.plot(\n    sol2, idxs = (3, 4), title = \"The orbit of the Hénon-Heiles system\", xaxis = \"x\",\n    yaxis = \"y\", leg = false)\n\nPlots.plot(sol2, idxs = (3, 1), title = \"Phase space for the Hénon-Heiles system\",\n    xaxis = \"Position\", yaxis = \"Velocity\")\nPlots.plot!(sol2, idxs = (4, 2), leg = false)\n\nbut now the energy change is essentially zero:\n\nenergy = map(x -> E(x[3], x[4], x[1], x[2]), sol2.u)\n#We use @show here to easily spot erratic behaviour in our system by seeing if the loss in energy was too great.\n@show ΔE = energy[1] - energy[end]\n\n#Plot\nPlots.plot(sol2.t, energy .- energy[1], title = \"Change in Energy over Time\",\n    xaxis = \"Time in iterations\", yaxis = \"Change in Energy\")\n\nAnd let's try to use a Runge-Kutta-Nyström solver to solve this. Note that Runge-Kutta-Nyström isn't symplectic.\n\nsol3 = ODE.solve(prob, ODE.DPRKN6());\nenergy = map(x -> E(x[3], x[4], x[1], x[2]), sol3.u)\n@show ΔE = energy[1] - energy[end]\nPlots.gr()\nPlots.plot(sol3.t, energy .- energy[1], title = \"Change in Energy over Time\",\n    xaxis = \"Time in iterations\", yaxis = \"Change in Energy\")\n\nNote that we are using the DPRKN6 solver at reltol=1e-3 (the default), yet it has a smaller energy variation than Vern9 at abstol=1e-16, reltol=1e-16. Therefore, using specialized solvers to solve its particular problem is very efficient.","category":"section"},{"location":"features/noise_process/#noise_process","page":"Noise Processes","title":"Noise Processes","text":"Noise processes are essential in continuous stochastic modeling. The NoiseProcess types are distributionally-exact, meaning they are not solutions of stochastic differential equations and instead are directly generated according to their analytical distributions. These processes are used as the noise term in the SDE and RODE solvers. Additionally, the noise processes themselves can be simulated and solved using the DiffEq common interface (including the Monte Carlo interface).\n\nFor more details, see DiffEqNoiseProcess.jl","category":"section"},{"location":"solvers/sdae_solve/#sdae_solve","page":"SDAE Solvers","title":"SDAE Solvers","text":"","category":"section"},{"location":"solvers/sdae_solve/#Recommended-Methods","page":"SDAE Solvers","title":"Recommended Methods","text":"The recommendations for SDAEs are the same recommended implicit SDE methods for stiff equations when the SDAE is specified in mass matrix form.","category":"section"},{"location":"solvers/sdae_solve/#Mass-Matrix-Form","page":"SDAE Solvers","title":"Mass Matrix Form","text":"ImplicitEM - An order 0.5 Ito drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSTrapezoid - An alias for ImplicitEM with theta=1/2\nSImplicitMidpoint - An alias for ImplicitEM with theta=1/2 and symplectic=true\nImplicitEulerHeun - An order 0.5 Stratonovich drift-implicit method. This is a theta method which defaults to theta=1/2 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nImplicitRKMil - An order 1.0 drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. Defaults to solving the Ito problem, but ImplicitRKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Handles diagonal and scalar noise. Uses a 1.5/2.0 heuristic for adaptive time stepping.\nISSEM - An order 0.5 split-step Ito implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nISSEulerHeun - An order 0.5 split-step Stratonovich implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, Q scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSKenCarp - Adaptive L-stable drift-implicit strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal and scalar additive noise.*†","category":"section"},{"location":"solvers/sdae_solve/#Notes","page":"SDAE Solvers","title":"Notes","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.\n\n*:  Note that although SKenCarp uses the same table as KenCarp3, solving a ODE problem using SKenCarp by setting g(du,u,p,t) = du .= 0 will take many more steps than KenCarp3 because error estimator of SKenCarp is different (because of noise terms) and default value of qmax (maximum permissible ratio of relaxing/tightening dt for adaptive steps) is smaller for StochasticDiffEq algorithms.","category":"section"}]
}
